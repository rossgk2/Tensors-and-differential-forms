\chapter{Review of logic, proofs, and functions}
\label{ch::logic_pf_fns}

\subsection*{Notation for definitions}

In this book, we use $:=$ whenever an equality is enforced to be true by definition (e.g. ``define a function $f$ by $f(x) := x^2$''), and use $=$ whenever an equality follows from further reasoning (e.g. since $f(x) := x^2$ we have $f(x) = (x^2)^2 = x^4$).

\section{Propositional logic}

A \textit{proposition} is a statement that is either true or false; an example is ``the sky is blue right now''.

We can denote propositions  with letters, and say things such as ``let $P$ be a proposition''. When a proposition $P$ is true, we write $P = T$. The $=$ symbol is used here to denote \textit{logical equality}, and the $T$ denotes ``truth''. Similarly, we write $P = F$ when $P$ is false.

\subsection*{Logical operators}

More complicated propositions can be constructed from simpler propositions. Examples of some more complicated propositions are $\Big((3 > 4)$ and $(\text{every rectangle is a square}) \Big) = F$ and \\ $\Big((5 > -3) \text{ or } (2 > 100)\Big) = T$.

There are three fundamental operations on propositions that are used to build more complicated propositions from simpler ones: there are the two-argument (binary) operators \textit{and} and \textit{or} and the one-argument (unary) operator \textit{not}.

The operators \textit{and}, \textit{or}, and \textit{not} act on propositions $P$ and $Q$ as is expressed in the following ``truth tables''.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P$ and $Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $F$        \\ \hline
    $F$ & $F$ & $F$        \\ \hline
    \end{tabular}
    \quad
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P$ or $Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $T$        \\ \hline
    $F$ & $T$ & $T$        \\ \hline
    $F$ & $F$ & $F$        \\ \hline
    \end{tabular}
    \quad
    \begin{tabular}{|l|l|}
    \hline
    $P$ & $\text{not } P$ \\ \hline
    $T$ & $F$      \\ \hline
    $F$ & $T$      \\ \hline
    \end{tabular}
\end{table}

So, $P \textit{ and } Q$ evaluates to true only when both $P$ and $Q$ are true, $P \textit{ or } Q$ evaluates to true whenever either of $P$ or $Q$ is true, and $\textit{not } P$ evaluates to the ``opposite'' of $P$. Note that \textit{or} is not the same as \textit{exclusive or}, also called \textit{xor}, which evaluates to true whenever exactly one of $P, Q$ is true.

By making use of the above truth tables, you can check that, for propositions $P$ and $Q$, we have the following two logical identities, called \textit{DeMorgan's laws}:

\begin{align*}
    &\text{not }(P \text{ and } Q) = (\text{not } P) \text{ or } (\text{not } Q) \\
    &\text{not }(P \text{ or } Q) = (\text{not } P) \text{ and } (\text{not } Q).
\end{align*}

Sometimes, symbols are used to represent \textit{and}, \textit{or}, and \textit{not}: $\wedge$ denotes \textit{and}, $\vee$ denotes \textit{or}, and either $\sim$ or $\neg$ denotes \textit{not}. We will not use these symbols.

\newpage

\section{Predicate logic}

A \textit{predicate} is a proposition in which zero or more of the entities are variables. For example, $P(x) := (x > 3)$ is a predicate which is true for some values of $x$ but false for others. We will sometimes informally refer to predicates as \textit{properties}.

Note that a proposition is a predicate in which exactly zero of the entities are variables.

\subsection*{Quantificational logic}

The \textit{universal quantifier} is the symbol $\forall$; we read ``$\forall x$'' as ``for all $x$''. The \textit{existential quantifier} is the symbol $\exists$; we read ``$\exists x$'' as ``there exists $x$ such that''. The quantifiers $\forall$ and $\exists$ are used in the following way to create propositions from single-input predicates:

\begin{align*}
    \forall x \spc P(x) &\qquad \text{is the proposition which states ``for all $x$, $P(x) = T$''} \\
    \exists x \spc \spc Q(x) &\qquad \text{is the proposition which states ``there exists an $x$ such that $Q(x) = T$''}.
\end{align*}

The proposition $\forall x \spc P(x)$ is true exactly when $P(x)$ is true for all $x$, and the proposition $\exists x \spc Q(x)$ is true exactly when $Q(x)$ is true for one or more $x$. In this sense, $\forall$ is similar to \textit{and} and $\exists$ is similar to \textit{or}. Here's an example: the proposition $(\forall x \spc x > 3)$ is false, while the proposition $(\exists x \spc x > 3)$ is true\footnote{Technically, these propositions aren't really sensible since we haven't specified that the $x$'s involved can only be numbers and not other entities.}.

\subsubsection{Nested quantifers}

In the last section, we saw that applying a quantifier-variable pair (such as $\forall x$) to a single-input predicate produced a proposition. We now consider what happens when we apply a quantifier-variable pair to a two-input predicate $P(x, y)$. The four possible ways to apply a quantifier-variable pair to $P(x, y)$ are

\begin{align*}
    \forall x \spc P(x, y) \\
    \exists x \spc P(x, y) \\
    \forall y \spc P(x, y) \\
    \exists y \spc P(x, y).
\end{align*}

We see that each of the above is itself a single-input predicate. That is, we can define single-input predicates $Q_1(y), Q_2(y), Q_3(x), Q_4(x)$ as follows:

\begin{align*}
    Q_1(y) := \forall x \spc P(x, y) \\
    Q_2(y) := \exists x \spc P(x, y) \\
    Q_3(x) := \forall y \spc P(x, y) \\
    Q_4(x) := \exists y \spc P(x, y).
\end{align*}

Since we can apply quantifier-variable pairs to the single-input predicates $Q_1(y), Q_2(y), Q_3(y), Q_4(y)$, we obtain the following propositions:

\begin{align*}
    &\forall x \spc (\forall y \spc P(x, y)) \\
    &\forall x \spc (\exists y \spc P(x, y)) \\
    &\exists x \spc (\forall y \spc P(x, y)) \\
    &\exists x \spc (\exists y \spc P(x, y)).
\end{align*}

These propositions involve so-called \textit{nested quantifiers}. In practice, we omit the inner parentheses and use syntax such as $\forall x \spc \exists y \spc P(x, y)$ rather than syntax such as $\forall x \spc (\exists y \spc P(x, y))$.

\subsubsection*{Quantificational identities}

It's useful to know that when two quantifier-variable pairs are nested and the quantifiers are the same, we have the following commutative property:

\begin{align*}
    &\forall x \spc \forall y \spc P(x, y) = \forall y \spc \forall x \spc P(x, y) \\
    &\exists x \spc \exists y \spc Q(x, y) = \exists y \spc \exists x \spc Q(x, y).
\end{align*}

There is also a shorthand notation for situations in which we have two nested quantifiers of the same type:

\begin{align*}
    &\forall x, y \spc P(x, y) := \forall x \spc \forall y \spc P(x, y) \\
    &\exists x, y \spc P(x, y) := \exists x \spc \exists y \spc P(x, y).
\end{align*}

\subsubsection{Negating quantifiers}

The \textit{not} operator applies to single-input predicates constructed with quantifiers as follows:

\begin{align*}
    \text{not}\Big( \forall x \spc P(x) \Big) &= \exists x \spc \text{not } P(x) \\
    \text{not}\Big( \exists x \spc P(x) \Big) &= \forall x \spc \text{not } P(x).
\end{align*}

Intuitively, a property doesn't hold true for all objects exactly when that property doesn't hold for one or more of the objects, and a property isn't true for one or more of the objects exactly when it isn't true for all objects.

Nested quantifiers can be negated with this rule, as well. To negate a nested quantifier, just treat the inner quantifier-predicate pair as a single-input predicate. For example,

\begin{align*}
    \text{not }(\forall x \spc \exists y \spc P(x, y)) = \exists x \spc \text{not } (\exists y \spc P(x, y)) = \exists x \spc \forall y \spc \text{not } P(x, y).
\end{align*}

This algorithm works for any number of nested quantifiers (i.e. we could use it to compute \\ $\text{not }(\forall x \spc \exists y \spc \exists z \spc \forall w \spc P(x, y, z, w))$) because if we have $n$ quantifiers, then the predicate involving the innermost $n - 1$ quantifiers is always a single-input predicate.

\subsubsection{Essentially, all of math is expressed using quantifiers and logical operators}

We roughly define a \textit{first-order mathematical theory} to consist of

\begin{itemize}
    \item a list of \textit{axioms}, or assumptions thought of as inherently true, that are expressible by using the quantifiers $\forall$ and $\exists$ on variables (such as $x$) in conjunction with predicates and logical operators
    \item the collection of all predicates (``facts'') which are logically equivalent to the axioms.
\end{itemize}

The \textit{Zermeloâ€“Fraenkel set theory} with the \textit{axiom of choice} (abbreviated ZFC, where the ``C'' is for ``choice'') is a commonly used first-order mathematical theory. The axioms of ZFC are relatively complicated, and will not be stated here. The important point is that the axioms of ZFC are stated in accordance to the two bullet points above; they are stated completely in terms of quantifiers, predicates, and logical operators derived from \textit{and}, \textit{or}, and \textit{not}.

This may sound a bit esoteric. You may ask, ``just how much can we say with ZFC?'' The answer is: ``a lot''. Essentially all of math (calculus, real analysis, probability, statistics, linear algebra, differential equations, abstract algebra, number theory, topology, differential geometry, etc.) can be expressed in terms of ZFC. Since physics, engineering, and the other sciences are built on top of math, then the math that got humans to the moon can be derived from ZFC.

How can ZFC (and similar theories) do all of this? The answer is by building up abstraction. While mathematical constructions may always reduce down to quantificational logic, we do not in practice explicitly deal in quantificational logic all the time. Instead, sophisticated ideas are expressed by defining mathematical objects using previously defined notions, thinking about these objects in intuitive terms while still keeping the rigorous definition in mind, and proving facts (theorems) about these objects. The two ideas that most fields of math are built upon are the those of \textit{sets}, which are essentially lists, and \textit{functions}, which haven't been formally introduced yet. When you have these two concepts, you can build pretty much any theory.

\newpage

\section{Implications with sets}

\subsection*{Sets}

A \textit{set} is an unordered list of unique objects. Examples of sets include $S_1 = \{\text{grass}, \text{tree}, -1, \pi\}, S_2 = \{0, 2, 4, 6, ...\}$ and $S_3 = \{0\}$. The \textit{empty set} is the set which contains no objects, and is denoted $\emptyset$. Sets can contain finitely many objects or infinitely many. $S_1$ and $S_3$ are examples of finite sets, and $S_2$ is an example of an infinite set. Because the order of objects in a set is irrelevant, we have for example that $\{1, 2\} = \{2, 1\}$. Additionally, when we say that a set is considered to be a list of unique elements, we do not mean that a set \textit{cannot} contain multiple copies of the same element\footnote{The meaning of the word ``unique'' used in the standard terminology, ``unordered list of unique objects'', is a bit ambiguous, so it is understandable if interpreted ``unique'' to mean ''no copies allowed''.}, we just mean that those multiple copies ``don't matter''; so, for example, we have $\{1, 1, 2, 2, 3, 3 \} = \{1, 2, 3\}$.

Formally, the fact that the order of objects in a set doesn't matter is established by defining sets $S$ and $T$ to be equivalent exactly when $S$ contains all of $T$'s elements and $T$ contains all of $S$'s elements.

\subsection*{Set construction and membership}

\subsubsection*{Set construction}

The notation

\begin{align*}
    \{ x \mid P(x) \}
\end{align*}

denotes the set of objects $x$ which satisfy the property $P(x)$. The ``$\mid$'' symbol can be read as ``such that''; we read ``$\{ x \mid P(x) \}$'' out loud as ``$x$ such that $P$ of $x$''. 

Sometimes, authors prefer a colon $:$ over a vertical bar $|$ and use the notation 

\begin{align*}
    \{ x : P(x) \}
\end{align*}

instead of the above.

\subsubsection*{Set membership}

When an object $x$ is contained in a set $S$, we write $x \in S$. The symbol $\in$ is called the \textit{set membership symbol}. ``$x \in S$'' is read out loud as ``$x$ in $S$''.

\subsubsection*{Shorthand for set construction}

We define the following shorthand:

\begin{align*}
    \{ x \in S \mid P(x)\} := \{ x \mid x \in S \text{ and } P(x)\}.
\end{align*}

You would read ``$\{ x \in S \mid P(x)\}$'' out loud as ``$x$ in $S$ such that $P(x)$''.

\newpage

\subsection*{Implications}

We now define the \textit{implication} operator $\implies$, which is an operation that accepts two predicates as input and produces a predicate as output. (Recall, a predicate is a statement that is either true or false and that makes use of variables to represent concepts). Many authors introduce $\implies$ immediately after \textit{or}, \textit{and}, and \textit{or}; we introduce it now because it is best understood in the context of ``for all'' statements that involve sets.

This operator is read out loud as ``implies'', and is defined as follows:

\begin{align*}
    P \implies Q := (\text{not } P) \text{ or } Q.
\end{align*}

The argument before the $\implies$ symbol is referred to as the \textit{hypothesis}, and the argument after the $\implies$ symbol is referred to as the \textit{conclusion}. Here is the truth table for $\implies$.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P \implies Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $T$        \\ \hline
    $F$ & $F$ & $T$        \\ \hline
    \end{tabular}
\end{table}

Beware: just because $\implies$ is read as ``implies'' does \textit{not} mean that it functions in the way that you might expect. A better name for $\implies$ would be ``primitive implies'', since $\implies$ is defined \textit{for the purpose of being used after a ``for all'' quantifier}. ``For all'' quantifications that involve $\implies$ in the following way are what correspond to the English language meaning of ``implies'':

\begin{align*}
    \forall x \spc x \in S \implies P(x).
\end{align*}

We might express the above in English as ``$x$ being in $S$ implies $P(x)$''. The key difference between this informal sentence in English and the statement in mathematical notation is that the English sentence lacks a ``for all'' quantification. So, if the English sentence were truly correct, it would be ``For all $x$, $x$ being in $S$ implies $P(x)$''. In the mathematical notation, the ``$\forall x$'' technically indicates an inspection of \textit{all} entities (such as ``grass'', the function $f$ defined by $f(x) = 3x^2$, $-1$, etc.). We restrict our attention to the set $S$ by using $\implies$, so that it is only possible for the predicate $(x \in S \implies P(x))$ to be false when $x \in S$.

The following thought experiment helps further illustrate the idea behind $\implies$. Suppose that you suspect that all squares are rectangles (you are right). To test your belief, you set out to test every single geometric shape, one by one. (Looking at every single geometric shape corresponds to the ``$\forall x$'' part of the above line. You have no control over the fact that, in full formality, you are always considering \textit{all} objects $x$). You set $P(x) := (\text{$x$ is a square})$ and $Q(x) := (\text{$x$ is a rectangle})$. First, you inspect a square, and determine that it is indeed a rectangle. (This corresponds to the first row of the truth table; $P(x)$ is true and $Q(x)$ is true for this particular $x$, so your theory holds, at least so far). Next, you look at a circle. You're not interested in circles- you're interested in squares! Knowing whether or not the circle is a rectangle is irrelevant. (This corresponds to the last two rows of the truth table; for any $x$, whenever $P(x)$ is false, then $P(x) \implies Q(x)$ is true). After an infinite amount of time, you have tested all squares and determined that they are all rectangles, so your theory stands; that is, the predicate that is the ``for all'' statement evaluates to true. If there \textit{had} been a single square that wasn't a rectangle (there wasn't), then the predicate inside the ``for all'' would have evaluated as false for that square. (This corresponds to the second row of the truth table). This would make the entire ``for all'' statement false.

\subsubsection{``Necessary'' and ``sufficient''}

Let $P$ and $Q$ be predicates, and consider the proposition $\forall x \spc P(x) \implies Q(x)$. Due to the reasoning ``if $P$ happened then $Q$ must have happened'', $Q$ is said to be a \textit{necessary condition} for $P$. Because of the reasoning ``one of ways to make $Q$ happen is to make $P$ happen'', $P$ is said to be a \textit{sufficient} condition for $Q$. 

\subsubsection{Quantifiers with set membership}

We define

\begin{align*}
    &\forall x \in S \spc P(x) := \forall x \spc x \in S \implies P(x) \\
    &\exists x \in S \spc P(x) := \exists x \in S \text{ and } P(x).
\end{align*}

The first line was motivated in the section before the previous section. The second line is probably easier to understand than the first, since it doesn't involve $\implies$.

The negations of the above newly defined expressions are what you expect:

\begin{align*}
    &\text{not}\Big( \forall x \in S \spc P(x) \Big) = \exists x \in S \spc \text{not } P(x) \\
    &\text{not}\Big( \exists x \in S \spc P(x) \Big) = \forall x \in S \spc \text{not } P(x).
\end{align*}

This is because a slightly more general versions of the above facts hold. (To obtain the above from the below, substitute $P(x) = (x \in S)$ and $Q(x) = P(x)$ into the below).

\begin{align*}
    &\text{not}\Big( \forall x \spc P(x) \text{ and } Q(x) \Big) = \exists x \spc P(x) \text{ and } (\text{not } Q(x)) \\
    &\text{not}\Big( \exists x \spc P(x) \text{ and } Q(x) \Big) = \forall x \spc P(x) \text{ and } (\text{not } Q(x)).
\end{align*}

%These more general facts are useful when objects with a particular property are being considered. For example, consider all triangles in the plane. Given a right triangle $T$, let $a_T, b_T$, and $c_T$ be the lengths of the sides of $T$, with $c_T$ being the length of the hypotenuse. The Pythagorean theorem states\footnote{Because the ``for all statement'' considers all triangles $T$, not just the right triangles, the definition of $c_T$ does not make sense for non-right triangles, since non-right triangles don't have a hypotenuse. We could improve the definition of $c_T$ so it applies to all right triangles and is still the length of the hypotenuse when $T$ is a right triangle. However, it is actually fine for $c_T$ to be undefined for non-right triangles $T$, because an \textit{and} statement evaluates to false whenever either argument (such as ``$T$ is a right triangle'') is false. So, in the general case, allowing for this so-called \textit{short-circuit} interpretation of \textit{and} allows us to use predicates $Q(x)$ that are only defined when $P(x)$ is true.} $(\forall T \spc \text{$T$ is a right triangle} \text{ and } a_T^2 + b_T^2 = c_T^2)$. (Note: this ``for all'' statement does \textit{not} state that all triangles are right triangles satisfying the Pythagorean theorem! It states that all right triangles satisfy $a_T^2 + b_T^2 = c_T^2$). The negation of the Pythagorean theorem, using the above, is then $(\exists T \spc \text{$T$ is a right triangle} \text{ and } a_T^2 + b_T^2 \neq c_T^2)$. Since the Pythagorean theorem is true, this means that it is \textit{not} the case that there is a right triangle $T$ for which $a_T^2 + b_T^2 \neq c_T^2$. 

Here's a proof of the first line of the more general statement; the proof of the second line is similar.

\begin{align*}
    \text{not}\Big( \forall x \spc P(x) \text{ and } Q(x) \Big)
    =
    \exists x \spc \text{not}\Big( \spc P(x) \text{ and } Q(x) \Big)
    =
    \exists x \spc (\text{not } P(x)) \text{ or } (\text{not } Q(x))
    =
    \exists x \spc P(x) \text{ and } (\text{not } Q(x)).
\end{align*}

The last above logical equality follows because, for propositions $P$ and $Q$, we have $(P \text{ or } Q) = (P \text{ and } (\text{not } Q))$. This can be checked by truth table; it can also be understood intuitively: ``if $P$ or $Q$ happens, and $Q$ doesn't happen, then $P$ must happen''.

\subsubsection{Common shorthand}

\begin{itemize}
    \item When people write something of the form ``$\forall x \spc P(x) \implies Q(x)$'', they mean \\ ``$\Big(\forall x \spc P(x) \implies Q(x)\Big) = T$''. 
    \item When people write something of the form ``$P(x) \implies Q(x)$'', they really should have written ``$\forall x \spc P(x) \implies Q(x)$''. An extremly common example of this shorthand is ``$x \in S \implies P(x)$''.
    \begin{itemize}
        \item This shorthand has a verbal equivalent: ``If $P(x)$, then $Q(x)$''. The verbal equivalent is not considered bad notation, however.
    \end{itemize}
    \item Combining both shorthand styles is extremely common in proof writing. You will often see a proof that contains a sentence of the form ``$P(x) \implies Q(x) \implies R(x)$''.
\end{itemize}

\subsubsection{Converses}

Given predicates $P$ and $Q$, consider the implication $P \implies Q$. The \textit{converse} to this implication is the implication $Q \implies P$. Please note that $(P \implies Q) = (Q \implies P)$ is a false statement! On the level of the English language, this means that $\Big( \forall x \spc P(x) \implies Q(x) \Big) = \Big( \forall x \spc Q(x) \implies P(x) \Big)$ is a false statement. For example, ``whenever it rains, there are clouds'', but it is not true that ``whenever there are clouds, it rains''!

\subsection*{``If and only if''}

We define one more operator on predicates, the \textit{bidirectional implication} operator, denoted $\iff$. Given predicates $P$ and $Q$, we define

\begin{align*}
    P \iff Q := \Big( (P \implies Q) \text{ and } (Q \implies P) \Big).
\end{align*}

Here's the truth table for $\iff$.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P \iff Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $F$        \\ \hline
    $F$ & $F$ & $T$        \\ \hline
    \end{tabular}
\end{table}

The operator $\iff$ is spoken aloud as ``if and only if'', and is abbreviated in writing as ``iff''. In the context of the bidirectional implication $P \iff Q$, the implication $P \implies Q$ is referred to as the \textit{forward implication} and its converse $Q \implies P$ is referred to as the \textit{reverse implication}. Note that $\iff$ is symmetric in the sense that $(P \iff Q) = (Q \iff P)$.

The English language interpretation of $\iff$ is similar to to the English language interpretation of $\implies$: the mathematical version of what someone really means when they say ``$P(x)$ if and only if $Q(x)$'' is $\forall x \spc P(x) \iff Q(x)$. Again, we see the difference between full mathematical formalism and language is that language often omits the ``for all''.

Many theorems in math state that a certain ``if and only if'' predicate is true. More specifically, such theorems usually state that some property of an object manifests if and only if another property of that object also manifests. So, when you see a theorem about an \textit{equivalent condition} or an \textit{equivalent definition}, there is an ``if and only if'' statement at play. All definitions in math are also automatically ``if and only if'' statements. There is, however, the common misleading convention of writing definitions using the word ``if'' (for example: ``We say $x$ has property $P(x)$ \textit{if} $Q(x)$''; the ``if'' should really be an ``iff'').

A neat fact is that $=$ and $\iff$ are the same operator. One way to verify this is by checking that the truth tables of $=$ and $\iff$ are the same.

\subsection*{The contrapositive and proof by contradiction}

For predicates $P$ and $Q$, the following logical identity is true:

\begin{align*}
    (P \implies Q) = ((\text{not } Q) \implies (\text{not } P)).
\end{align*}

The right-hand side is called the \textit{contrapositive} of the left-hand side.

You could verify this identity by using truth tables. Here is a nicer proof that uses the facts $(P \text{ or } Q) = (Q \text{ or } P)$ and $Q = \text{not}(\text{not } Q)$.

\begin{align*}
    (P \implies Q) &= ((\text{not } P) \text{ or } Q ) 
    = (\text{not}(\text{not } Q) \text{ or } (\text{not } P))
    = ((\text{not } Q) \implies (\text{not } P)).
\end{align*}

\subsubsection{Proof by contradiction}

Suppose we want to prove $P \implies Q$. One way to do so is to use \textit{proof by contradiction}. Proof by contraction proceeds as follows. Assume $P$ is true, and suppose that $Q$ is false. Then if, as a direct result of supposing $Q$ to be false, we reach a logical impossibility, such as $1 = 0$, we know $Q$ must be true.

Formally, proof by contradiction is an application of the contrapositive. The first step in a proof by contradiction of writing ``assume $P$ is true'' serves no formal mathematical purpose, and is really just a reminder of the statement that will be contradicted. The next step, which is the first step that formally matters, is to assume $(\text{not } Q)$; this corresponds to the hypothesis of the contrapositive. Lastly, the contradiction (such as $1 = 0$) achieved at the end of the proof is actually always logically equivalent to $(\text{not } P)$ when the full context is considered. Proof by contradiction is just a more verbose way of proving the contrapositive, $((\text{not } Q) \implies (\text{not } P))$.

For some proofs, using the contrapositive in its raw logical form is most clear; for others, using the verbal format of proof by contradiction is more clear.

\newpage

\section{Sets}

\subsection*{Set equality, subsets}

Let $S$ and $T$ be sets. We define $S$ and $T$ to be \textit{equal} iff $\forall x \spc x \in S \iff x \in T$.

We say $T$ is a \textit{subset} of $S$ iff $\forall x \in S \spc x \in T$. When $T$ is a subset of $S$, we write $T \subseteq S$. 

\textbf{note similarity to $\leq$ notation}

Note that, for all sets $S$, the empty set is a subset of $S$, $\emptyset \subseteq S$, and $S$ is a subset of itself, $S \subseteq S$. 

When $T \subseteq S$ and $T \neq S$, we write $T \subsetneq S$. (Some authors write $T \subset S$ for this condition, but this is confusing because other authors write $T \subset S$ to mean $T \subseteq S$. Avoid the notation $T \subset S$).

\subsection*{There is no universal set in ZFC}

It seems natural that there would be a set which contains every other set, but not itself; it seems natural that there would be a so-called ``universal set''. However, this is not possible in ZFC. Why? Suppose there did exist such a ``universal set'' $U$. Since $U$ contains all sets, $U$ must contain itself: $U \in U$. But, by definition, $U$ was assumed to not contain itself: $U \notin U$. This is a contradiction\footnote{This paradox can be seen as motivation for the definition of a \textit{class}. Essentially, classes are set-like objects that are not allowed to contain themselves. There \textit{does} exist a class that contains all sets, but not itself}. Thus, there does not exist a ``set of all sets''.

\subsection*{Common sets}

We define notation for many common infinite sets.

\begin{align*}
    \N &:= \text{``the natural numbers''} = \{0, 1, 2, 3, ...\} \\
    \Z &:= \text{``the integers''} = \{..., -3, -2, -1, 0, 1, 2, 3, ... \} \\
    \Q &:= \text{``the rational numbers''} = \Big\{ \frac{n}{m} \mid n, m \in \Z \Big\} \\
    \R &:= \text{``the real numbers''} = \{ \text{all limits of sequences of rational numbers} \} \\
    \C &:= \text{``the complex numbers''} = \{ a + b \sqrt{-1} \mid a, b \in \R \}
\end{align*}

We have $\N \subsetneq \Z \subsetneq \Q \subsetneq \R \subsetneq \Q$.

\subsection*{Indexing sets}

Let $S$ be a set. An \textit{indexing set} of $S$ is a set $I$ that is thought of as ``labeling'' the elements $S$. Any set can be technically be used as an indexing set, though we tend to use sets of numbers as indexing sets. The set of elements in $S$ indexed by $I$ is written as

\begin{align*}
    \{ x_\alpha \in S \mid \alpha \in I \}.
\end{align*}

We use a Greek letter such as ``$\alpha$'' when the size of the indexing set is unspecified; an indexing set can be either finite or infinite. When the indexing set is finite, we use a normal Roman letter such as ``$i$''.

\subsection*{Union and intersection}

We now define ways to build sets from a collection\footnote{The word ``collection'' is often used to mean ``set of sets''.} of sets. Let $I$ be an indexing set, and consider the collection $C = \{S_\alpha \mid \alpha \in I \}$.

The \textit{union} of the sets in $C$ is defined to be the result of ``adding'' all the sets in $C$ together:

\begin{align*}
    \bigcup_{\alpha \in I} S_\alpha := \{ x \mid \exists \alpha \in I \spc x \in S_\alpha \}.
\end{align*}

The \textit{intersection} of the sets in $C$ is defined to be the result of considering only the elements that are common to every single set in $C$:

\begin{align*}
    \bigcap_{\alpha \in I} S_\alpha := \{ x \mid \forall \alpha \in I \spc x \in S_\alpha \}.
\end{align*}

\subsection*{Set difference and set complement}

\subsubsection*{Set difference}

The union of two sets, introduced above, is analogous to addition of numbers. The following \textit{set difference} operation is analogous to the subtraction of one number from another:

\begin{align*}
    S - T := \{ x \mid x \in S \text{ and } x \notin T \}.
\end{align*}

\subsubsection*{Set complement}

Sometimes, it is convenient to consider some set $U$ to be the ``largest possible set''. For example, it is common to set $U$ to be the unit interval, $U = [0, 1]$, when proving theorems from calculus. In this sort of situation, the \textit{set complement (in $U$)} of some other set $S$ is denoted $S^C$ and is defined to be $S^C := U - S$.

\subsection*{Ordered pairs}

Roughly speaking, ordered pairs are objects notated in the form ``$(x, y)$'' (an ordered pair consists of two objects, such as $x$ and $y$, that are separated by a comma and are enclosed in parentheses) that are subject to the characterizing property $(x_1, y_1) = (x_2, y_2)$ iff $x_1 = y_1$ and $x_2 = y_2$. 

Formally, we can \textit{construct} the apparatus of ordered pairs by the definition

\begin{align*}
    (x, y) := \{x, \{y\}\}.
\end{align*}

The ordered pairs of this definition satisfy the characterizing property of ordered pairs mentioned above.

\subsection*{Cartesian product}

\subsubsection*{$n$th Cartesian product}

$((S \times S) \times S) \times ...) \times S$ vs. ``$S \times ... \times S$''

\newpage

\section{Relations}

inverse relations

inverse functions <=> bijection

\subsection*{Equivalence relations}

\begin{defn}
    (Equivalence relation).
\end{defn}

\begin{defn}
\label{ch::logic_pf_fns::defn::quotient_set}

    (Quotient set).
\end{defn}

\subsection*{Functions}

Latin meanings of injective and surjective.

\begin{defn}
    (Function).
    
    Functions are also commonly called \textit{maps}, or \textit{mappings}.
    
    domain, codomain, range
\end{defn}

\begin{defn}
    (Uniqueness).
    
    $\exists !x \spc P(x) := (\exists x_0 \spc P(x_0)) \text{ and } (\forall x \spc P(x) \implies x = x_0)$.
\end{defn}

\begin{remark}
    (Well-definedness and uniqueness).
    
    What well-definedness refers to
    
    Any property which is unique is the output of a well-defined function
\end{remark}

\begin{defn}
    (One-to-one, injection).
\end{defn}

\begin{defn}
    (Onto, surjection).
\end{defn}

\begin{defn}
    (Bijection).
\end{defn}

left-inverse, right-inverse, relation to -jectivities

addition of fns, scaling of fns?, image-set

preimages (not neccess fns)

\begin{defn}
\label{ch::logic_pf_fns::defn::inverse_fn}

    (Inverse function).
\end{defn}

\begin{theorem}
\label{ch::logic_pf_fns::thm::invertible_iff_bijection}
    (Invertible iff bijection).
\end{theorem}

\subsection*{Cardinality of sets}

It was mentioned earlier that sets can be finite or infinite. We formally define

\begin{theorem}
    (Left inverse and right inverse implies two-sided inverse). Let $X$ and $Y$ be sets, and let $f:X \rightarrow Y$ be a function. A \textit{left-inverse} of $f$ is a function $\ell:Y \rightarrow X$ such that $\ell \circ f = I_X$, where $I_X$ is the identity on $X$. A \textit{right-inverse} of $f$ is a function $r:Y \rightarrow X$ such that $f \circ r = I_Y$, where $I_Y$ is the identity on $Y$.
    
    If $f:X \rightarrow Y$ has a left-inverse $\ell$ and a right-inverse $r$, then they must be equal, and we denote them by $f^{-1} := \ell = r$.
\end{theorem}

\begin{proof}
    Let $\ell$ and $r$ be left- and right- inverses of $f$, respectively. Then by associativity of function composition, $\ell \circ f \circ r = \ell \circ I_Y = I_X \circ r$. Therefore $\ell = r$.
\end{proof}

\begin{defn}
    (Proof by induction).
    
    Defn?
\end{defn}
