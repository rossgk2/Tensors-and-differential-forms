\chapter{Logic, proofs, and functions}
\label{ch::logic_pf_fns}

\subsection*{Notation for definitions}

In this book, we use $:=$ whenever an equality is enforced to be true by definition (e.g. ``define a function $f$ by $f(x) := x^2$''), and use $=$ whenever an equality follows from further reasoning (e.g. since $f(x) := x^2$ we have $f(x) = (x^2)^2 = x^4$).

\section{Propositional logic}

A \textit{proposition} is a statement that is either true or false; an example is ``giraffes have ears''.

We can denote propositions with letters, and say things such as ``let $P$ be a proposition''. When a proposition $P$ is true, we write $P = T$. The $=$ symbol is used here to denote \textit{logical equality}, and the $T$ denotes ``truth''. Similarly, we write $P = F$ when $P$ is false.

\subsection*{Logical operators}

There are three fundamental operations on propositions that are used to build more complicated propositions from simpler ones: there are the two-argument (binary) operators \textit{and} and \textit{or} and the one-argument (unary) operator \textit{not}.

The operators \textit{and}, \textit{or}, and \textit{not} act on propositions $P$ and $Q$ as is expressed in the following ``truth tables''.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P$ and $Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $F$        \\ \hline
    $F$ & $F$ & $F$        \\ \hline
    \end{tabular}
    \quad
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P$ or $Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $T$        \\ \hline
    $F$ & $T$ & $T$        \\ \hline
    $F$ & $F$ & $F$        \\ \hline
    \end{tabular}
    \quad
    \begin{tabular}{|l|l|}
    \hline
    $P$ & $\text{not } P$ \\ \hline
    $T$ & $F$      \\ \hline
    $F$ & $T$      \\ \hline
    \end{tabular}
\end{table}

So, $P \textit{ and } Q$ evaluates to true only when both $P$ and $Q$ are true, $P \textit{ or } Q$ evaluates to true whenever either of $P$ or $Q$ is true, and $\textit{not } P$ evaluates to the ``opposite'' of $P$. Note that \textit{or} is not the same as \textit{exclusive or}, also called \textit{xor}, which evaluates to true whenever exactly one of $P, Q$ is true.

Here are some examples: we have $\Big((3 > 4)$ and $(\text{every rectangle is a square}) \Big) = F$ and \\ $\Big((5 > -3) \text{ or } (2 > 100)\Big) = T$.

\subsubsection*{DeMorgan's laws}

You can check by using truth tables for propositions $P$ and $Q$, we have the following two logical identities, called \textit{DeMorgan's laws}:

\begin{align*}
    &\text{not}(P \text{ and } Q) = (\text{not } P) \text{ or } (\text{not } Q) \\
    &\text{not}(P \text{ or } Q) = (\text{not } P) \text{ and } (\text{not } Q).
\end{align*}

Sometimes, symbols are used to represent \textit{and}, \textit{or}, and \textit{not}: $\wedge$ denotes \textit{and}, $\vee$ denotes \textit{or}, and either $\neg$ or $\sim$ denotes \textit{not}. We will not use these symbols.

\newpage

\section{Predicate logic}

A \textit{predicate} is a statement involving one or more ``variables'' such that the statement is a proposition when the variables take on concrete values. An example of a predicate with a single variable is ``giraffes have $x$ many ears''.

Just as we could with propositions, we can denote predicates with letters. For example, we can define a predicate $Q$ by $Q(x) = (\text{the sky is blue at time $x$})$. When we substitute in concrete meaning for the entities in a predicate, we obtain a proposition; for example, $Q(8)$ is the proposition ``giraffes have 8 ears''.

\subsubsection*{Quantifiers}

In predicate logic, the symbols $\forall$ (read as ``for all'') and $\exists$ (read as ``there exists'') are used to produce propositions from single-input predicates. The symbol $\forall$ is known as the \textit{universal quantifier}, and the symbol $\exists$ is known as the \textit{existential quantifier}.

\begin{align*}
    \forall x \spc P(x) &\qquad \text{is the proposition which states ``for all $x$, $P(x) = T$''} \\
    \exists x \spc \spc Q(x) &\qquad \text{is the proposition which states ``there exists an $x$ such that $Q(x) = T$''}.
\end{align*}

We see that $\forall x \spc P(x)$ is true exactly when $P(x)$ is true for all $x$, and that $\exists x \spc Q(x)$ is true exactly when $Q(x)$ is true for one or more $x$. In this sense, $\forall$ is similar to \textit{and} and $\exists$ is similar to \textit{or}.

Here's an example: the proposition $(\forall x \spc x > 3)$ is false, while the proposition $(\exists x \spc x > 3)$ is true\footnote{Technically, these propositions aren't really sensible since we haven't specified that the $x$'s involved can only be numbers and not other entities.}.

\subsubsection{Nested quantifers}

In the last section, we saw that applying a quantifier-variable pair (such as $\forall x$) to a single-input predicate produced a proposition. We now consider what happens when we apply a quantifier-variable pair to a two-input predicate $P(x, y)$. There are four possible quantifier-variable pairs we can apply to $P(x, y)$:

\begin{align*}
    \forall x \spc P(x, y) \\
    \exists x \spc P(x, y) \\
    \forall y \spc P(x, y) \\
    \exists y \spc P(x, y).
\end{align*}

Now we claim that while $P(x, y)$ depends on both $x$ and $y$, the expression $\forall x \spc P(x, y)$ depends only on $y$. The idea is that since the $\forall x$ specifies an iteration over all $x$, the only variable in $\forall x \spc P(x, y)$ that is not being modified by a quantifier is $y$. (This intuition is similar to the principle that a sum such as $\sum_i a_{ij}$ depends on $j$ but not $i$, since $i$ is being ``summed over'' but $j$ is not). Thus, we can define a single-input predicate $Q_1$ by $Q_1(y) := \forall x \spc P(x, y)$.

Similar statements can be made for the other lines, so we can define single-input predicates $Q_1, Q_2, Q_3, Q_4$ as follows:

\begin{align*}
    Q_1(y) := \forall x \spc P(x, y) \\
    Q_2(y) := \exists x \spc P(x, y) \\
    Q_3(x) := \forall y \spc P(x, y) \\
    Q_4(x) := \exists y \spc P(x, y).
\end{align*}

Since we can apply quantifier-variable pairs to the single-input predicates $Q_1(y), Q_2(y), Q_3(y), Q_4(y)$, we obtain the following propositions:

\begin{align*}
    &\forall x \spc (\forall y \spc P(x, y)) \\
    &\forall x \spc (\exists y \spc P(x, y)) \\
    &\exists x \spc (\forall y \spc P(x, y)) \\
    &\exists x \spc (\exists y \spc P(x, y)).
\end{align*}

These propositions involve so-called \textit{nested quantifiers}. In practice, we omit the inner parentheses and use syntax such as $\forall x \spc \exists y \spc P(x, y)$ rather than syntax such as $\forall x \spc (\exists y \spc P(x, y))$.

\subsubsection*{Quantificational identities}

It's useful to know that when two quantifier-variable pairs are nested and the quantifiers are the same, we have the following commutative property:

\begin{align*}
    &\forall x \spc \forall y \spc P(x, y) = \forall y \spc \forall x \spc P(x, y) \\
    &\exists x \spc \exists y \spc Q(x, y) = \exists y \spc \exists x \spc Q(x, y).
\end{align*}

There is also a shorthand notation for situations in which we have two nested quantifiers of the same type:

\begin{align*}
    &\forall x, y \spc P(x, y) := \forall x \spc \forall y \spc P(x, y) \\
    &\exists x, y \spc P(x, y) := \exists x \spc \exists y \spc P(x, y).
\end{align*}


\subsubsection*{Negating quantifications}

The \textit{not} operator applies to single-input predicates constructed with quantifiers as follows:

\begin{align*}
    \text{not}\Big( \forall x \spc P(x) \Big) &= \exists x \spc \text{not } P(x) \\
    \text{not}\Big( \exists x \spc P(x) \Big) &= \forall x \spc \text{not } P(x).
\end{align*}

Intuitively, a property doesn't hold true for all objects exactly when that property doesn't hold for one or more of the objects, and a property isn't true for one or more of the objects exactly when it isn't true for all objects.

Nested quantifiers can be negated with this rule, as well. To negate a nested quantifier, just treat the inner quantifier-predicate pair as a single-input predicate. For example,

\begin{align*}
    \text{not }(\forall x \spc \exists y \spc P(x, y)) = \exists x \spc \text{not } (\exists y \spc P(x, y)) = \exists x \spc \forall y \spc \text{not } P(x, y).
\end{align*}

This algorithm works for any number of nested quantifiers (i.e. we could use it to compute \\ $\text{not }(\forall x \spc \exists y \spc \exists z \spc \forall w \spc P(x, y, z, w))$) because if we have $n$ quantifiers, then the predicate involving the innermost $n - 1$ quantifiers is always a single-input predicate.

\subsubsection{Essentially, all of math is expressed using predicate logic}

We roughly define a \textit{first-order mathematical theory} to consist of

\begin{itemize}
    \item a list of \textit{axioms}, or assumptions thought of as inherently true, that are expressible by using the quantifiers $\forall$ and $\exists$, predicates, and logical operators derived from \textit{and}, \textit{or}, and \textit{not}
    \item the collection of all propositions (``facts'') which are logically equivalent to the axioms.
\end{itemize}

The \textit{Zermeloâ€“Fraenkel set theory} with the \textit{axiom of choice} (abbreviated ZFC, where the ``C'' is for ``choice'') is a commonly used first-order mathematical theory. The axioms of ZFC are relatively complicated, and will not be stated here. The important point is that the axioms of ZFC are stated in accordance to the two bullet points above; they are stated completely in terms of quantifiers, predicates, and logical operators derived from \textit{and}, \textit{or}, and \textit{not}.

This may sound a bit esoteric. You may ask, ``just how much can we say with ZFC?'' The answer is: ``a lot''. Essentially all of math (calculus, real analysis, probability, statistics, linear algebra, differential equations, abstract algebra, number theory, topology, differential geometry, etc.) can be expressed in terms of ZFC. Since physics, engineering, and the other sciences are built on top of math, then the math that got humans to the moon can be derived from ZFC.

How can ZFC (and similar theories) do all of this? The answer is by building up abstraction. While mathematical constructions may always reduce down to quantificational logic, we do not in practice explicitly deal in quantificational logic all the time. Instead, sophisticated ideas are expressed by defining mathematical objects in terms of previously defined notions, then thinking about these objects in intuitive terms while still keeping the rigorous definitions in mind, and then proving facts (theorems) about these objects. The two ideas that most fields of math are built upon are the those of \textit{sets}, which are essentially lists, and \textit{functions}, which haven't been formally introduced yet. When you have these two concepts, you can build pretty much any theory.

\newpage

\section{Implications with sets}

\subsection*{Sets}

A \textit{set} is an unordered list of unique objects. Examples of sets include $S_1 = \{\text{grass}, \text{tree}, -1, \pi\}, S_2 = \{0, 2, 4, 6, ...\}$ and $S_3 = \{0\}$. The \textit{empty set} is the set which contains no objects, and is denoted $\emptyset$. Sets can contain finitely many objects or infinitely many. $S_1$ and $S_3$ are examples of finite sets, and $S_2$ is an example of an infinite set. Because the order of objects in a set is irrelevant, we have for example that $\{1, 2\} = \{2, 1\}$. Additionally, when we say that a set is considered to be a list of unique elements, we do not mean that a set \textit{cannot} contain multiple copies of the same element\footnote{The meaning of the word ``unique'' used in the standard terminology, ``unordered list of unique objects'', is a bit ambiguous, so it is understandable if interpreted ``unique'' to mean ''no copies allowed''.}, we just mean that those multiple copies ``don't matter''; so, for example, we have $\{1, 1, 2, 2, 3, 3 \} = \{1, 2, 3\}$.

Formally, the fact that the order of objects in a set doesn't matter is established by defining sets $S$ and $T$ to be equivalent exactly when $S$ contains all of $T$'s elements and $T$ contains all of $S$'s elements.

\subsection*{Set construction and membership}

\subsubsection*{Set construction}

The notation

\begin{align*}
    \{ x \mid P(x) \}
\end{align*}

denotes the set of objects $x$ which satisfy the predicate $P(x)$. The ``$\mid$'' symbol can be read as ``such that''; we read ``$\{ x \mid P(x) \}$'' out loud as ``$x$ such that $P$ of $x$''. 

Sometimes, authors prefer a colon $:$ over a vertical bar $|$ and use the notation 

\begin{align*}
    \{ x : P(x) \}
\end{align*}

instead of the above.

\subsubsection*{Set membership}

When an object $x$ is contained in a set $S$, we write $x \in S$. The symbol $\in$ is called the \textit{set membership symbol}. ``$x \in S$'' is read out loud as ``$x$ in $S$''.

\subsubsection*{Shorthand for set construction}

We define the following shorthand:

\begin{align*}
    \{ x \in S \mid P(x)\} := \{ x \mid x \in S \text{ and } P(x)\}.
\end{align*}

You would read ``$\{ x \in S \mid P(x)\}$'' out loud as ``$x$ in $S$ such that $P(x)$''.

\newpage

\subsection*{Implications with sets}

We now define the \textit{implication} operator $\implies$, which is an operation that accepts two propositions as input and produces a proposition as output. Many authors introduce $\implies$ immediately after \textit{or}, \textit{and}, and \textit{or}; we introduce it now because it is best understood in the context of ``for all'' statements that involve sets.

This operator is read out loud as ``implies'', and is defined on propositions $P$ and $Q$ as follows:

\begin{align*}
    P \implies Q := (\text{not } P) \text{ or } Q.
\end{align*}

The argument before the $\implies$ symbol is referred to as the \textit{hypothesis}, and the argument after the $\implies$ symbol is referred to as the \textit{conclusion}. Here is the truth table for $\implies$.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P \implies Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $T$        \\ \hline
    $F$ & $F$ & $T$        \\ \hline
    \end{tabular}
\end{table}

Beware: just because $\implies$ is read as ``implies'' does \textit{not} mean that it functions in the way that you might expect. A better name for $\implies$ would be ``primitive implies'', since $\implies$ is defined \textit{for the purpose of being used after a ``for all'' quantifier}. ``For all'' quantifications that involve $\implies$ in the following way are what correspond to the English language meaning of ``implies'':

\begin{align*}
    \forall x \spc P(x) \implies Q(x).
\end{align*}

We might express the above in English as ``$P$ implies $Q$''. The key difference between this informal sentence in English and the statement in mathematical notation is that the English sentence lacks a ``for all'' quantification. So, if the English sentence were truly correct, it would be ``For all $x$, $P(x)$ implies $Q(x)$''. In the mathematical notation, the ``$\forall x$'' technically indicates an inspection of \textit{all} entities (such as ``grass'', the function $f$ defined by $f(x) = 3x^2$, $-1$, etc.). We restrict our attention to the set of entities for which $P$ is satisfied by using $\implies$; it is only possible for the predicate $(P(x) \implies Q(x))$ to be false when $P(x)$ is true.

The following thought experiment helps further illustrate the idea behind $\implies$. Suppose that you suspect that all squares are rectangles (you are right). To test your belief, you set out to test every single geometric shape, one by one. (Looking at every single geometric shape corresponds to the ``$\forall x$'' part of the above line. You have no control over the fact that, in full formality, you are always considering \textit{all} objects $x$). You set $P(x) := (\text{$x$ is a square})$ and $Q(x) := (\text{$x$ is a rectangle})$. First, you inspect a square, and determine that it is indeed a rectangle. (This corresponds to the first row of the truth table; $P(x)$ is true and $Q(x)$ is true for this particular $x$, so your theory holds, at least so far). Next, you look at a circle. You're not interested in circles- you're interested in squares! Knowing whether or not the circle is a rectangle is irrelevant. (This corresponds to the last two rows of the truth table; for any $x$, whenever $P(x)$ is false, then $P(x) \implies Q(x)$ is true). After an infinite amount of time, you have tested all squares and determined that they are all rectangles, so your theory stands; that is, the predicate that is the ``for all'' statement evaluates to true. If there \textit{had} been a single square that wasn't a rectangle (there wasn't), then the predicate inside the ``for all'' would have evaluated as false for that square. (This corresponds to the second row of the truth table). This would make the entire ``for all'' statement false.

\subsubsection*{Bounded quantifiers}

Motivated by the above, we define

\begin{align*}
    &\forall x \in S \spc Q(x) := \forall x \spc x \in S \implies Q(x) \\
    &\exists x \in S \spc Q(x) := \exists x \spc x \in S \text{ and } Q(x).
\end{align*}

This new syntax provides an easy way to use quantifiers that effectively only consider objects that satisfy the predicate $(x \in S)$. For example, the above claim that all squares are rectangles would be written as 

\begin{align*}
    \forall x \in (\text{set of all squares}) \spc \text{$x$ is a rectangle}.
\end{align*}

The notations $\forall x \in S$ and $\exists x \in S$ are \textit{bounded quantifiers} because they effectively restrict, i.e. bound, the scope of quantification. 

\subsubsection*{Negation of an implication}

We have the following negation rule for implication:

\begin{align*}
    \text{not}(P \implies Q) = \Big(\text{not}(\text{not } P) \text{ or } Q)\Big) = \Big(P \text{ and } (\text{not } Q)\Big).
\end{align*}

This implies the following more intuitive fact:

\begin{align*}
    \text{not}\Big(\forall x \spc P(x) \implies Q(x) \Big) = \Big(\exists x \spc P(x) \text{ and } (\text{not } Q(x))\Big).
\end{align*}

The wordy explanation is that if it is not the case that $P(x)$ implies $Q(x)$ for all $x$, then there must be some $x$ satisfying $P(x)$ that does not also satisfy $Q(x)$.

Some ``thematic inverses'' to the above facts are the following:

\begin{align*}
    \text{not}(P \text{ and } Q) = \Big((\text{not } P) \text{ or } (\text{not } Q)\Big) = \Big(P \implies (\text{not } Q)\Big) = \Big(Q \implies (\text{not } P)\Big).
\end{align*}

\subsubsection*{Negating bounded quantifiers}

The negations of bounded quantifications are what you expect:

\begin{align*}
    &\text{not}\Big( \forall x \in S \spc P(x) \Big) = \exists x \in S \spc \text{not } P(x) \\
    &\text{not}\Big( \exists x \in S \spc P(x) \Big) = \forall x \in S \spc \text{not } P(x).
\end{align*}

Use the facts $\text{not}(P \implies Q) = (P \text{ and } (\text{not } Q))$ and $\text{not}(P \text{ and } Q) = (P \implies (\text{not } Q))$ from the ``Negation of an implication'' section to prove this.

\newpage

\subsection*{More about implications}

\subsubsection*{Converses}

Given propositions $P$ and $Q$, consider the implication $P \implies Q$. The \textit{converse} to this implication is the implication $Q \implies P$. Please note that $(P \implies Q) = (Q \implies P)$ is not true for all $P, Q$! For example, it is true that ``whenever it rains, there are clouds'', but it is not true that ``whenever there are clouds, it rains''!

\subsubsection*{The contrapositive}

For propositions $P$ and $Q$, the following logical identity is true:

\begin{align*}
    (P \implies Q) = ((\text{not } Q) \implies (\text{not } P)).
\end{align*}

The right-hand side is called the \textit{contrapositive} of the left-hand side.

You could verify this identity by using truth tables. Here is a nicer proof that uses\footnote{We will soon learn more about the assumption $\text{not}(\text{not}(Q)) = Q$.} the facts $(P \text{ or } Q) = (Q \text{ or } P)$ and $Q = \text{not}(\text{not } Q)$:

\begin{align*}
    (P \implies Q) &= ((\text{not } P) \text{ or } Q ) 
    = (\text{not}(\text{not } Q) \text{ or } (\text{not } P))
    = ((\text{not } Q) \implies (\text{not } P)).
\end{align*}

The contrapositive is practically important because it is sometimes easier to prove the contrapositive of an implication than it is to prove the implication itself.

\subsubsection*{Proof by contradiction}

Another way of proving $P \implies Q$ that is similar in spirit to a proof by contrapositive is \textit{proof by contradiction}. In a proof by contradiction, one assumes $(\text{not}(P \implies Q))$ and then derives from this assumption that some additional proposition $R$ and its negation $\text{not}(R)$ are \textit{both} true. Since for any proposition $R$ we have $(R \text{ and } (\text{not } R)) = F$, it intuitively follows that the original assumption $(\text{not}(P \implies Q))$ must have been false; that is, we must have $P \implies Q$.

In summary, a proof of $P \implies Q$ by contradiction is a proof of

\begin{align*}
    \text{not}(P \implies Q) \implies (R \text{ and } (\text{not } R)),
\end{align*}

where $R$ is some proposition.

Since we have $(\text{not}(P \implies Q)) = (P \text{ and } (\text{not } Q))$, we see that a proof of $P \implies Q$ by contradiction is equivalently a proof of

\begin{align*}
    (P \text{ and } (\text{not } Q)) \implies (R \text{ and } (\text{not } R)).
\end{align*}

This is the form of proof by contradiction that is used in practice. Authors typically begin a proof of $P \implies Q$ by contradiction by saying ``Assume $P$. For the sake of obtaining a contradiction, also assume $\text{not } Q$.'' A contradiction $(R \text{ and } (\text{not } R))$ is then derived, and the author writes the following to finish the proof: ``This contradicts the assumption $\text{not } Q$. Therefore $Q$ must be true.'' 

\subsubsection*{Contradiction vs. contrapositive}

We now investigate the difference between proof by contradiction and proof by contrapositive. 

When the contradiction $(R \text{ and } (\text{not } R))$ derived in a proof by contradiction is $(P \text{ and } (\text{not } P))$, the overall proof by contradiction is

\begin{align*}
    (P \text{ and } (\text{not } Q)) \implies (P \text{ and } (\text{not } P)).
\end{align*}

The above implies the following:

\begin{align*}
    (P \text{ and } (\text{not } Q)) \implies (\text{not } P).
\end{align*}

If we are additionally in a context where we know $P$ to be true, then the above further simplifies to:

\begin{align*}
    (\text{not } Q) \implies (\text{not } P),
\end{align*}

which is the contrapositive. Thus, we see that when $P$ is known to be true and the contradiction obtained is $(P \text{ and } (\text{not } P))$, the proof by contradiction can be rewritten to be a proof by contrapositive. This situation occurs very often in practice.

\subsubsection*{``If and only if''}

We define one more operator on propositions, the \textit{bidirectional implication} operator, denoted $\iff$. Given propositions $P$ and $Q$, we define

\begin{align*}
    P \iff Q := \Big( (P \implies Q) \text{ and } (Q \implies P) \Big).
\end{align*}

That is, $P \iff Q$ is true exactly when the implication $P \iff Q$ and its converse are both true. Here's the truth table for $\iff$.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    $P$ & $Q$ & $P \iff Q$ \\ \hline
    $T$ & $T$ & $T$        \\ \hline
    $T$ & $F$ & $F$        \\ \hline
    $F$ & $T$ & $F$        \\ \hline
    $F$ & $F$ & $T$        \\ \hline
    \end{tabular}
\end{table}

The operator $\iff$ is spoken aloud as ``if and only if'', and is abbreviated in writing as ``iff''. In the context of the bidirectional implication $P \iff Q$, the implication $P \implies Q$ is referred to as the \textit{forward implication} and its converse $Q \implies P$ is referred to as the \textit{reverse implication}. Note that $\iff$ is symmetric in the sense that $(P \iff Q) = (Q \iff P)$.

The English language interpretation of $\iff$ is similar to to the English language interpretation of $\implies$: the mathematical version of what someone really means when they say ``$P(x)$ if and only if $Q(x)$'' is $\forall x \spc P(x) \iff Q(x)$. Again, we see the difference between full mathematical formalism and language is that language often omits the ``for all''.

Many theorems in math state that a certain ``if and only if'' proposition is true. More specifically, such theorems usually state that some property of an object manifests if and only if another property of that object also manifests. So, when you see a theorem about an \textit{equivalent condition} or an \textit{equivalent definition}, there is an ``if and only if'' statement at play. All definitions in math are also automatically ``if and only if'' statements. There is, however, the common misleading convention of writing definitions using the word ``if'' (for example: ``We say $x$ has property $P(x)$ \textit{if} $Q(x)$''; the ``if'' should really be an ``iff'').

A neat fact is that $=$ and $\iff$ are the same operator. One way to verify this is by checking that the truth tables of $=$ and $\iff$ are the same.

\subsubsection*{``Necessary'' and ``sufficient''}

Let $P$ and $Q$ be predicates, and consider the proposition $\forall x \spc P(x) \implies Q(x)$. Due to the reasoning ``if $P$ happened then $Q$ must have happened'', $Q$ is said to be a \textit{necessary condition} for $P$. Similarly, due to the reasoning ``one of ways to make $Q$ happen is to make $P$ happen'', $P$ is said to be a \textit{sufficient} condition for $Q$. 

Notice that when $\forall x \spc P(x) \iff Q(x)$ iff $P$ and $Q$ are both necessary and sufficient conditions for each other (i.e. $P$ is a necessary and sufficient for $Q$ and $Q$ is a necessary and sufficent condition for $P$).

\newpage

\section{Sets}

\subsection*{Set equality, subsets}

Let $S$ and $T$ be sets. We define $S$ and $T$ to be \textit{equal} iff $\forall x \spc x \in S \iff x \in T$.

We say $T$ is a \textit{subset} of $S$ and write $T \subseteq S$ iff $\forall x \in S \spc x \in T$. We say $T$ is a \textit{proper subset} of $S$ iff $T$ is a subset of $S$ and $T \neq S$.

For all sets $S$, the empty set $\emptyset$ is a subset of $S$ because the proposition $(\forall x \in \emptyset \spc x \in S)$ is\footnote{$(\forall x \in \emptyset \spc x \in S) = (\forall x \spc x \in \emptyset \implies x \in S) = (\forall x \spc F \implies x \in S) = \forall x \spc T = T$.} ``true by false hypothesis''. The more intuitive fact that $S \subseteq S$ for all sets $S$ is also true.

Notice, the similarity of the subset notation $\subseteq$ for sets to the ``less than or equal to'' $\leq$ notation for numbers is a helpful reminder of the fact that for any set $S$ we have $S \subseteq S$. A little less intuitive is that we write $T \subsetneq S$ iff $T$ is a proper subset of $S$; we don't use $T \subset S$ to indicate ``proper subset''. It's true that the $\subset$ notation would be more analogous to the $<$ notation for numbers, but, unfortunately, some authors write $T \subset S$ to mean $T \subseteq S$.

\subsection*{There is no universal set in ZFC}

It seems natural that there would exist a set which contains every set. However, this is not possible in ZFC. Why? Suppose there did exist such a ``universal set'' $U$. Then we can consider the subset $V$ of $U$ which contains all non-self-containing sets, $V := \{ S \in U \mid S \notin S \}$. Now notice that we have $(V \in V \iff V \notin V)$. This is clearly a contradiction. Thus, there is no ``set of all sets'' in ZFC.

\subsection*{Common sets}

We now define notation for many common infinite sets.

\begin{align*}
    \N &:= \text{``the natural numbers''} = \{0, 1, 2, 3, ...\} \\
    \Z &:= \text{``the integers''} = \{..., -3, -2, -1, 0, 1, 2, 3, ... \} \\
    \Q &:= \text{``the rational numbers''} = \Big\{ \frac{n}{m} \spc \Big| \spc n, m \in \Z \Big\} \\
    \R &:= \text{``the real numbers''} = \{ \text{all limits of sequences of rational numbers} \} \\
    \C &:= \text{``the complex numbers''} = \{ a + b \sqrt{-1} \mid a, b \in \R \}
\end{align*}

We have $\N \subsetneq \Z \subsetneq \Q \subsetneq \R \subsetneq \C$.

\subsection*{Indexing sets}

Let $S$ be a set. An \textit{indexing set} of $S$ is a set $I$ that is thought of as ``labeling'' the elements $S$. Any set can be used as an indexing set\footnote{A technical point is that a set $I$ can only index \textit{all} elements of a set $S$ iff there exists an onto function $I \rightarrow S$. If there does not exist such a function, $I$ can only index a proper subset of $S$. We will learn about onto functions later.}, though we tend to use sets of numbers as indexing sets. The set of elements in $S$ indexed by $I$ is written as

\begin{align*}
    \{ x_i \in S \mid i \in I \}.
\end{align*}

\newpage

\subsection*{Union and intersection}

We now define ways to build sets from a collection\footnote{The word ``collection'' is often used to mean ``set of sets''.} of sets. Let $C$ be a set which contains sets as elements.

The \textit{union} of the sets in $C$ is defined to be the result of collecting all the elements of all of the sets in $C$. The \textit{intersection} of the sets in $C$ is defined to be the result of considering only the elements that are common to every single set in $C$.

\begin{align*}
    \bigcup_{S \in C} S &:= \{ x \mid \exists S \in C \spc x \in S \} \quad \text{(union)} \\
    \bigcap_{S \in C} S &:= \{ x \mid \forall S \in C \spc x \in S \} \quad \text{(intersection)}.
\end{align*}

When the collection $C$ is finite and contains $n \in \N$ sets, we often index $C$ by the set $\{1, ..., n\}$, so that $C = \{S_i \mid i \in \{1, ..., n\}\}$, and denote the unions and intersections, respectively, by the following three notations:

\begin{align*}
    \bigcup_{i \in \{1, ..., n\}} S_i &= \bigcup_{i = 1}^n S_i = S_1 \cup ... \cup S_n \\
    \bigcap_{i \in \{1, ..., n\}} S_i &= \bigcap_{i = 1}^n S_i = S_1 \cap ... \cap S_n.
\end{align*}

When $C$ contains just two sets, $C = \{S, T\}$, we write $S \cup T$ rather than $\bigcup_{S \in C} S$ and $S \cap T$ rather than $\bigcap_{S \in C}$. Applying the above definitions\footnote{To formally prove the first line of the claimed result, show that $ \{ x \mid \forall S \in C \spc x \in S \}$ and $\{x \mid x \in S \text{ and } x \in T\}$ are subsets of each other. The second line is proved similarly.}, we see that

\begin{align*}
    S \cup T &= \{x \mid x \in S \text{ or } x \in T\} \\
    S \cap T &= \{x \mid x \in S \text{ and } x \in T\}.
\end{align*}

Here are some examples: we have $\{a, b, c\} \cup \{b, c, d\} = \{a, b, c, d\}, \emptyset \cap \{a, b\} = \{a, b\}$ and $\{a, b, c\} \cap \{b, c, d\} = \{b, c\}, \{a, b\} \cap \{c, d\} = \emptyset$.

When considered as operations on two sets, we see that $\cup$ and $\cap$ are associative:

\begin{align*}
    \forall S \spc \forall T \spc \forall R \spc (S \cup T) \cup R &= S \cup (T \cup R) \\
    \forall S \spc \forall T \spc \forall R \spc (S \cap T) \cap R &= S \cap (T \cap R).
\end{align*}

So, we can ``drop the parentheses'' and define for sets $S, T$, and $R$:

\begin{align*}
    S \cup T \cup R &:= (S \cup T) \cup R = S \cup (T \cup R) \\
    S \cap T \cap R &:= (S \cap T) \cap R = S \cap (T \cap R).
\end{align*}

\subsection*{Set difference and set complement}

\subsubsection*{Set difference}

The union of two sets, introduced above, is in some sense analogous to addition of numbers. The following \textit{set difference} operation is analogous to the subtraction of one number from another:

\begin{align*}
    S - T := \{ x \mid x \in S \text{ and } x \notin T \}.
\end{align*}

Most authors denote the set difference $S - T$ with the notation $S \setminus T$.

\subsubsection*{Set complement}

Sometimes, it is convenient to consider some set $U$ to be the ``largest possible set''. For example, it is common to set $U$ to be the unit interval, $U = [0, 1]$, when proving theorems from calculus. In this sort of situation, the \textit{set complement (in $U$)} of some other set $S$ is denoted $S^C$ and is defined to be $S^C := U - S$.

\subsection*{Cartesian products}

\subsubsection*{Ordered pairs}

\textit{Ordered pairs} are objects notated in the form ``$(x, y)$'' (an ordered pair consists of two objects, such as $x$ and $y$, that are separated by a comma and are enclosed in parentheses) that are subject to the characterizing property

\begin{align*}
    \forall x_1 \spc \forall x_2 \spc \forall y_1 \spc \forall y_2 \spc (x_1, y_1) = (x_2, y_2) \iff (x_1 = y_1 \text{ and } x_2 = y_2).
\end{align*}

One definition that gives us the above characterizing property is $(x, y) := \{x, \{y\}\}$. There are other definitions that work, too. The choice of which particular definition we use for $(x, y)$ isn't really important, and is really just an ``implementation detail''. What's most important is that the proposition ``there exists a mathematical object with the above characterizing property'' is true.

\subsubsection*{Tuples}

Similar to the concept of ordered pair is the concept of ``$n$-tuples''. An \textit{$n$-tuple}, $n \in \N$, is a ordered list of $n$ entities, notated in the form ``$(x_1, ..., x_n)$'', such that the following characterizing property holds for all indexed sets $\{x_i \mid i \in \{1, ..., n\}\}$ and $\{y_i \mid i \in \{1, ..., n\}\}$:

\begin{align*}
     \spc (x_1, ..., x_n) = (y_1, ..., y_n) \iff (\forall i \in \{1, ..., n\} \spc x_i = y_i).
\end{align*}

To prove that there do indeed exist mathematical objects with such a characterizing property, we rely on the concept of an ordered pair. We define the $n$-tuple $(x_1, ..., x_n)$ to be the ordered pair $((x_1, ..., x_{n - 1}), x_n)$, where when $n = 2$ the $n$-tuple $(x_1, x_n)$ is defined to be the ordered pair $(x_1, x_2)$ and when $n = 1$ the $n$-tuple $(x_1)$ is defined to be the element $x_1$. Of course, we could have also used the definition $(x_1, ..., x_n) := (x_1, (x_2, ..., x_n))$; but as before, this is only an ``implementation detail'', and what's really important is that the proposition ``there exists a mathematical object with the above characterizing property'' is true.

\subsubsection*{Cartesian products}

The \textit{Cartesian product} of sets $S$ and $T$ is the set $S \times T$ of all ordered pairs whose elements are from $S$ and $T$:

\begin{align*}
    S \times T := \{(s, t) \mid s \in S \text{ and } t \in T\}.
\end{align*}

The Cartesian product $\times$ is unfortunately not associative: for sets $S, T$, and $R$, we have

\begin{align*}
    (S \times T) \times R &= \{ ((s, t), r) \mid s \in S \text{ and } t \in T \text{ and } r \in R\} \\ &\neq \{(s, (t, r)) \mid s \in S \text{ and } t \in T \text{ and } r \in R\} = S \times (T \times R).
\end{align*}

To circumvent this issue, we define $\times$ to act on $n$ sets $S_1, ..., S_n$ as follows:

\begin{align*}
    \bigtimes_{i \in \{1, ..., n\}} S_i = \bigtimes_{i = 1}^n S_i = S_1 \times ... \times S_n := \{(s_1, ..., s_n) \mid \forall i \in \{1, ..., n\} \spc s_i \in S_i\}.
\end{align*}

Note that here, $(s_1, ..., s_n)$ is an $n$-tuple.

\subsubsection*{$n$th Cartesian products}

Sometimes it's useful to take the Cartesian product of a set with itself multiple times. For this situation, we define, for a set $S$ and $n \in \N$,

\begin{align*}
    S^n := \bigtimes_{i = 1}^n S.
\end{align*}

One important example is that the identity $\R^2 = \R \times \R$ can be interpreted to state that the plane ($\R^2$) can be thought of as the Cartesian product of the ``$x$-axis'' (the first $\R$) with the ``$y$-axis'' (the second $\R$).

\newpage

\section{Relations}

\begin{defn}
    (Relation).

    Let $S$ be a set. A \textit{relation (on $S$)} is simply a subset of $S \times S$. Given a relation $R$ on $S$, we write $xRy$ to mean $(x, y) \in R$, and say ``$x$ is related to $y$'' iff $xRy$.
\end{defn}

\begin{defn}
    (Inverse relation).

    Given a relation $R$ on a set $S$, the \textit{inverse relation} to $R$ is the relation on $S$ denoted by $R^{-1}$ and defined by $R^{-1} := \{(y, x) \in S \times S \mid (x, y) \in R\}$. That is, $yR^{-1}x$ iff $xRy$.
\end{defn}

\subsection*{Equivalence relations}

\begin{defn}
    (Equivalence relation).

    An \textit{equivalence relation} on a set $S$ is a relation on $S$ that satisfies the following properties:

    \begin{itemize}
        \item (Reflexivity). $\forall x \in S \spc xRx$.
        \item (Symmetry). $\forall x \in S \spc \forall y \in S \spc xRy \implies yRx$.
        \item (Transitivity). $\forall x \in S \spc \forall y \in S \spc \forall z \in S \spc xRy \text{ and } yRz \implies xRz$.
    \end{itemize}
\end{defn}

\begin{defn}
\label{ch::logic_pf_fns::defn::quotient_set}

    (Quotient set).
\end{defn}

\subsection*{Functions}

Latin meanings of injective and surjective.

\begin{defn}
    (Function).
    
    Functions are also commonly called \textit{maps}, or \textit{mappings}.
    
    domain, codomain, range
\end{defn}

\begin{defn}
    (Uniqueness).
    
    $\exists !x \spc P(x) := (\exists x_0 \spc P(x_0)) \text{ and } (\forall x \spc P(x) \implies x = x_0)$.
\end{defn}

\begin{remark}
    (Well-definedness and uniqueness).
    
    What well-definedness refers to
    
    Any property which is unique is the output of a well-defined function
\end{remark}

\begin{defn}
    (One-to-one, injection).
\end{defn}

\begin{theorem}
    (A function's inverse relation is a function iff the function is one-to-one).
\end{theorem}

\begin{defn}
    (Onto, surjection).
\end{defn}

\begin{defn}
    (Bijection).
\end{defn}

\begin{theorem}
    (A function is a bijection iff it is invertible).
\end{theorem}

left-inverse, right-inverse, relation to -jectivities

addition of fns, scaling of fns?, image-set

preimages (not neccess fns)

\begin{defn}
\label{ch::logic_pf_fns::defn::inverse_fn}

    (Inverse function).
\end{defn}

\begin{theorem}
\label{ch::logic_pf_fns::thm::invertible_iff_bijection}
    (Invertible iff bijection).
\end{theorem}

\subsection*{Cardinality of sets}

It was mentioned earlier that sets can be finite or infinite. We formally define

\begin{theorem}
    (Left inverse and right inverse implies two-sided inverse). Let $X$ and $Y$ be sets, and let $f:X \rightarrow Y$ be a function. A \textit{left-inverse} of $f$ is a function $\ell:Y \rightarrow X$ such that $\ell \circ f = I_X$, where $I_X$ is the identity on $X$. A \textit{right-inverse} of $f$ is a function $r:Y \rightarrow X$ such that $f \circ r = I_Y$, where $I_Y$ is the identity on $Y$.
    
    If $f:X \rightarrow Y$ has a left-inverse $\ell$ and a right-inverse $r$, then they must be equal, and we denote them by $f^{-1} := \ell = r$.
\end{theorem}

\begin{proof}
    Let $\ell$ and $r$ be left- and right- inverses of $f$, respectively. Then by associativity of function composition, $\ell \circ f \circ r = \ell \circ I_Y = I_X \circ r$. Therefore $\ell = r$.
\end{proof}

\begin{defn}
    (Proof by induction).
    
    Defn?
\end{defn}
