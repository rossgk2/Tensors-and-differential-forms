\section{The determinant and orientation}

\section{Permutations}

\begin{defn}
    (Permutation).
    
    A \textit{permutation (on $\{1, ..., n\}$)} is a bijection $\{1, ..., n\} \rightarrow \{1, ... n \}$. The set of permutations on $\{1, ..., n\}$ is denoted $S_n$.
\end{defn}

\begin{defn}
    (Cycle).

    A permutation in $S_n$ is a \textit{cycle} iff it is defined by

    \begin{align*}
        \begin{cases}
            i_j \mapsto i_{j + 1} & j \in \{1, ..., k - 1\} \\
            i_j \mapsto i_1 & j = k
        \end{cases}
    \end{align*}

    for some $i_1, ..., i_k \in \{1, ..., n\}$. If such $i_1, ..., i_k$ exist, then the cycle is represented as the following tuple without commas: $\begin{pmatrix} i_1 & \hdots & i_k \end{pmatrix}$.
\end{defn}

\begin{defn}
    (Transposition).

    A permutation in $S_n$ is a \textit{transposition} iff it swaps two elements and leaves all of the others fixed. Notice, every transposition can be written as the cycle $\transp{i}{j}$ for some $i, j \in \{1, ..., n\}$.
\end{defn}

Now, we prove an intuitive theorem.

\begin{theorem}
\label{ch::lin_alg::thm::permutations_decomposition_into_transpositions}
    (Every permutation is a composition of transpositions).

    Every permutation $\sigma \in S_n$ can be decomposed as $\sigma = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_n$ are transpositions. Note, the set of transpositions in the decomposition does not uniquely correspond to $\sigma$.
\end{theorem}

\begin{proof}  
    Notice that
    
    \begin{itemize}
        \item The function $\text{sort}_\sigma \in S_n$ that sorts $(\sigma(1), ..., \sigma(n))$ from least to greatest satisfies $\text{sort}_\sigma((\sigma(1), ..., \sigma(n))) = (1, ..., n)$.
        \item We also have $\sigma^{-1}((\sigma(1), ..., \sigma(n))) = (\sigma^{-1}(\sigma(1)), ..., \sigma^{-1}(\sigma(n)) = (1, ..., n)$.
    \end{itemize} 
    
    Thus, $\text{sort}_\sigma = \sigma^{-1}$. Since there are many ways to describe $\text{sort}_\sigma$ as a composition of transpositions\footnote{See Definition \ref{ch::appendix::defn::bubble_sort} in the appendix for a description of one such description, the \textit{bubble sort algorithm}.}, we have $\text{sort}_\sigma = \sigma^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1 \circ ... \circ \sigma_k$ are transpositions. The inverse of a transposition is a transposition, so we have $\sigma = \sigma_1^{-1} \circ ... \circ \sigma_k^{-1}$, where $\sigma_1^{-1}, ..., \sigma_k^{-1}$ are transpositions. So, we have shown that $\sigma$ decomposes into a composition of transpositions. This decomposition is not unique, as it depends on which sorting algorithm is used to compute $\text{sort}_\sigma = \sigma^{-1}$.
    
    % (Case: $X \neq S_n$ for all $n$). Take any bijection $f:X \rightarrow \{1, ..., |X|\}$. Since $f \circ \sigma \circ f^{-1} \in S_{|X|}$, then applying the previous case implies $f \circ \sigma \circ f^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_{|X|}$ are transpositions. Solving for $\sigma$, we have $\sigma = f^{-1} \circ (\sigma_k \circ ... \circ ... \sigma_1) \circ f = (f^{-1} \circ \sigma_k \circ f) \circ (f^{-1} \circ \sigma_{k - 1} \circ f) \circ ... \circ (f^{-1} \circ \sigma_1 \circ f)$. Each $f^{-1} \circ \sigma_i \circ f$ is a permutation on $X$, so we have shown that $\sigma$ is a composition of permutations on $X$, as desired.
\end{proof}

The following definition and theorem are the main reason for our excursion into permutations.

\begin{defn}
    (Parity of a permutation).

    A permutation in $S_n$ is said to have \textit{even parity}, or to be \textit{even}, iff it is equal to the composition of an even number of transpositions. A permutation is said to have \textit{odd parity}, or to be \textit{odd}, iff it is equal to the composition of an odd number of transpositions.
\end{defn}

\begin{theorem}
    (The parity of a permutation is unique).

    A permutation in $S_n$ has a unique parity. That is, no permutation is both even and odd.
\end{theorem}

\begin{proof}
    Let $\sigma \in S_n$, and suppose $\sigma = \sigma_k \circ ... \circ \sigma_1$ and $\sigma = \pi_\ell \circ ... \circ \pi_1$, where $\sigma_1, ..., \sigma_k$ and $\pi_1, .., \pi_\ell$ are transpositions. We need to prove that $k$ and $\ell$ have the same parity.
    
    Suppose for contradiction that $k$ and $\ell$ have different parities. We have $\sigma_k \circ ... \circ \sigma_1 = \pi_k \circ ... \circ \pi_1$, and so $\text{id} = \pi_1^{-1} \circ ... \circ \pi_\ell^{-1} \circ \sigma_k \circ ... \circ \sigma_1 = \pi_1 \circ ... \circ \pi_\ell \circ \sigma_k \circ ... \circ \sigma_1$. Since $k$ and $\ell$ have different parities, then  $k + \ell$ is odd, and the identity permutation is a composition of an odd number of transpositions.
    
    Since the identity is a composition of an odd number of transpositions, it cannot be a composition of zero transpositions. It also cannot be a composition of one of one transposition, as no single transposition is the identity! Therefore, the identity is a composition of more than one transposition. Let $r$ be the smallest integer with $r > 1$ such that $\text{id} = \tau_r \circ ... \circ \tau_1$, where $\tau_1, ..., \tau_r$ are transpositions.
    
    Consider the following cases.

    (Case: $\tau_r = \tau_{r - 1}$). Then $\tau_r \circ \tau_{r - 1} = \text{id}$, and $\tau_r \circ ... \circ \tau_1 = \tau_{r - 2} \circ ... \circ \tau_1$ is a composition of $r - 2$ transpositions. We see $r$ is not minimal; this is a contradiction, so this case is impossible.

    (Case: $\tau_r \neq \tau_{r - 1}$). Suppose $\tau_r = \transp{i}{j}$.
    
    \fullindent
    {
        (Case: $\tau_r$ and $\tau_{r - 1}$ are disjoint). If $\tau_r$ and $\tau_{r - 1}$ are disjoint, then $\tau_r \circ \tau_{r - 1} = \tau_{r - 1} \circ \tau_r = \tau_{r - 1} \circ \transp{i}{j}$.
    
        (Case: $\tau_r$ and $\tau_{r - 1}$ share one element).
    
        \fullindent
        {
            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r}$ for some $r \neq j$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r} = \begin{pmatrix} i & r & j \end{pmatrix} = \begin{pmatrix} j & i & r \end{pmatrix} = \transp{j}{r} \circ \transp{i}{j}$. 
            }

            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s}$ for some $s \neq i$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s} = \begin{pmatrix} i & j & s \end{pmatrix} = \transp{s}{j} \circ \transp{i}{s}$.
            }
        }
    }

    \newcommand{\tttau}{\widetilde{\tau}}
            
    To summarize the above, in all of the possible cases, we have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \tau_{r - 1} = \tttau_r \circ \transp{i}{a_1}$ for some $a_1 \in \{1, ..., n\} - \{i\}$ and some transposition $\tttau_r$ that fixes $a_1$. Thus, $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ \transp{i}{a_1} \circ \tau_{r - 2} \circ ... \circ \tau_1$, where $\tttau_r$ fixes $i$. Repeating this process, we see $\id = \tau_r \circ \tau_{r - 1} \circ \transp{i}{a_2} \circ ... \circ \tau_1$. It follows from induction that there exist $a \in \{1, ..., n\} - \{i\}$ and transpositions $\tttau_2, ..., \tttau_r$ such that $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ ... \circ \tttau_2 \circ \transp{i}{a}$, where all of $\tttau_2, ..., \tttau_r$ fix $i$. Thus $\id(i) = a \neq i$. This is a contradiction, since the identity fixes every element of $\{1, ..., n\}$.
\end{proof}

\begin{defn}
    The \textit{sign} $\sgn(\sigma)$ of a permutation $\sigma \in S_n$ is defined to be
    
    \begin{align*}
        \sgn(\sigma) := 
        \begin{cases}
            1 & \text{$\sigma$ is even} \\
            -1 & \text{$\sigma$ is odd}
        \end{cases}.
    \end{align*}

    Notice, $\sgn$ is a well-defined function because the parity of a permutation is unique.

    It's useful to note that $\sgn(\sigma) = (-1)^k$, where $k$ is the number of transpositions in one of the decompositions for $\sigma$.
\end{defn}

\begin{theorem}
    (Sign of a composition of permutations).

    We have $\sgn(\pi \circ \sigma) = \sgn(\pi) \sgn(\sigma)$ for all permutations $\sigma, \pi \in S_n$.
\end{theorem}

\begin{proof}
    $\sigma$ is a composition of $k$ transpositions for some $k$, and $\pi$ is a composition of $\ell$ transpositions for some $\ell$. Notice that the theorem is equivalent to the statement ``the parity of $k + \ell$ is even iff $k$ and $\ell$ have the same parity and odd iff $k$ and $\ell$ have different parity''. This statement is an elementary fact about integers, so the theorem is true.
\end{proof}

\newpage

\section{The determinant}

\label{ch::lin_alg::determinant}

\begin{defn}
\label{ch::lin_alg::defn::determinant}
    (The determinant).
    
    Let $K$ be a field, and let $\sE = \{\see_1, ..., \see_n\}$ be the standard basis of $K^n$. We want to define a function $(K^n)^{\times n} \rightarrow K$ which, given $\vv_1, ..., \vv_n \in K^n$, returns the $n$-dimensional volume of the parallelapiped spanned by $\vv_1, ..., \vv_n$. We will denote this function by $\det:(K^n)^{\times n} \rightarrow K$. We require that $\det$ satisfy the following axioms:
    
    \begin{enumerate}
        \item $\det(\see_1, ..., \see_n) = 1$, since we want the unit $n$-cube to have an $n$-dimensional volume of $1$.
        \item $\det$ is multilinear, because...
        \begin{itemize}
            \item The volume of a parallelapiped that is the disjoint union of two smaller parallelapipeds should be the sum of the volumes of the smaller parallelapipeds.
            \item Scaling one of the sides of a parallelapiped by $c \in K$ should increase that paralellapiped's volume by a factor of $c$. 
        \end{itemize}
        \item $\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$. This should hold because when two sides of a parallelapiped coincide, its \textit{$n$-dimensional} volume is zero.
    \end{enumerate}
\end{defn}

\begin{deriv}
    (Alternatingness).

    Let $\vv, \ww \in K^n$. By the third axiom of the determinant, we have $\det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n) + \det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) = \det(\vv_1, ..., \vv + \ww, ..., \ww + \vv, ..., \vv_n) = 0$. Thus,

    \begin{align*}
        \det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n) = -\det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) \text{ for all $\vv, \ww \in K^n$}.
    \end{align*}

    This condition is known as the \textit{alternatingness} property of the determinant. It is quick to show that alternatingness property not only follows from the third axiom of the determinant, but is equivalent to it.

    Notice that the alternatingness property of the determinant implies that there exist $\vv_1, ..., \vv_n \in K^n$ with $\det(\vv_1, ..., \vv_n) < 0$. So, we see our intuitive assumptions about volume require that volume be \textit{signed}. In Theorem [...] we will see how signed volume corresponds to the concept of orientation.
\end{deriv}

We quickly define alternating functions to be functions that have this alternatingness property, as this definition allows for a succinct characterization of the determinant.

\begin{defn}
    (Alternating function).

    If $V$ and $W$ are vector spaces, then a function $\ff:V^{\times k} \rightarrow W$ is \textit{alternating} iff \\ ${\ff(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_k) = -\ff(\vv_1, ..., \vv_j, ..., \vv_i, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$. Equivalently, $\ff:V^{\times k} \rightarrow W$ is alternating iff
    ${\ff(\vv_{\sigma(1)}, ..., \vv_{\sigma(k)}) = \sgn(\sigma) \ff(\vv_1, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$ and $\sigma \in S_k$.
\end{defn}

\begin{theorem}
    (Succinct characterization of the determinant).

    The determinant is the unique\footnote{We have not yet explained why a function satisfying the determinant axioms is unique. We do so at the beginning of Derivation \label{ch::exterior_powers::deriv::permutation_formula_for_determinant}.} alternating linear $k$-form sending $\sE \mapsto 1$. (Recall Definition \ref{ch::bilinear_forms_metric_tensors::defn::linear_k_form}).
\end{theorem}

We now investigate more properties of the determinant that follow from this succint characterization.

\begin{theorem}
\label{ch::lin_alg::thm::consequent_det_props}
    (Consequent properties of the determinant). 
    
    \begin{enumerate}
    \setcounter{enumi}{3}
        \item $\det$ is invariant under linearly combining input vectors into a different input vector. That is, $\det(\vv_1, ..., \vv_i, ..., \vv_n) = \det(\vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^n d_j \vv_j, ..., \vv_n)$ for all $i \in \{1, ..., n\}$.
        \item $\det(\vv_1, ..., \vv_n) = 0$ iff $\{\vv_1, ..., \vv_n\}$ is a linearly dependent set.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
    \setcounter{enumi}{4}
        \item 
        
        Using the axiom ``$\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$'' together with the multilinearity of the determinant, we have
        
        \begin{align*}
            \det(\vv_1, ..., \vv_i, ..., \vv_n)
            &= \det(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_n) + \sum_{j = 1, j \neq i}^n d_j \det(\vv_1, ..., \vv_{i - 1}, \vv_j, \vv_{i + 1}, ..., \vv_j, ..., \vv_n) \\
            &= \det\Big( \vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^n d_j \vv_j, ..., \vv_n \Big).
        \end{align*}
        
        \item 
        \mbox{}
        \\ \indent ($\det(\vv_1, ..., \vv_n) = 0 \implies \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). If the input vectors are linearly dependent, we can use the invariance of $\det$ under linearly combining some columns into others (which we just proved) to produce an equal determinant in which two columns are the same. By the third axiom, this determinant is zero.
        \\ \indent ($\det(\vv_1, ..., \vv_n) = 0 \impliedby \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). Suppose for contradiction that the determinant of a set of $n$ linearly independent vectors is zero. These $n$ linearly independent vectors form a basis for $K^n$, so we have shown that the determinant of a basis set is zero. But then, using multilinearity together with the invariance of $\det$ under linearly combining some vectors into a different vector, we can show that $\det(\vv_1, ..., \vv_n) = 0$ for \textit{all} $\vv_1, ..., \vv_n \in K^n$. This contradicts the first axiom that specifies $\det(\see_1, ..., \see_n) = 1$.
    \end{enumerate}
\end{proof}

Using the properties we have amassed, we can derive a formula for the determinant.

\begin{defn}
    (Determinant of a square matrix). 
    
    We define the \textit{determinant of a square matrix} to be the result of applying $\det$ to the column vectors of that matrix.
\end{defn}

\begin{deriv}
\label{ch::lin_alg::deriv::permutation_formula_for_determinant}
    (Permutation formula for the determinant).
    
    We now derive the \textit{permutation formula} for the determinant. The formula we obtain shows that the function $\det$ specified in Definition \ref{ch::lin_alg::defn::determinant} exists; since the right side of the formula is a well-defined function, the determinant is the unique function satisfying the axioms.
    
    Consider vectors $\vv_1, ..., \vv_n \in K^n$, and\footnote{There is no hidden meaning behind the upper and lower indices on $a^i_j$ here; we only want to consider an arbitrary $n \times n$ matrix of scalars in $K$, and prefer to think of this matrix as storing the coordinates of a $(1, 1)$ tensor rather those of a $(2, 0)$ or $(0, 2)$ tensor.} set $(a^i_j) := ([\vv_i]_\sE)^j$. Then...
    
    \begin{align*}
        \det((a^i_j)) = \det(\vv_1, ..., \vv_n)
        &= \det \Big(\sum_{i_1 = 1}^n a^1_{i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a^n_{i_n} \see_{i_n} \Big) \\
        &= \sum_{i_1 = 1}^n \det \Big( a^1_{i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a^n_{i_n} \see_{i_n} \Big) \\
        &\vdots \\
        &= \sum_{i_1 = 1}^n ... \sum_{i_n = 1}^n \det(a^1_{i_1} \see_{i_1}, ..., a^n_{i_n} \see_{i_n}) \\
        &= \sum_{i_1 = 1}^n ... \sum_{i_n = 1}^n \det(a^1_{i_1} \see_{i_1}, ..., a^n_{i_n} \see_{i_n}), \text{ where $i_1, ..., i_n$ are distinct from each other} \\
        &= \sum_{\sigma \in S_n}
        \det(a^1_{\sigma(1)} \see_{\sigma(1)}, ..., a^n_{\sigma(n)} \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n}
        a^1_{\sigma(1)} ... a^n_{\sigma(n)} \det(\see_{\sigma(1)}, ..., \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \text{sgn}(\sigma)
        \det(\see_1, ..., \see_n) \\
        &= \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \text{sgn}(\sigma)
    \end{align*}
    
    Therefore, we have
    
    \begin{align*}
        \boxed
        {
            \det(\vv_1, ..., \vv_n) = \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma)
        }
    \end{align*}
    
    In this derivation, we have mostly used the multilinearity of the determinant. Though, the expression labeled with ``where $i_1, ..., i_n$ are distinct from each other'' results from the previous line due to the third axiom of the determinant, $\det(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_n) = 0$ when $\vv_i = \vv_j$.
    
    There are four major steps in the derivation. The first step is to use the multilinearity of the determinant to turn the determinant of $(a^i_j)$ into a sum of the determinants of matrices that only have one nonzero entry in each column (see the line directly above the line labeled with ``where $i_1, ..., i_n$ are distinct from each other''). The second step is to disregard all determinants in this previous sum whose matrix arguments have two or more columns that have their nonzero entries in the same row (i.e. whose matrix arguments are matrices of linearly dependent columns). This leaves us with a sum of determinants of diagonal matrices whose columns have been shuffled (this corresponds to the line labeled with ``where $i_1, ..., i_n$ are distinct from each other'' and the line directly below it). The third step, which corresponds to the third to last line, is to use multilinearity to pull out all the constants. The fourth step is to use the alternatingness of the determinant so that every determinant argument in the sum is the identity matrix; this results in multiplying each term in the sum by $\sgn(\sigma)$.
\end{deriv}

\begin{deriv}
\label{ch::lin_alg::thm::det_transpose_invariant}    
    (Determinant of a matrix is transpose-invariant).
    
    Let $\AA$ be a square matrix with entries in $K$. Recall from the discussion after the statement of the permutation formula for the determinant that the determinant of a matrix is a sum of determinants of diagonal matrices whose columns have each been shuffled by a different permutation $\sigma$:
    
    \begin{align*}
        \det(\AA) = 
        \sum_{\sigma \in S_n}
        \det(a^1_{\sigma(1)} \see_{\sigma(1)}, ..., a^n_{\sigma(n)} \see_{\sigma(n)}).
    \end{align*}
    
    Now, notice that a shuffled diagonal matrix that has been shuffled in accordance to a permutation $\sigma$ can be converted to a diagonal matrix via application of $\sigma^{-1}$, transposed (the transpose of a diagonal matrix $\DD$ is $\DD$), and then re-shuffled via application of $\sigma$. Doing this does not change the determinant, since the $\sgn(\sigma^{-1})$ introduced by the shuffle is canceled by the $\sgn(\sigma)$ factor introduced by the re-shuffle.

    When this is done to every matrix in the sum, then the sum is what we would obtain if we were to compute $\det(\AA^\top)$ by repeating the derivation of the permutation formula for the determinant. Therefore
    
    \begin{align*}
        \det(\AA) = \det(\AA^\top).
    \end{align*}
\end{deriv}

\begin{theorem}
    (Laplace expansion for the determinant).
    
    Consider an $n \times n$ matrix $\AA = (a^i_j)$, and let $\AA^i_j$ denote the so-called \textit{$ij$ minor matrix} obtained by erasing the $i$th row and $j$th column of $\AA$. We have
    
    \begin{align*}
        \det(\AA) = \sum_{i = 1}^{n} a^i_j \det(\AA^i_j) \text{ for all $i \in \{1, ..., n\}$} \\
        \det(\AA) = \sum_{j = 1}^{n} a^i_j \det(\AA^i_j) \text{ for all $j \in \{1, ..., n\}$}
    \end{align*}
    
    The first equation is called the \textit{Laplace expansion for the determinant along the $i$th row}, and the second equation is called the \textit{Laplace expansion for the determinant along the $j$th column}. Note that each equation implies the other because $\det(\AA) = \det(\AA^\top)$.
\end{theorem}
    
\begin{proof}
    We prove the second equation of the theorem.
    
    Consider all terms in the permutation formula's sum for $\det(\AA)$ that have the factor $a^i_j$. Let $\BB$ denote the shuffled diagonal matrix that corresponds to one of these terms. We can view $\det(\BB)$ as $\det(\BB) = \pm a^i_j \det(\BB^i_j)$, where $\BB^i_j$ is the determinant of the matrix obtained by removing the $i$th column and $j$th row from $\BB$. The $\pm$ sign is a result of the fact that the matrices $\BB$ and $\BB^i_j$ may have different inversion counts.

    The main effort of this proof is to determine the $\pm$ sign and specify how the inversion counts of $\BB$ and $\BB^i_j$ differ.
    
    As a first step, note that the difference in the inversion count between $\BB$ and $\BB^i_j$ is the number of inversions that involve $a^i_j$. Thus, our problem reduces to determining an expression for the number of inversions that involve $a^i_j$. So, divide the matrix $\BB$ into quadrants that are centered on $a^i_j$. Let $k_1, k_2, k_3, k_4$ be the number of inversions in the upper left, upper right, lower left, and bottom right corners of $\AA$, respectively. The number of inversions involving $a^i_j$ is $k_2 + k_3$. Since we know $k_1 + k_2 + 1 = i$ and $k_1 + k_3 + 1 = j$, we have $k_2 + k_3 = i + j - 2 - 2k_1 = i + j - 2(k_1 + 1)$. (We also know $k_1 + k_2 + k_3 + k_4 = n$, but this is not that helpful). Thus, if $\sigma$ is the permutation corresponding to $\BB$ and $\pi$ is the permutation corresponding to $\BB^i_j$, then $\sgn(\sigma) = \sgn(\pi)(-1)^{i + j - 2(k_1 + 1)} = \sgn(\pi)(-1)^{i + j}$. Thus $\sgn(\sigma) = (-1)^{i + j}\sgn(\pi) \iff \sgn(\pi) = (-1)^{i + j}\sgn(\sigma)$. 
    
    So,
    
    \begin{align*}
        a^i_j \det(\BB^i_j) &= a^i_j \sum_{\pi \in S_n} a^{\pi(1)}_1 ... \cancel{a^{\pi(i)}_j} ..., a^{\pi(n)}_n \sgn(\pi) \\
        &= a^i_j \sum_{\sigma \in S_n} a^{\pi(1)}_1 ... \cancel{a^{\pi(i)}_j} ..., a^{\pi(n)}_n (-1)^{i + j} \sgn(\sigma) \\
        &= (-1)^{i + j} a^i_j \det(\BB)
    \end{align*}
    
    Thus $a^i_j \det(\BB^i_j) = (-1)^{i + j} a^i_j \det(\BB) \iff \det(\BB) = (-1)^{i + j} a^i_j \det(\BB^i_j)$. Now sum all of the $\BB$'s (the diagonal shuffled matrices) to get $\det(\AA) = \sum_{j = 1}^{n} a^i_j \det(\AA^i_j)$.
\end{proof}
    
%\begin{theorem}
%    (Determinant of an upper triangular matrix).
%\end{theorem}

%\begin{theorem}
%    (Adjoint and Cramer's rule).
%\end{theorem}

\begin{defn}
    (Determinant of a linear function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces \textit{of the same dimension}, and let $E$ and $F$ be be bases for $V$ and $W$. We define the \textit{determinant of a linear function $\ff:V \rightarrow W$} to be the determinant of the matrix of $\ff$ relative to $E$ and $F$, $\det(\ff) := \det([\ff(E)]_F)$.
\end{defn}

\begin{remark}
    We have not yet shown that the determinant of a linear function $V \rightarrow V$ is well-defined; we have not shown that it doesn't on the basis chosen for $V$. We will see that this is the case soon.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::rmk::det_dual_invariant}
    (Determinant of a matrix is dual-invariant).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Consider also the dual $\ff:W^* \rightarrow V^*$ (recall Definition \ref{ch::appendix::defn::dual_transf_after_id}). Then $\det(\ff^*) = \det(\ff)$.
\end{theorem}

\begin{proof}
    Recall from\footnote{Technically, the equivalent conditions of the definition we reference only apply to linear functions $V \rightarrow V$. This is not an issue because $V$ and $W$ have the same dimension; if we want to be very formal, we can use the linear function $\widetilde{\ff}:V \rightarrow V$ that is obtained from $\ff$ by identifying $W \cong V$ with the identification that sends basis vectors of $W$ to basis vectors of $V$.} condition (3) of Definition \ref{ch::bilinear_forms_metric_tensors::defn::symmetric_linear_fn} that if $\AA$ is the matrix of $\ff$ relative to orthonormal bases $\hU_1$ and $\hU_2$, then the matrix of $\ff^*$ relative to the induced dual bases $\hU_2^*$ and $\hU_1^*$ is $\AA^\top$. Since the determinant of a matrix is transpose-invariant (recall Theorem \ref{ch::lin_alg::thm::det_transpose_invariant},) we have $\det(\ff) = \det(\AA) = \det(\AA^\top) = \det(\ff^*)$.
\end{proof}

\begin{lemma}
\label{ch::lin_alg::lemma:det_as_scale_factor}
    (Determinant as scale factor).

    Let $V$ and $W$ be vector spaces over the same field. Let $h$ be an alternating linear $k$-form. If $\ff:V \rightarrow W$ is a linear function, then $h(\vv_1, ..., \vv_k) = \det(\ff) h(\ff(\vv_1), ..., \ff(\vv_k))$.
\end{lemma}

\begin{proof}
     Because $h$ is multilinear and alternating, it is analogous to the determinant $\det(\ff(\ee_1), ..., \ff(\ee_n)) = \det([\ff(E)]_F) = \det(\ff)$. So, set $(a^i_j) = [\ff(E)]_F$, and then use essentially the same argument as was made to derive the permutation formula on the left side of the above. We obtain
    
    \begin{align*}
        h(\ff(\ee_1), ...,
        \ff(\ee_k)) 
        &= \sum_{\sigma \in S_k} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) h(\ee_1, ..., \ee_k)
        = \Big( \sum_{\sigma \in S_k} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) \Big) h(\ee_1, ..., \ee_k) \\
        &= \det([\ff(E)]_F) h(\ee_1, ..., \ee_k)
        = \det(\ff) h(\ee_1, ..., \ee_k).
    \end{align*}
    
   So, we have the statement on the basis $E$
   
   \begin{align*}
        h(\ff(\ee_1), ..., 
        \ff(\ee_k)) = \det(\ff) h(\ee_1, ..., \ee_k).
   \end{align*}
   
   Since $h$ is multilinear, we can extend this fact to apply to any list of vectors in $V$. This gives the lemma.
\end{proof}

\begin{theorem}
    (Product rule for determinants). 
    
    Let $V, W$ and $Z$ be finite-dimensional vector spaces of the same dimension, and consider linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Z$. Then $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$. Thus, if $\AA$ is an $m \times n$ matrix and $\BB$ is an $n \times p$ matrix, then $\det(\BB \AA) = \det(\BB) \det(\AA)$.
\end{theorem}

\begin{proof}
   Set $n := \dim(V) = \dim(W) = \dim(Z)$, and let $h$ be an alternating linear $k$-form. By the lemma, $\det(\gg \circ \ff)$ satisfies
   
   \begin{align*}
       h((\gg \circ \ff)(\vv_1), ..., (\gg \circ \ff)(\vv_n)) = \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) \text{ for all $\vv_1, ..., \vv_n \in V$}.
   \end{align*}
   
   Notice that the left side is
   
   \begin{align*}
       h\Big(\gg(\ff(\vv_1)), ..., \gg(\ff(\vv_n))\Big) = \det(\gg) h(\ff(\vv_1), ..., \ff(\vv_n)) =
       \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   Thus
   
   \begin{align*}
       \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) = \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   This is a statement on the tuple $(\vv_1, ..., \vv_n) \in V^{\times n}$. Extending this statement to a statement on any tuple $\TT \in V^{\times n}$, we have $\det(\gg \circ \ff) h(\TT) = \det(\gg) \det(\ff) h(\TT)$. Thus ${\det(\gg \circ \ff) - \det(\gg) \det(\ff)) h(\TT) = \mathbf{0}}$ for all $\TT \in V^{\times n}$. Since we can choose $h$ such that $h(\TT) \neq 0$ for some $\TT \in V^{\times n}$, this forces $\det(\gg \circ \ff) - \det(\gg) \det(\ff) = 0$, giving us $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$, as desired.
\end{proof}

\begin{theorem}
    (Determinant of an inverse function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Then $\det(\ff^{-1}) = \frac{1}{\det(\ff)}$.
\end{theorem}

\begin{proof}
   We have $\det(\ff \circ \ff^{-1}) = \det(\II) = 1$, and $\det(\ff \circ \ff^{-1}) = \det(\ff) \det(\ff^{-1})$ by the previous theorem, so $\det(\ff) \det(\ff^{-1}) = 1$.
\end{proof}

\newpage

\section{Orientation of finite-dimensional vector spaces}
\label{ch::lin_alg::section::orientation}
%\begin{itemize}
%    \item \url{https://arxiv.org/pdf/1103.5263.pdf}
%    \item \url{https://en.wikipedia.org/wiki/Cartan\%E2\%80\%93Dieudonn\%C3\%A9_theorem}
%    \item \url{https://en.wikipedia.org/wiki/Rotor_(mathematics)#:~:text=A\%20rotor\%20is\%20an\%20object,the\%20Cartan\%E2\%80\%93Dieudonn\%C3\%A9\%20theorem).}
%    \item \url{https://en.wikipedia.org/wiki/Geometric_algebra#Rotating_systems}
%    \item \url{https://www.euclideanspace.com/maths/algebra/clifford/d4/transforms/index.htm}
%\end{itemize}

\textit{Orientation} is the mathematical formalization of the notions of ``clockwise'' and ``counterclockwise''; it is the notion which distinguishes different ``rotational configurations'' from each other.

Our discussion of orientation will be as follows. First, we define an \textit{orientation on a vector space} to be a choice of an orthonormal ordered basis. (We heavily rely on inner product spaces for their inner-product-induced orthonormality). This definition of orientation will only allow us to check the orientation of permutations of the chosen orthonormal basis, however. In order to give orientation to arbitrary ordered bases, we introduce rotations in $n$-dimensions, so that an arbitrary ordered basis can be given the orientation of a ``close-by'' permuted ordered basis.

After we finish the definition of orientation for inner product spaces, we end the subsection on oriented inner product spaces by presenting the fact that the determinant ``tracks'' orientation. This fact allows us to generalize the notion of orientation to finite dimensional vector spaces that may or may not have an inner product. Lastly, we show how the top exterior power of a finite-dimensional vector space can be used for the purposes of orientation.

[TO-DO: fix the terminology of the above]

\subsection*{First notions of orientation}

\subsubsection*{First notions of orientation in two dimensions}

\begin{defn}
    (Ordered basis).
    
    An \textit{ordered basis} for a vector space is a simply a tuple containing some ordering of the basis vectors for that space.
    
    For example, if $V$ is a 2-dimensional vector space and has basis $E = \{\ee_1, \ee_2\}$, then $(\ee_1, \ee_2)$ and $(\ee_2, \ee_1)$ are both ordered bases of $V$. We have $(\ee_1, \ee_2) \neq (\ee_2, \ee_1)$.
\end{defn}

We now discover a consequence of imposing that the bases under consideration be ordered.

\begin{deriv}
\label{ch::lin_alg::deriv::ordered_bases_antisymmetry_intuition}
    (Intuition for the antisymmetry of ordered bases).
    
    Consider the plane $\R^2$, and consider also two permutations of the standard ordered basis $\sE = (\see_1, \see_2)$ for $\R^2$: $(\see_1, \see_2)$ and $(\see_1, -\see_2)$. (Draw these ordered bases out on paper). Notice that no matter how you rotate the entire second ordered basis (rotate each vector in the second ordered basis by the same amount), it is impossible to make all vectors from the second ordered basis simultaneously align with their counterparts from the first ordered basis. This is also impossible for the ordered bases $(\see_1, \see_2)$ and $(\see_2, \see_1)$. Finally, consider the ordered bases $(\see_1, \see_2)$ and $(-\see_2, \see_1)$ of $\R^2$. It \textit{is} possible to make each vector from the first ordered basis with its counterpart from the second ordered basis by rotating either the entire first ordered basis or the entire second ordered basis.
    
    What we have discovered is that \textit{swapping adjacent vectors in an ordered basis of two vectors produces an ordered basis that is} equivalent under rotation \textit{to the ordered basis obtained from the original by negating one of the vectors that have been swapped}. We refer to this fact as the \textit{antisymmetry of orthonormal ordered bases}.
\end{deriv}

We would now like to work towards a more precise statement of the antisymmetry of orthonormal ordered bases. The notion of \textit{rotational equivalence} is what will facilitate this formalization. Before we define rotational equivalence, however, we must formalize what a ``rotation'' is. The following definition accomplishes this.

\begin{defn}
\label{ch::lin_alg::defn::2-rotation}
    ($2$-dimensional rotation).
    
    Let $V$ be a $2$-dimensional inner product space, and let $\hU$ be an orthonormal ordered basis for $V$.
    A \textit{$2$-dimensional rotation on $V$} is a linear function $\RR_\theta:V \rightarrow V$ whose matrix relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix},
        \text{ where $\theta \in [0, 2\pi)$}.
    \end{align*}
\end{defn}

\begin{defn}
    (Equivalence under rotation for $2$-dimensional inner product spaces).

    We define orthonormal ordered bases $\hU = (\huu_1, \huu_2)$ and $\hW = (\hww_1, \hww_2)$ of a 2-dimensional inner product space\footnote{Note, this is the first time we have required that the vector space under consideration be an inner product space. We need this constraint so that we can speak of orthonormal bases.} to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).

    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_2_dimensions}

    (Antisymmetry of orthonormal ordered bases for a $2$-dimensional inner product space).
    
    Now we see how the notion of rotational equivalence for $2$-dimensional inner product spaces formalizes the antisymmetry of orthonormal ordered bases. Let $\hU = (\huu_1, \huu_2)$ be an orthonormal ordered basis of a $2$-dimensional inner product space. When $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2} \}$, the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \pm
        \begin{pmatrix}
            0 & -1 \\
            1 & 0
        \end{pmatrix}.
    \end{align*}
    
    Computing $\RR_\theta(\hU) = \RR_\theta((\huu_1, \huu_2)) = (\RR_\theta(\huu_1), \RR_\theta(\huu_2))$ for $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$, we see that the following ordered bases are rotationally equivalent:
    
    \begin{align*}
        (\huu_2, \huu_1 ) &\sim (-\huu_1, \huu_2) \sim (\huu_1, - \huu_2) \\
        (\huu_1, \huu_2) &\sim (-\huu_2, \huu_1) \sim (\huu_2, -\huu_1)
    \end{align*}

    This is what we noticed in the informal discussion of Derivation \ref{ch::lin_alg::deriv::ordered_bases_antisymmetry_intuition}.
\end{theorem}

\subsubsection*{First notions of orientation in $n$ dimensions}

To study orientation in $n$ dimensions, we generalize the key notion of rotational equivalence by ``extending'' $2$-dimensional rotations so that they can be applied to elements of $n$-dimensional inner product spaces. 

\begin{defn}
    (Extension of a function).

    Let $X$ and $Y$ be sets, let $X_1 \subset X$ be subset of $X$, and consider a function $f:X_1 \rightarrow Y$. The \textit{extension of $f$ to $X$} is the function $f_{\text{ext}}$ defined by

    \begin{align*}
        f_{\text{ext}}(x) =
        \begin{cases}
            f(x) & x \in X_1 \\
            x & x \notin X_1 
        \end{cases}.
    \end{align*}
    
    We will speak frequently of extensions of $2$-dimensional rotations. In fact, we will engage in a bit of abuse of notation: when $V$ is an inner product space, we will say ``the $2$-dimensional rotation $\RR_\theta$ on $V$'' to mean ``the extension of the $2$-dimensional rotation $\RR_\theta$ to $V$'', and use $\RR_\theta$ to denote $(\RR_\theta)_{\text{ext}}$.
    
    It's helpful to spell out a further detail. If $\RR_\theta$ is a $2$-dimensional rotation on a finite-dimensional inner product space and $\hU$ is an orthonormal basis of the space, then the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is    
    
    \begin{align*}
        [\RR_\theta(\hU)]_{\hU} =
        \kbordermatrix
        {
             & & & \text{$i$th column} &  & \text{$j$th column}   \\
             & 1 & \hdots & \cos(\theta) & 0 & -\sin(\theta) & 0 \\
             & 0 & & 0 & \vdots & 0 & \vdots \\
             & 0 & & \vdots & 1 & \vdots & \vdots \\
             & \vdots & & 0 & \vdots & 0 & \vdots \\
             & 0 & \hdots & \sin(\theta)  & 0 & \cos(\theta) & 1
        }.
    \end{align*}
    
    (The columns other than the $i$th and $j$th columns are the columns of the $n \times n$ identity matrix).
\end{defn}

Now that we are equipped with $2$-rotations that can be applied on finite-dimensional inner product spaces, we can generalize our definition of rotational equivalence to $n$ dimensions.

\begin{defn}
    (Equivalence under rotation).
    
    We define orthonormal ordered bases $\hU$ and $\hW$ of a finite-dimensional inner product space to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).
    
    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

\begin{defn}
    (Permutation acting on a tuple). 
    
    Let $X = (x_1, ..., x_n)$ be an $n$-tuple. Given a permutation $\sigma \in S_n$, we define $X^\sigma := (x_{\sigma(1)}, ..., x_{\sigma(n)})$. For example, if $E = (\ee_1, ..., \ee_n)$ is a basis of a finite-dimensional vector space, then $E^\sigma = (\ee_{\sigma(1)}, ..., \ee_{\sigma(n)})$.
\end{defn}

\begin{theorem}
    \label{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}

    (Permutations preserve rotational equivalence).
    
    If $\hU$ and $\hW$ are orthonormal ordered bases of an $n$-dimensional inner product space, then ${\hU \sim \hW \iff \hU^\sigma \sim \hW^\sigma}$ for all permutations $\sigma \in S_n$.
\end{theorem}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$ and $\hW = (\hww_1, ..., \hww_n)$. We have that $\hU \sim \hW$ iff there exists $\theta \in [0, 2\pi)$ such that $\hW = \RR_\theta(\hU)$, which is true iff $\hww_i = \RR_\theta(\huu_i)$ for all $i$. This is true iff for all $\sigma \in S_n$ we have $\hww_{\sigma(i)} = \RR_\theta(\huu_{\sigma(i)})$, which is equivalent to $\hU^\sigma \sim \hW^\sigma$.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_general}

    (Antisymmetry of orthonormal ordered bases for an $n$-dimensional inner product space).

    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. Similarly to what was done in Theorem \ref{ch::lin_alg::thm::antisymmetry_ordered_bases_2_dimensions}, we use $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$ in the matrix relative to $\hU$ and $\hU$ of a $2$-dimensional rotation to obtain the formal characterization of the antisymmetry of orthonormal ordered bases.
    
    For any orthonormal ordered basis $\hU = (\huu_1, ..., \huu_n)$ of an $n$-dimensional inner product space, we have
    
    \begin{align*}
        (\huu_1, ..., \huu_i, ..., \huu_j, ..., \huu_n)
        \sim
        (\huu_1, ..., -\huu_j, ..., \huu_i, ..., \huu_n)
        \sim
        (\huu_1, ..., \huu_j, ..., -\huu_i, ..., \huu_n) \text{ for all $i, j \in \{1, ..., n\}$}.
    \end{align*}
\end{theorem}

This fundamental result leads to the following [...].

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_signs}

    (Orthonormal ordered bases of signed vectors).
    
    Let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis of an $n$-dimensional inner product space. If $s_1, ..., s_n \in \{-1, 1\}$, then

    \begin{align*}
        (s_1 \huu_1, ..., s_n \huu_n) \sim
        \begin{cases}
            (\huu_1, ..., \huu_n), & \text{the number of $s_i$ equal to $-1$ is $-1$ is even} \\
            (\huu_1, ..., \huu_{j - 1}, -\huu_j, \huu_{j + 1}, ..., \huu_n) \text{ for any $j \in \{1, ..., n\}$}, & \text{the number of $s_i$ equal to $-1$ is $-1$ is odd} 
        \end{cases}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\
    
    (Case: the number of $s_i$ equal to $-1$ is even). Since the number of $s_i$ equal to $-1$ is even, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. If we swap the vectors in each pair twice, then, as illustrated by the example $(-\huu_i, -\huu_j) \sim (\huu_j, -\huu_i) \sim (\huu_i, \huu_j)$, the coefficients on these vectors in the rotationally equivalent orthonormal ordered basis will be $1$, with the order of the vectors being the same as in the original pair. Thus $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_n)$.

    (Case: the number of $s_i$ equal to $-1$ is odd). Since the number of $s_i$ equal to $-1$ is odd, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all but one $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. Let $s_k \huu_k$ be the vector with $s_k = -1$ that is not a member of a pair. Applying the same reasoning as in the previous case, we see that $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ... \huu_n)$. Making use of the example $(-\huu_k, \huu_\ell) \sim (\huu_\ell, \huu_k) \sim (\huu_k, -\huu_\ell)$, we see that this orthonormal ordered basis is rotationally equivalent to $(\huu_1, \huu_2, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Thus, when the number of $s_i$ equal to $-1$ is odd, the orthonormal ordered basis $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Replace $\ell$ with $j$ to obtain the result.
\end{proof}

\begin{lemma}
    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. We have $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$. The sign of a permutation is $1$ iff the permutation consists of an even number of swaps. So, an equivalent statement to the theorem is: for all $n$, if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We prove this statement by induction on $n$.
    
    (Base case). Consider $\hU^\sigma$, where $\sigma = \tau \circ \pi$, and where $\pi, \tau$ are transpositions. 
    
    The permutation $\pi$ swaps some $i, j$. Using the example $(\huu_j, \huu_i) \sim (-\huu_i, \huu_j)$, we see \\ $\hU^\pi \sim (\huu_1, ..., \huu_{i - 1}, -\huu_i, \huu_{i + 1}, ..., \huu_n)$. Since this orthonormal ordered basis has only one basis vector with a coefficient of $-1$, it follows from the previous theorem that it is rotationally equivalent to every other such orthonormal ordered basis; in particular, it is rotationally equivalent to $\hW :=  (-\huu_1, ..., \huu_n)$.

    So far, we've shown $\hU^\pi \sim \hW$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}), we may apply the permutation $\tau$ to each side of the rotational equivalence $\hU^\pi \sim \hW$ to obtain $(\hU^\pi)^\tau = \hU^{\tau \circ \pi}  = \hU^\sigma \sim \hW^\tau$.

    We now compute the right side of this rotational equivalence, $\hW^\tau$.
    
    \indent \indent (Case: $\tau$ swaps $1$ with some $\ell > 1$). By studying the example $(\huu_\ell, -\huu_1) \sim (\huu_1, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (\huu_1, ..., \huu_n) = \hU$.
    
    \indent \indent (Case: $\tau$ swaps $k \neq 1$ with some $\ell > k$). By studying the example $(\huu_\ell, \huu_k) \sim (-\huu_k, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (-\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ..., \huu_n)$. In this last orthonormal ordered basis, the number of basis vectors with a coefficient of $-1$ is even. It follows from the previous theorem that this last orthonormal ordered basis is rotationally equivalent to $\hU$.

    We've shown that $\hW^\tau = \hU$ for all $\tau \in S_n$. Therefore, we have $\hU^\sigma \sim \hW^\tau \sim \hU$, as desired.
    
    (Inductive case). Assume as the inductive hypothesis if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We need to prove that if $\sigma$ consists of $2(n + 1)$ transpositions, then $\hU^\sigma \sim \hU$.

    So, assume $\sigma$ consists of $2(n + 1) = 2n + 2$ transpositions. Then $\sigma = \tau \circ \pi$, where $\pi$ consists of $2n$ transpositions and $\tau$ consists of $2$ transpositions. By the inductive hypothesis, $\hU^\pi \sim \hU$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}), then $(\hU^\pi)^\tau = \hU^{\tau \circ \pi} = \hU^\sigma \sim \hU^\tau$. Since $\tau$ consists of $2$ transpositions, then from the base case it follows that $\hU^\tau \sim \hU$. Thus $\hU^\sigma \sim \hU^\tau \sim \hU$, as desired.
\end{proof}

\begin{theorem}
    \begin{align*}
        \hU^\sigma \sim \hU^\pi \iff \sgn(\sigma) = \sgn(\pi) \text{ for all $\sigma, \pi \in S_n$}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    ($\impliedby$).
    
    The reverse implication is true when the following two statements are:
    
    \begin{enumerate}
        \item If $\sgn(\pi) = 1$, then $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
        \item If $\sgn(\pi) = -1$, then $\sgn(\sigma) = -1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
    \end{enumerate}

    We restate the previous lemma for convenience:

    \begin{itemize}
        \item $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
    \end{itemize}

    Now we prove (1) and (2).

    \begin{enumerate}
        \item If $\sgn(\sigma) = \sgn(\pi) = 1$, then $\hU^\sigma \sim \hU \sim \hU^\pi$ by the above bullet.
        \item Let $\pi$ be any odd permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi$, i.e. $\hU^{\sigma \circ \pi} \sim \hU^\pi$. Thus $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} = \hU^\pi$. In general, every odd permutation is of the form $\sigma \circ \pi$, where $\sigma$ is some even permutation. Thus, as $\sigma$ varies over the even permutations, the permutation $\tau := \sigma \circ \pi$ varies over the odd permutations. This makes the statement $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} \sim \hU^\pi$ equivalent to the statement $\sgn(\tau) = -1 \implies \hU^\tau \sim \hU^\pi$. Since the former statement is true, so is the later, as desired.
    \end{enumerate}

   ($\implies$).

    The forward implication is true when the following two statements are:

    \begin{enumerate}
        \item[3.] If $\sgn(\pi) = 1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.
        \item[4.] If $\sgn(\pi) = -1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = -1$ for all $\sigma \in S_n$.
    \end{enumerate}

    Before we can prove (3) and (4), we need to prove the following bullet point:

    \begin{itemize}
        \item $\hU \sim \hU^\sigma \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.
        \begin{itemize}
            \item (Proof). We show the contrapositive, $\sgn(\sigma) = -1 \implies \hU \nsim \hU^\sigma$ for all $\sigma \in S_n$. Because of (2), it suffices to show that $\hU \nsim \hU^\sigma$ for \textit{any} $\sigma$ with $\sgn(\sigma) = -1$.

            \textbf{Show $(\huu_1, \huu_2) \nsim (-\huu_1, \huu_2)$, and then that $(\huu_1, \huu_2, ..., \huu_n) \nsim (-\huu_1, \huu_2, ..., \huu_n)$ by induction.}
        \end{itemize}
    \end{itemize}

    Now we prove (3) and (4).
    
    \begin{enumerate}
        \item[3.] Let $\pi$ be any even permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$, i.e., $\hU^{\sigma \circ \pi} \sim \hU^\pi \implies \sgn(\sigma) = 1$. If we allow $\pi$ to vary over the even permutations, then since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the even permutations, and we obtain the statement $\hU^\tau \sim \hU^\pi \implies \sgn(\tau) = 1$ for all $\tau \in S_n$, as desired.
        \item[4.] The proof is analogous to that of (3). Begin the proof with ``Let $\pi$ be any odd permutation'', and use the fact that since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the odd permutations.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Orientation of permuted ordered bases).

    \textbf{LOOK OVER THIS. MAYBE EDIT IN LIGHT OF NEW PREVIOUS THEOREM}
    
    This means that there are only two equivalence classes\footnote{I find this relatively surprising. My intuition is that there would be something like $2^n$ or $n!$ equivalence classes of ``equivalence under rotation'' in $n$ dimensions, but nope! There are $2$ equivalence classes of ``equivalence under rotation'' for every $n$.} of ``equivalence under rotation''.
    
    We can now begin to set up the notion of orientation. An \textit{orientation for the $n$-dimensional inner product space $V$} is a choice of an orthonormal ordered basis $\hU$ for $V$. When $V$ is given the orientation $\hU$, then the \textit{orientation of a permutation of $\hU$ (relative to $\hU$)} is said to be \textit{positive} iff that permutation of $\hU$ is rotationally equivalent to $\hU$, and is said to be \textit{negative} otherwise. Per the previous paragraph, every permutation of $\hU$ is either positively oriented or negatively oriented relative to $\hU$.
\end{defn}

\begin{remark}
\label{ch::lin_alg::rmk::formalization_ccw_cw}
    (The formalization of ``counterclockwise'' and ``clockwise'').
    
    At the beginning of this section, we said that orientation would formalize the notions of ``clockwise'' and ``counterclockwise''. This formalization has been achieved by the previous definition.
    
    A \textit{counterclockwise rotational configuration} is another name for the orientation given to $\R^3$ by the standard basis, \textit{when we use the normal} human \textit{convention} of drawing the ordered basis $\sE = (\see_1, \see_2, \see_3)$ such that $\sE$ can be rotated so that $\see_1$ points out of the page, $\see_2$ points to the right, and $\see_3$ points upwards on the page. In this visual convention, the direction of each basis vector corresponds to its position in $\sE$. Counterclockwise rotational configurations are also called ``right handed coordinate systems''.
    
    A \textit{clockwise rotational configuration} then corresponds to the ordered bases which are not rotationally equivalent to $\sE$. One such ordered basis, $(-\see_1, \see_2, \see_3)$, can be depicted using the visual convention just established by drawing $\see_1$ as pointing into the page (i.e. $-\see_1$ points out of the page), $\see_2$ as pointing to the right, and $\see_3$ as pointing upwards on the page. Clockwise rotational configurations are also called ``left handed coordinate systems''.
    
    We could have easily picked a different visual convention (i.e. a different permutation of in/out, left/right, up/down) to represent the ordering of the basis that is considered to orient the space.
\end{remark}

At this point, we need some definitions and facts about $n$-rotations before we complete our development of orientation.

\subsection*{Orientation in $n$ dimensions}

\begin{defn}
    ($n$-dimensional rotations by Euler angles).

    Let $V$ be an $n$-dimensional inner product space and let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis for $V$. An \textit{($n$-dimensional) Euler rotation} is a function $\RR_{\theta_1, ..., \theta_k}:V \rightarrow V$ of the form ${\RR_{\theta_1, ..., \theta_k} = \RR_{k, \theta_k} \circ ... \circ \RR_{1, \theta_1}}$, where, for each $i$, $\RR_{i, \theta_i}$ is the $2$-dimensional rotation by $\theta_i \in [0, 2\pi)$ on the subspace $\spann(\hU - \{\huu_i\})$. The angles $\theta_1, ..., \theta_k$ are called \textit{Euler angles}.
\end{defn}

\begin{theorem}
    All Euler rotations are orthogonal linear functions.
\end{theorem}

\begin{proof}
    This follows because $2$-dimensional rotations are orthogonal linear functions, and since a composition of orthogonal linear functions is another orthogonal linear function.
\end{proof}

% \begin{theorem}
%     Outline of proof that determinant tracks orientation:

%     \begin{itemize}
%         \item arbitrary ordered basis of $V$ -> orthonormal ordered basis of $V$ via Gram-Schmidt
%         \item orthonormal ordered basis of $V$ -> permutation of $\hU$ via Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
%         \item Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}: any ordered basis is rotationally close to a permuted orthonormal basis
%     \end{itemize} 

%     ``To do so, we first choose $\alpha, \beta, \gamma$ such that $\RR(\hww_1) = \huu_1$.''    
% \end{theorem}

\begin{comment}
\begin{lemma}
    (Standard matrix of Euler rotation).

    Let $V$ be a $3$-dimensional inner product space and let $\hU$ be an orthonormal ordered basis for $V$. Consider a rotation $\RR_{\alpha, \beta, \gamma}$ by the Euler angles $\alpha, \beta, \gamma \in [0, 2\pi)$. The matrix of $\RR_{\alpha, \beta, \gamma}$ relative to $\hU$ and $\hU$ is the product of the standard matrices of $\RR_\alpha$, $\RR_\beta$, and $\RR_\gamma$, and is equal to

    \begin{align*}
        &\begin{pmatrix}
            \cos(\gamma) & -\sin(\gamma) & 0 \\ \sin(\gamma) & \cos(\gamma) & 0 \\
            0 & 0 & 1
        \end{pmatrix} 
        \begin{pmatrix}
            \cos(\beta) & 0 & \sin(\beta) \\
            0 & 1 & 0 \\
            -\sin(\beta) & 0 & \cos(\beta)
            \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & \cos(\alpha) & -\sin(\alpha) \\
            0 & \sin(\alpha) & \cos(\alpha)
        \end{pmatrix} \\
        = 
        &\begin{pmatrix}
            \cos(\gamma)\cos(\beta) & \cos(\gamma)\sin(\beta)\sin(\alpha)-\sin(\gamma)\cos(\alpha) & \cos(\gamma)\sin(\beta)\cos(\alpha)+\sin(\gamma)\sin(\alpha) \\
            \sin(\gamma)\cos(\beta) & \sin(\gamma)\sin(\beta)\sin(\alpha)+\cos(\gamma)\cos(\alpha) & \sin(\gamma)\sin(\beta)\cos(\alpha)-\cos(\gamma)\sin(\alpha) \\
            -\sin(\beta) & \cos(\beta)\sin(\alpha) & \cos(\beta)\cos(\alpha)
        \end{pmatrix}.
    \end{align*}
\end{lemma}

\begin{proof}
    Left as an exercise :)
\end{proof}
\end{comment}

\begin{lemma}
    (In three dimensions, there is an Euler rotation taking any nonzero vector to any other vector).

    Let $V$ be a $3$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, \huu_2, \huu_3)$ be an orthornormal ordered basis of $V$. If we can find angles $\alpha_1, \beta_1$ that rotate $\vv$ to align with $\huu_3$,

    \begin{align*}
        (\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3,
    \end{align*}
    
    and angles $\alpha_2, \beta_2$ that rotate $\huu_3$ to align with $\ww$, 

    \begin{align*}
        (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww,
    \end{align*}

    then the Euler rotation $\RR := \RR_{2, \beta_2} \circ \RR_{1, \alpha_2} \circ \RR_{2, \beta_1} \circ \RR_{1, \alpha_1}$ satisfies $\RR(\vv) = \ww$, the lemma is proven.

    \vspace{.5cm}

    First, we find $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    Letting $(v_1, v_2, v_3)^\top := [\vv]_{\hU}$, we have
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_1) & -\sin(\alpha_1) \\
                0 & \sin(\alpha_1) & \cos(\alpha_1)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\ v_2 \\ v_3
            \end{pmatrix}
        }_{[\vv]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \\
            v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) \\
            v_2 \sin(\alpha_1) + v_3 \cos(\alpha_1)
        \end{pmatrix}.
    \end{align*}

    $\alpha_1$ is such that the second component of the above is zero,

    \begin{align*}
        v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) = 0.
    \end{align*}
    
    One value of $\alpha_1$ that solves this equation is $\alpha_1 = \arctan(v_2/v_3)$. With this choice of $\alpha_1$, we have \\ ${\cos(\alpha_1) = v_3/\sqrt{v_2^2 + v_3^2}}$ and ${\sin(\alpha_1) = v_2/\sqrt{v_2^2 + v_3^2}}$. Thus the above is
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \begin{pmatrix}
            v_1 \\
            (v_2 v_3)/\sqrt{v_2^2 + v_3^2} - (v_3 v_2)/\sqrt{v_2^2 + v_3^2} \\
            (v_2^2 + v_3^2)/\sqrt{v_2^2 + v_3^2}
        \end{pmatrix}
        =
        \begin{pmatrix}
            v_1 \\
            0 \\
            \sqrt{v_2^2 + v_3^2}
        \end{pmatrix}.
    \end{align*}

    Now we have

    \begin{align*}
        [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} &=
        [\RR_{2, \beta_1}(\RR_{1, \alpha_1}(\vv))]_{\hU} = [\RR_{2, \beta_1}(\hU)]_{\hU} [\RR_{1, \alpha_1}(\vv)]_{\hU} \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_1) & 0 & -\sin(\beta_1) \\
                0 & 1 & 0 \\
                \sin(\beta_1) & 0 & \cos(\beta_1)
            \end{pmatrix}
        }_{[\RR_{2, \beta_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\
                0 \\
                \sqrt{v_2^2 + v_3^2}
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\vv)]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) \\
            0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1)
        \end{pmatrix}.
    \end{align*}

    $\beta_1$ is such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3 \iff [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} = ||\vv||\see_3$. So $\beta_1$ satisfies the following system of equations:

    \begin{align*}
        \begin{cases}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) = 0 \\
            0 = 0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1) = ||\vv||
        \end{cases}.
    \end{align*}

    One value of $\beta_1$ that solves the first equation is $\beta_1 = \arctan(v_1/ \sqrt{v_2^2 + v_3^2})$. 
    
    Since $\cos(\beta_1) = \sqrt{v_2^2 + v_3^2}/||\vv||$ and $\sin(\beta_1) = v_1/||\vv||$, the third equation is also satisfied:

    \begin{align*}
        \frac{v_1^2}{||\vv||} + \frac{\sqrt{v_2^2 + v_3^2}^2}{||\vv||} = \frac{v_1^2 + v_2^2 + v_3^2}{||\vv||} = \frac{||\vv||^2}{||\vv||} = ||\vv||.
    \end{align*}

    Thus we have found $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    \vspace{.5cm}

    Now, we find $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
    
    We have

    \begin{align*}
        [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU} &=
        [\RR_{2, \beta_2}(\hU)]_{\hU} [\RR_{1, \alpha_2}(\hU)]_{\hU} \huu_3 \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_2) & 0 & -\sin(\beta_2) \\
                0 & 1 & 0 \\
                \sin(\beta_2) & 0 & \cos(\beta_2)
            \end{pmatrix}
        }_{[\RR_{2, \beta_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_2) & -\sin(\alpha_2) \\
                0 & \sin(\alpha_2) & \cos(\alpha_2)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                0 \\ 0 \\ 1
            \end{pmatrix}
        }_{\huu_3} \\
        &=
        \begin{pmatrix}
            \cos(\beta_2) & 0 & -\sin(\beta_2) \\
            0 & 1 & 0 \\
            \sin(\beta_2) & 0 & \cos(\beta_2)
        \end{pmatrix}
        \begin{pmatrix}
            0 \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2)
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\cos(\alpha_2) \sin(\beta_2) \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2) \cos(\beta_2)
        \end{pmatrix}.
    \end{align*}

    \newcommand{\hw}{\hat{w}}

    $\alpha_2$ and $\beta_2$ are such that $\hww = (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) \iff [\hww]_{\hU} = [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU}$. Letting $\hw_i$ denote the $i$th component of $[\hww]_{\hU}$, we see $\alpha_2$ and $\beta_2$ must satisfy the following system of equations:

    \begin{align*}
        \begin{cases}
            \hw_1 = -\cos(\alpha_2) \sin(\beta_2) \\
            \hw_2 = -\sin(\alpha_2) \\
            \hw_3 = \cos(\alpha_2) \cos(\beta_2)
        \end{cases}.
    \end{align*}

    One value of $\alpha_2$ that solves the second equation is $\alpha_2 = -\arcsin(\hw_2)$. Dividing the first equation by the third, we have $\hw_1/\hw_3 = -\tan(\beta_2)$ and thus one value of $\beta_2$ that solves the first equation is $\beta_2 = \arctan(-\hw_1/\hw_3) = -\arctan(\hw_1/\hw_3)$.

    Thus we have found $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
\end{proof}

The previous lemma can easily be generalized so that it holds for all dimensions greater than or equal to $2$.

\begin{lemma}
    (There is an Euler rotation taking any nonzero vector to any other vector).
    
    Let $V$ be an $n$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    \mbox{} \\

    (Case: $n = 2$). The rotation $\RR_{\theta(\vv, \ww)}$ by $\theta(\vv, \ww)$ sends $\vv$ to $\ww$.
    
    (Case: $n = 3$). See the previous lemma.
    
    (Case: $n > 3$).
    
    \indent (Case: $n$ is even). [TO-DO]
    
    \indent (Case: $n$ is odd). [TO-DO]
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
     (Any ordered basis is ``rotationally close'' to a permuted orthonormal basis).
     
     Let $V$ be an finite-dimensional inner product space. If $\hW = (\hww_1, ..., \hww_k)$ and $\hU = (\huu_1, ..., \huu_k)$ are orthonormal ordered bases of subspaces of $V$, then there is an Euler rotation taking $\hW$ to an ordered basis that is rotationally equivalent to some permutation $\hU^\sigma$ of $\hU$. 
\end{theorem}

\begin{proof}
    By the previous lemma, we know there is an Euler rotation $\RR$ that satisfies $\RR(\hww_1) = \huu_1$. 
    
    We first prove that for all $i$ we have $\RR(\hww_i) = s_i \huu_i$, where $s_i \in \{-1, 1\}$. And we show this by using induction to show that for all $i$, $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    (Base case). We have $R(\hww_1) = \huu_1 = s_1 u_1$, where $s_1 = 1$.

    (Inductive case). Assume that $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    Since $\RR$ is an orthogonal linear function, it preserves orthonormality (\textbf{see Theorem [...]}), and so $(\RR(\hww_1), ..., \RR(\hww_k))$ is orthonormal. Thus, for each $j$, $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \cancel{\RR(\hww_j)}, ..., \RR(\hww_k)$. In particular, when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \RR(\hww_i)$. By the inductive hypothesis, $\RR(\hww_1) = s_1 \huu_1, ..., \RR(\hww_i) = s_i \huu_i$, where $s_1, ..., s_i \in \{-1, 1\}$. So when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $s_1 \huu_1, ..., s_i \huu_i$; it follows that each $\RR(\hww_j)$ with $j \geq i + 1$ is perpendicular to all of $\huu_1, ..., \huu_i$. This means that each $\RR(\hww_j)$ with $j \geq i + 1$ is in the orthogonal complement of $\spann((\huu_1, …, \huu_i))$.
    
    This orthogonal complement is equal to $\spann((\huu_{i + 1}, …, \huu_k)$. Therefore $\RR(\hww_{i + 1}), ..., \RR(\hww_k) \in \spann(\huu_{i + 1}, ..., \huu_k)$. Since $\RR$ preserves orthonormality (\textbf{again, see Theorem [...]}), then $\RR(\hww_{i + 1}), ..., \RR(\hww_k)$ are also orthonormal.
    
    Since $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1}, ..., \huu_k)$, then $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1} + c_{i + 2} \huu_{i + 2} + ... + c_k \huu_k$ for some scalars $c_{i + 1}, ..., c_k$. We claim that we must have $c_{i + 2} = ... = c_k = 0$; assume for contradiction that $c_{\ell_1}, ..., c_{\ell_p}$ are nonzero scalars with $\ell_1, ..., \ell_p > i + 1$. A quick inner product computation shows that $\RR(\hww_{i + 1})$ is not perpendicular to any of $\huu_{\ell_1}, ..., \huu_{\ell_p}$. Since $\RR(\hww_{i + 2}), ..., \RR(\hww_k)$ must all be perpendicular to $\RR(\hww_{i + 1})$, it follows that $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \notin \spann((\huu_{\ell_1}, ..., \huu_{\ell_p}))$. Thus $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \in \spann((\huu_{i + 2}, ..., \huu_k) - (\huu_{\ell_1}, ..., \huu_{\ell_p}))$, which is a subspace of dimension $|[i + 2, k] \cap \Z| - p$. On the other hand, since $(\RR(\hww_{i + 2}, ..., \RR(\hww_k))$ is an orthonormal basis, then $(\RR(\hww_{i + 2}), ..., \RR(\hww_k))$ span a $|[i + 2, k]|$-dimensional space. This is a contradiction; vectors cannot be in a space that has a lesser dimension than their span. Therefore, $c_{i + 2} = ... = c_k = 0$, and $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1}$, i.e., $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1})$. Since $\RR(\hww_{i + 1})$ is a unit vector, then $||\RR(\hww_{i + 1})|| = ||s_{i + 1} \huu_{i + 1}|| = |s_{i + 1}| = 1$. Thus $s_{i + 1} \in \{-1, 1\}$, and $\RR(\hww_{i + 1}) = s_{i + 1} \huu_{i + 1}$, as desired.

    \vspace{.25cm}

    Since $\RR(\hww_i) = s_i \huu_i$ for all $i$, where $s_i \in \{-1, 1\}$, then $\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k)$, where $s_1, ..., s_k \in \{-1, 1\}$. 

    If the number of $s_i$ equal to $-1$ is even, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = 1$, and if the number of $s_i$ equal to $-1$ is odd, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = -1$. (Recall Derivation     \ref{ch::lin_alg::thm::antisymmetry_ordered_bases_signs}). We see that in either case, ${\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for some $\sigma \in S_k$. This proves the theorem.
\end{proof}

\subsubsection*{Completing the definition of orientation for finite-dimensional inner product spaces}

\begin{defn}
    (Orientation of arbitrary ordered bases).
    
    Let $V$ be an $n$-dimensional inner product space, and fix an orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ for $V$. We know how to ``orient'' ordered bases for $V$ that happen to be permutations of $\hU$. Now, we generalize the notion of orientation so that it applies to any orthonormal ordered basis of $V$.
    
    We define the \textit{orientation of an orthonormal ordered basis $E$ of $V$} that is not a permutation of $\hU$ to be the orientation of the unique permuted orthonormal basis $\hU^\sigma$, $\sigma \in S^n$, of $\hU$ for which there exists an $n$-rotation taking $E$ to $\hU^\sigma$.
    
    Then, we define the \textit{orientation of an arbitrary orthonormal ordered basis $E$ of $V$} to be the orientation of the unique orthonormal basis $\hU_E$ obtained from performing the Gram-Schmidt process on $E$ (see Theorem \ref{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}).
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::det_tracks_orientation}
    (The determinant tracks orientation). 
    
    Let $V$ be an $n$-dimensional inner product space with an orientation given by an orthonormal ordered basis $\hU$. Let $E = (\ee_1, ..., \ee_n)$ be any ordered basis (not necessarily orthonormal) of $V$. We have $\det([\EE]_{\hU}) > 0$ iff $E$ is positively oriented relative to $\hU$, and $\det([\EE]_{\hU}) < 0$ iff $E$ is negatively oriented relative to $\hU$.
\end{theorem}

\begin{proof}
   This proof has two overarching steps. First, we pass the definition of orientation for arbitrary ordered bases of $V$ to the definition of orthonormal ordered bases of $V$ by obtaining an orthonormal ordered basis $\hU_E$ from $E$. Then we pass the definition of orientation for orthonormal ordered bases of $V$ that are not permutations of $\hU$ to the definition of orientation for orthonormal ordered bases of $V$ that are permutations of $\hU$.
   
   To begin the first step, consider $\det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$, and perform Gram-Schmidt on $([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$. In the $i$th step of Gram-Schmidt, a linear combination of the vectors $[\ee_1]_{\hU}, ..., \cancel{[\ee_i]_{\hU}}, ..., [\ee_n]_{\hU}$ is added to $[\ee_i]_{\hU}$. Recall from Theorem \ref{ch::lin_alg::thm::consequent_det_props} that the determinant is invariant under linearly combining input vectors into a different input vector. Therefore, performing Gram-Schmidt does not change the determinant. That is, if $\hU_E = (\hww_1, ..., \hww_n)$ is the orthonormal basis obtained by performing Gram-Schmidt on $E$, then 
   
   \begin{align*}
       \det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU}) = \det([\hww_1]_{\hU}, ..., [\hww_n]_{\hU})
       =
       \det([\hU_E]_{\hU}).
   \end{align*}
   
   In performing this first step of the proof, the determinant has stayed the same as we've passed from $E$ to $\hU_E$. We now show that the determinant continues to stay the same as we pass from  $\hU_E$ to some permutation $\hU^\sigma$ of $\hU$.
   
   Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis} says that there is a $n$-rotation $\RR$ taking $\hU_E$ to $\hU^\sigma$, for some $\sigma \in S_n$, and Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1} guarantees that $\det(\RR) = 1$. Thus, since $\hU^\sigma = \RR(\hU_E)$, we have
   
   \begin{align*}
        \det([\hU_E]_{\hU}) 
        = \det([\RR(\hU_E)]_{\hU}) \det([\hU_E]_{\hU})
        = \det([(\RR \circ \II)(\hU_E)]_{\hU})
        = \det([\RR(\hU_E)]_{\hU})
        = \det([\hU^\sigma]_{\hU}).
   \end{align*}
   
   To conclude the proof, we will show that $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$; once we have shown this, we are done, since $\sgn(\sigma) \det([\hU]_{\hU}) = \sgn(\sigma) \det(\II) = \sgn(\sigma)$. Since any permutation is a composition of transpositions, then $\hU^\sigma$ can be obtained from $\hU$ by repeatedly swapping vectors in $\hU$. Whenever vectors are swapped in the determinant, the sign of the determinant is multiplied by $-1$. This accounts for the $\sgn(\sigma)$ factor in the equation $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$.
\end{proof}

\subsection*{Orientation of finite-dimensional vector spaces}

\label{ch::lin_alg::orientation_finite_dim_vector_space}

The fact that the determinant tracks orientation is the main result of our discussion of orientation. Because determinants do not rely on the existence of an inner product, the determinant can be used to generalize the notion of orientation to any finite-dimensional vector space.

\begin{defn}
\label{ch::lin_alg::defn::orientation_finite_dim_vector_space}
    (Orientation of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space (not necessarily an inner product space). An \textit{orientation on $V$} is a choice of ordered basis $E$ for $V$. (Notice here that $E$ is not necessarily orthonormal, because $V$ might not have an inner product!). If we have given $V$ the orientation $E$, then we say that an ordered basis $F$ of $V$ is \textit{positively oriented (relative to $E$)} iff $\det([\FF]_E) > 0$, and that $F$ is \textit{negatively oriented (relative to $E$)} iff $\det([\FF]_E) < 0$.
    
    A finite-dimensional vector space that has an orientation is called an \textit{oriented (finite-dimensional) vector space}.
\end{defn}
 
\begin{remark}
    (Antisymmetry of orthonormal ordered bases).
    
    Notice that we still have the previous antisymmetry of orthonormal ordered bases due to the antisymmetry of the determinant.
\end{remark}

As a last sidenote, the following theorem gives some justification as to why our definition of $n$-rotation was a good definition. (Strictly speaking, though, the justification is somewhat circular, since we used the notion of $n$-rotations to explain how the determinant- which is involved in the justification- tracks orientation).

\begin{theorem}
    If $V$ is an $n$-dimensional inner product space, then 
    
    \begin{align*}
        \{\text{$n$-rotations on $V$}\} = \{\text{orthogonal linear functions $V \rightarrow V$ with determinant 1}\}
    \end{align*}
\end{theorem}

\begin{proof}
    \scriptsize Proof idea is from jagr2808. 
    \fontsize{10pt}{12pt}\selectfont \\
    \indent ($\subseteq$). This is just Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1}.
    
    \indent ($\supseteq$). If $\ff:V \rightarrow V$ is an orthogonal linear function, then $\ff$ sends an arbitrary orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ of $V$ to another orthonormal basis $\ff(\hU) = (\ff(\huu_1), ..., \ff(\huu_n))$.

    Let $\RR_1$ be the $2$-dimensional rotation sending $\ff(\huu_1)$ to $\huu_1$. Then $\RR_2 := \RR_1 \circ \ff:V \rightarrow V$ fixes $\huu_1$, so we can think of it as a function from $\spann(\huu_2, ..., \huu_n)$ to $\spann(\huu_2, ..., \huu_n)$. 
    
    We now use the above idea finitely many times. Define $\RR_{i + 1}$ to be the $2$-dimensional rotation sending ${(\RR_i \circ \RR_{i - 1})(\huu_i)}$ to $\huu_i$. We know that such a $2$-dimensional rotation always exists because $\det(\ff) = 1$ implies that $(\ff(\huu_1), ..., \ff(\huu_n))$ has the same orientation as $(\huu_1, ..., \huu_n)$. (Recall, having the same orientation involves ``rotational equivalence'' via $n$-rotations, which are compositions of $2$-dimensional rotations). By induction, $\RR_n \circ \ff$ is a composition of $2$-dimensional rotations.
    
    Thus, since $\ff = \RR_n^{-1} \circ (\RR_n \circ \ff)$, we see $\ff$ is a composition of the $2$-dimensional rotations $\RR_n^{-1}$ and $\RR_n^{-1} \circ \ff$.
    
    %If T is an orthogonal endomorphism then T maps the standard basis e_1, ..., e_n to an orthonormal basis u_1, ..., u_n.

    %Let R be the 2-rotation sending u_1 to e_1. Then RT is an endomorphism in SO(n) that fixes e_1, so we can think of it as an endomorphism in SO(n-1) on the orthogonal complement. By induction RT is the composition of 2-rotations. And T = R-1RT, so T is as well.
\end{proof}