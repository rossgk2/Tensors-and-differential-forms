\chapter{The determinant and orientation}
\label{ch::antisymmetry}

\section{Permutations}

\begin{defn}
    (Permutation).
    
    A \textit{permutation (on $\{1, ..., n\}$)} is a bijection $\{1, ..., n\} \rightarrow \{1, ... n \}$. The set of permutations on $\{1, ..., n\}$ is denoted $S_n$.
\end{defn}

\begin{defn}
    (Cycle).

    A permutation in $S_n$ is a \textit{cycle} iff it is defined by

    \begin{align*}
        \begin{cases}
            i_j \mapsto i_{j + 1} & j \in \{1, ..., k - 1\} \\
            i_j \mapsto i_1 & j = k
        \end{cases}
    \end{align*}

    for some $i_1, ..., i_k \in \{1, ..., n\}$. If such $i_1, ..., i_k$ exist, then the cycle is represented as the following tuple without commas: $\begin{pmatrix} i_1 & \hdots & i_k \end{pmatrix}$.
\end{defn}

\begin{defn}
    (Transposition).

    A permutation in $S_n$ is a \textit{transposition} iff it swaps two elements and leaves all of the others fixed. Notice, every transposition can be written as the cycle $\transp{i}{j}$ for some $i, j \in \{1, ..., n\}$.
\end{defn}

Now, we prove an intuitive theorem.

\begin{theorem}
\label{ch::antisymmetry::thm::permutations_decomposition_into_transpositions}
    (Every permutation is a composition of transpositions).

    Every permutation $\sigma \in S_n$ can be decomposed as $\sigma = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_n$ are transpositions. Note, the set of transpositions in the decomposition does not uniquely correspond to $\sigma$.
\end{theorem}

\begin{proof}  
    Notice that
    
    \begin{itemize}
        \item The function $\text{sort}_\sigma \in S_n$ that sorts $(\sigma(1), ..., \sigma(n))$ from least to greatest satisfies $\text{sort}_\sigma((\sigma(1), ..., \sigma(n))) = (1, ..., n)$.
        \item We also have $\sigma^{-1}((\sigma(1), ..., \sigma(n))) = (\sigma^{-1}(\sigma(1)), ..., \sigma^{-1}(\sigma(n)) = (1, ..., n)$.
    \end{itemize} 
    
    Thus, $\text{sort}_\sigma = \sigma^{-1}$. Since there are many ways to describe $\text{sort}_\sigma$ as a composition of transpositions\footnote{See Definition \ref{ch::appendix::defn::bubble_sort} in the appendix for a description of one such description, the \textit{bubble sort algorithm}.}, we have $\text{sort}_\sigma = \sigma^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1 \circ ... \circ \sigma_k$ are transpositions. The inverse of a transposition is a transposition, so we have $\sigma = \sigma_1^{-1} \circ ... \circ \sigma_k^{-1}$, where $\sigma_1^{-1}, ..., \sigma_k^{-1}$ are transpositions. So, we have shown that $\sigma$ decomposes into a composition of transpositions. This decomposition is not unique, as it depends on which sorting algorithm is used to compute $\text{sort}_\sigma = \sigma^{-1}$.
    
    % (Case: $X \neq S_n$ for all $n$). Take any bijection $f:X \rightarrow \{1, ..., |X|\}$. Since $f \circ \sigma \circ f^{-1} \in S_{|X|}$, then applying the previous case implies $f \circ \sigma \circ f^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_{|X|}$ are transpositions. Solving for $\sigma$, we have $\sigma = f^{-1} \circ (\sigma_k \circ ... \circ ... \sigma_1) \circ f = (f^{-1} \circ \sigma_k \circ f) \circ (f^{-1} \circ \sigma_{k - 1} \circ f) \circ ... \circ (f^{-1} \circ \sigma_1 \circ f)$. Each $f^{-1} \circ \sigma_i \circ f$ is a permutation on $X$, so we have shown that $\sigma$ is a composition of permutations on $X$, as desired.
\end{proof}

The following definition and theorem are the main reason for our excursion into permutations.

\begin{defn}
    (Parity of a permutation).

    A permutation in $S_n$ is said to have \textit{even parity}, or to be \textit{even}, iff it is equal to the composition of an even number of transpositions. A permutation is said to have \textit{odd parity}, or to be \textit{odd}, iff it is equal to the composition of an odd number of transpositions.
\end{defn}

\begin{theorem}
    (The parity of a permutation is unique).

    A permutation in $S_n$ has a unique parity. That is, no permutation is both even and odd.
\end{theorem}

\begin{proof}
    Let $\sigma \in S_n$, and suppose $\sigma = \sigma_k \circ ... \circ \sigma_1$ and $\sigma = \pi_\ell \circ ... \circ \pi_1$, where $\sigma_1, ..., \sigma_k$ and $\pi_1, .., \pi_\ell$ are transpositions. We need to prove that $k$ and $\ell$ have the same parity.
    
    Suppose for contradiction that $k$ and $\ell$ have different parities. We have $\sigma_k \circ ... \circ \sigma_1 = \pi_k \circ ... \circ \pi_1$, and so $\text{id} = \pi_1^{-1} \circ ... \circ \pi_\ell^{-1} \circ \sigma_k \circ ... \circ \sigma_1 = \pi_1 \circ ... \circ \pi_\ell \circ \sigma_k \circ ... \circ \sigma_1$. Since $k$ and $\ell$ have different parities, then  $k + \ell$ is odd, and the identity permutation is a composition of an odd number of transpositions.
    
    Since the identity is a composition of an odd number of transpositions, it cannot be a composition of zero transpositions. It also cannot be a composition of one of one transposition, as no single transposition is the identity! Therefore, the identity is a composition of more than one transposition. Let $r$ be the smallest integer with $r > 1$ such that $\text{id} = \tau_r \circ ... \circ \tau_1$, where $\tau_1, ..., \tau_r$ are transpositions.
    
    Consider the following cases.

    (Case: $\tau_r = \tau_{r - 1}$). Then $\tau_r \circ \tau_{r - 1} = \text{id}$, and $\tau_r \circ ... \circ \tau_1 = \tau_{r - 2} \circ ... \circ \tau_1$ is a composition of $r - 2$ transpositions. We see $r$ is not minimal; this is a contradiction, so this case is impossible.

    (Case: $\tau_r \neq \tau_{r - 1}$). Suppose $\tau_r = \transp{i}{j}$.
    
    \fullindent
    {
        (Case: $\tau_r$ and $\tau_{r - 1}$ are disjoint). If $\tau_r$ and $\tau_{r - 1}$ are disjoint, then $\tau_r \circ \tau_{r - 1} = \tau_{r - 1} \circ \tau_r = \tau_{r - 1} \circ \transp{i}{j}$.
    
        (Case: $\tau_r$ and $\tau_{r - 1}$ share one element).
    
        \fullindent
        {
            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r}$ for some $r \neq j$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r} = \begin{pmatrix} i & r & j \end{pmatrix} = \begin{pmatrix} j & i & r \end{pmatrix} = \transp{j}{r} \circ \transp{i}{j}$. 
            }

            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s}$ for some $s \neq i$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s} = \begin{pmatrix} i & j & s \end{pmatrix} = \transp{s}{j} \circ \transp{i}{s}$.
            }
        }
    }

    \newcommand{\tttau}{\widetilde{\tau}}
            
    To summarize the above, in all of the possible cases, we have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \tau_{r - 1} = \tttau_r \circ \transp{i}{a_1}$ for some $a_1 \in \{1, ..., n\} - \{i\}$ and some transposition $\tttau_r$ that fixes $a_1$. Thus, $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ \transp{i}{a_1} \circ \tau_{r - 2} \circ ... \circ \tau_1$, where $\tttau_r$ fixes $i$. Repeating this process, we see $\id = \tau_r \circ \tau_{r - 1} \circ \transp{i}{a_2} \circ ... \circ \tau_1$. It follows from induction that there exist $a \in \{1, ..., n\} - \{i\}$ and transpositions $\tttau_2, ..., \tttau_r$ such that $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ ... \circ \tttau_2 \circ \transp{i}{a}$, where all of $\tttau_2, ..., \tttau_r$ fix $i$. Thus $\id(i) = a \neq i$. This is a contradiction, since the identity fixes every element of $\{1, ..., n\}$.
\end{proof}

\begin{defn}
    The \textit{sign} $\sgn(\sigma)$ of a permutation $\sigma \in S_n$ is defined to be
    
    \begin{align*}
        \sgn(\sigma) := 
        \begin{cases}
            1 & \text{$\sigma$ is even} \\
            -1 & \text{$\sigma$ is odd}
        \end{cases}.
    \end{align*}

    Notice, $\sgn$ is a well-defined function because the parity of a permutation is unique.

    It's useful to note that $\sgn(\sigma) = (-1)^k$, where $k$ is the number of transpositions in one of the decompositions for $\sigma$.
\end{defn}

\begin{theorem}
    (Sign of a composition of permutations).

    We have $\sgn(\pi \circ \sigma) = \sgn(\pi) \sgn(\sigma)$ for all permutations $\sigma, \pi \in S_n$.
\end{theorem}

\begin{proof}
    $\sigma$ is a composition of $k$ transpositions for some $k$, and $\pi$ is a composition of $\ell$ transpositions for some $\ell$. Notice that the theorem is equivalent to the statement ``the parity of $k + \ell$ is even iff $k$ and $\ell$ have the same parity and odd iff $k$ and $\ell$ have different parity''. This statement is an elementary fact about integers, so the theorem is true.
\end{proof}

\newpage

\section{The determinant}

\label{ch::antisymmetry::determinant}

\begin{defn}
\label{ch::antisymmetry::defn::determinant}
    (The determinant).
    
    Let $K$ be a field, and let $\sE = \{\see_1, ..., \see_n\}$ be the standard basis of $K^n$. We want to define a function $(K^n)^{\times n} \rightarrow K$ which, given $\vv_1, ..., \vv_n \in K^n$, returns the $n$-dimensional volume of the parallelapiped spanned by $\vv_1, ..., \vv_n$. We will denote this function by $\det:(K^n)^{\times n} \rightarrow K$. We require that $\det$ satisfy the following axioms:
    
    \begin{enumerate}
        \item $\det(\see_1, ..., \see_n) = 1$, since we want the unit $n$-cube to have an $n$-dimensional volume of $1$.
        \item $\det$ is multilinear, because...
        \begin{itemize}
            \item The volume of a parallelapiped that is the disjoint union of two smaller parallelapipeds should be the sum of the volumes of the smaller parallelapipeds.
            \item Scaling one of the sides of a parallelapiped by $c \in K$ should increase that paralellapiped's volume by a factor of $c$. 
        \end{itemize}
        \item $\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$. This should hold because when two sides of a parallelapiped coincide, its \textit{$n$-dimensional} volume is zero.
    \end{enumerate}
\end{defn}

\begin{deriv}
    (Alternatingness).

    Let $\vv, \ww \in K^n$. By the third axiom of the determinant, we have $\det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n) + \det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) = \det(\vv_1, ..., \vv + \ww, ..., \ww + \vv, ..., \vv_n) = 0$. Thus,

    \begin{align*}
        \det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n) = -\det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) \text{ for all $\vv, \ww \in K^n$}.
    \end{align*}

    This condition is known as the \textit{alternatingness} property of the determinant. It is quick to show that alternatingness property not only follows from the third axiom of the determinant, but is equivalent to it.

    Notice that the alternatingness property of the determinant implies that there exist $\vv_1, ..., \vv_n \in K^n$ with $\det(\vv_1, ..., \vv_n) < 0$. So, we see our intuitive assumptions about volume require that volume be \textit{signed}. In Theorem [...] we will see how signed volume corresponds to the concept of orientation.
\end{deriv}

We quickly define alternating functions to be functions that have this alternatingness property, as this definition allows for a succinct characterization of the determinant.

\begin{defn}
    (Alternating function).

    If $V$ and $W$ are vector spaces, then a function $\ff:V^{\times k} \rightarrow W$ is \textit{alternating} iff \\ ${\ff(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_k) = -\ff(\vv_1, ..., \vv_j, ..., \vv_i, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$. Equivalently, $\ff:V^{\times k} \rightarrow W$ is alternating iff
    ${\ff(\vv_{\sigma(1)}, ..., \vv_{\sigma(k)}) = \sgn(\sigma) \ff(\vv_1, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$ and $\sigma \in S_k$.
\end{defn}

\begin{theorem}
    (Succinct characterization of the determinant).

    The determinant is the unique\footnote{We have not yet explained why a function satisfying the determinant axioms is unique. We do so at the beginning of Derivation \ref{ch::antisymmetry::deriv::permutation_formula_for_determinant}.} alternating linear $k$-form sending $\sE \mapsto 1$. (Recall Definition \ref{ch::bilinear_forms_metric_tensors::defn::linear_k_form}).
\end{theorem}

We now investigate more properties of the determinant that follow from this succint characterization.

\begin{theorem}
\label{ch::antisymmetry::thm::consequent_det_props}
    (Consequent properties of the determinant). 
    
    \begin{enumerate}
    \setcounter{enumi}{3}
        \item $\det$ is invariant under linearly combining input vectors into a different input vector. That is, $\det(\vv_1, ..., \vv_i, ..., \vv_n) = \det(\vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^n d_j \vv_j, ..., \vv_n)$ for all $i \in \{1, ..., n\}$.
        \item $\det(\vv_1, ..., \vv_n) = 0$ iff $\{\vv_1, ..., \vv_n\}$ is a linearly dependent set.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
    \setcounter{enumi}{4}
        \item 
        
        Using the axiom ``$\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$'' together with the multilinearity of the determinant, we have
        
        \begin{align*}
            \det(\vv_1, ..., \vv_i, ..., \vv_n)
            &= \det(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_n) + \sum_{j = 1, j \neq i}^n d_j \det(\vv_1, ..., \vv_{i - 1}, \vv_j, \vv_{i + 1}, ..., \vv_j, ..., \vv_n) \\
            &= \det\Big( \vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^n d_j \vv_j, ..., \vv_n \Big).
        \end{align*}
        
        \item 
        \mbox{}
        \\ \indent ($\det(\vv_1, ..., \vv_n) = 0 \implies \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). If the input vectors are linearly dependent, we can use the invariance of $\det$ under linearly combining some columns into others (which we just proved) to produce an equal determinant in which two columns are the same. By the third axiom, this determinant is zero.
        \\ \indent ($\det(\vv_1, ..., \vv_n) = 0 \impliedby \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). Suppose for contradiction that the determinant of a set of $n$ linearly independent vectors is zero. These $n$ linearly independent vectors form a basis for $K^n$, so we have shown that the determinant of a basis set is zero. But then, using multilinearity together with the invariance of $\det$ under linearly combining some vectors into a different vector, we can show that $\det(\vv_1, ..., \vv_n) = 0$ for \textit{all} $\vv_1, ..., \vv_n \in K^n$. This contradicts the first axiom that specifies $\det(\see_1, ..., \see_n) = 1$.
    \end{enumerate}
\end{proof}

Using the properties we have amassed, we can derive a formula for the determinant.

\begin{defn}
    (Determinant of a square matrix). 
    
    We define the \textit{determinant of a square matrix} to be the result of applying $\det$ to the column vectors of that matrix.
\end{defn}

\begin{deriv}
\label{ch::antisymmetry::deriv::permutation_formula_for_determinant}
    (Permutation formula for the determinant).
    
    We now derive the \textit{permutation formula} for the determinant. The formula we obtain shows that the function $\det$ specified in Definition \ref{ch::antisymmetry::defn::determinant} exists; since the right side of the formula is a well-defined function, the determinant is the unique function satisfying the axioms.
    
    Consider vectors $\vv_1, ..., \vv_n \in K^n$, and\footnote{There is no hidden meaning behind the upper and lower indices on $a^i_j$ here; we only want to consider an arbitrary $n \times n$ matrix of scalars in $K$, and prefer to think of this matrix as storing the coordinates of a $(1, 1)$ tensor rather those of a $(2, 0)$ or $(0, 2)$ tensor.} set $(a^i_j) := ([\vv_i]_\sE)^j$. Then...
    
    \begin{align*}
        \det((a^i_j)) = \det(\vv_1, ..., \vv_n)
        &= \det \Big(\sum_{i_1 = 1}^n a^1_{i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a^n_{i_n} \see_{i_n} \Big) \\
        &= \sum_{i_1 = 1}^n \det \Big( a^1_{i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a^n_{i_n} \see_{i_n} \Big) \\
        &\vdots \\
        &= \sum_{i_1 = 1}^n ... \sum_{i_n = 1}^n \det(a^1_{i_1} \see_{i_1}, ..., a^n_{i_n} \see_{i_n}) \\
        &= \sum_{i_1 = 1}^n ... \sum_{i_n = 1}^n \det(a^1_{i_1} \see_{i_1}, ..., a^n_{i_n} \see_{i_n}), \text{ where $i_1, ..., i_n$ are distinct from each other} \\
        &= \sum_{\sigma \in S_n}
        \det(a^1_{\sigma(1)} \see_{\sigma(1)}, ..., a^n_{\sigma(n)} \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n}
        a^1_{\sigma(1)} ... a^n_{\sigma(n)} \det(\see_{\sigma(1)}, ..., \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \text{sgn}(\sigma)
        \det(\see_1, ..., \see_n) \\
        &= \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \text{sgn}(\sigma)
    \end{align*}
    
    Therefore, we have
    
    \begin{align*}
        \boxed
        {
            \det(\vv_1, ..., \vv_n) = \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma)
        }
    \end{align*}
    
    In this derivation, we have mostly used the multilinearity of the determinant. Though, the expression labeled with ``where $i_1, ..., i_n$ are distinct from each other'' results from the previous line due to the third axiom of the determinant, $\det(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_n) = 0$ when $\vv_i = \vv_j$.
    
    There are four major steps in the derivation. The first step is to use the multilinearity of the determinant to turn the determinant of $(a^i_j)$ into a sum of the determinants of matrices that only have one nonzero entry in each column (see the line directly above the line labeled with ``where $i_1, ..., i_n$ are distinct from each other''). The second step is to disregard all determinants in this previous sum whose matrix arguments have two or more columns that have their nonzero entries in the same row (i.e. whose matrix arguments are matrices of linearly dependent columns). This leaves us with a sum of determinants of diagonal matrices whose columns have been shuffled (this corresponds to the line labeled with ``where $i_1, ..., i_n$ are distinct from each other'' and the line directly below it). The third step, which corresponds to the third to last line, is to use multilinearity to pull out all the constants. The fourth step is to use the alternatingness of the determinant so that every determinant argument in the sum is the identity matrix; this results in multiplying each term in the sum by $\sgn(\sigma)$.
\end{deriv}

\begin{deriv}
\label{ch::antisymmetry::thm::det_transpose_invariant}    
    (Determinant of a matrix is transpose-invariant).
    
    Let $\AA$ be a square matrix with entries in $K$. Recall from the discussion after the statement of the permutation formula for the determinant that the determinant of a matrix is a sum of determinants of diagonal matrices whose columns have each been shuffled by a different permutation $\sigma$:
    
    \begin{align*}
        \det(\AA) = 
        \sum_{\sigma \in S_n}
        \det(a^1_{\sigma(1)} \see_{\sigma(1)}, ..., a^n_{\sigma(n)} \see_{\sigma(n)}).
    \end{align*}
    
    Now, notice that a shuffled diagonal matrix that has been shuffled in accordance to a permutation $\sigma$ can be converted to a diagonal matrix via application of $\sigma^{-1}$, transposed (the transpose of a diagonal matrix $\DD$ is $\DD$), and then re-shuffled via application of $\sigma$. Doing this does not change the determinant, since the $\sgn(\sigma^{-1})$ introduced by the shuffle is canceled by the $\sgn(\sigma)$ factor introduced by the re-shuffle.

    When this is done to every matrix in the sum, then the sum is what we would obtain if we were to compute $\det(\AA^\top)$ by repeating the derivation of the permutation formula for the determinant. Therefore
    
    \begin{align*}
        \det(\AA) = \det(\AA^\top).
    \end{align*}
\end{deriv}

\begin{theorem}
    (Laplace expansion for the determinant).
    
    Consider an $n \times n$ matrix $\AA = (a^i_j)$, and let $\AA^i_j$ denote the so-called \textit{$ij$ minor matrix} obtained by erasing the $i$th row and $j$th column of $\AA$. We have
    
    \begin{align*}
        \det(\AA) = \sum_{i = 1}^{n} a^i_j \det(\AA^i_j) \text{ for all $i \in \{1, ..., n\}$} \\
        \det(\AA) = \sum_{j = 1}^{n} a^i_j \det(\AA^i_j) \text{ for all $j \in \{1, ..., n\}$}
    \end{align*}
    
    The first equation is called the \textit{Laplace expansion for the determinant along the $i$th row}, and the second equation is called the \textit{Laplace expansion for the determinant along the $j$th column}. Note that each equation implies the other because $\det(\AA) = \det(\AA^\top)$.
\end{theorem}
    
\begin{proof}
    We prove the second equation of the theorem.
    
    Consider all terms in the permutation formula's sum for $\det(\AA)$ that have the factor $a^i_j$. Let $\BB$ denote the shuffled diagonal matrix that corresponds to one of these terms. We can view $\det(\BB)$ as $\det(\BB) = \pm a^i_j \det(\BB^i_j)$, where $\BB^i_j$ is the determinant of the matrix obtained by removing the $i$th column and $j$th row from $\BB$. The $\pm$ sign is a result of the fact that the matrices $\BB$ and $\BB^i_j$ may have different inversion counts.

    The main effort of this proof is to determine the $\pm$ sign and specify how the inversion counts of $\BB$ and $\BB^i_j$ differ.
    
    As a first step, note that the difference in the inversion count between $\BB$ and $\BB^i_j$ is the number of inversions that involve $a^i_j$. Thus, our problem reduces to determining an expression for the number of inversions that involve $a^i_j$. So, divide the matrix $\BB$ into quadrants that are centered on $a^i_j$. Let $k_1, k_2, k_3, k_4$ be the number of inversions in the upper left, upper right, lower left, and bottom right corners of $\AA$, respectively. The number of inversions involving $a^i_j$ is $k_2 + k_3$. Since we know $k_1 + k_2 + 1 = i$ and $k_1 + k_3 + 1 = j$, we have $k_2 + k_3 = i + j - 2 - 2k_1 = i + j - 2(k_1 + 1)$. (We also know $k_1 + k_2 + k_3 + k_4 = n$, but this is not that helpful). Thus, if $\sigma$ is the permutation corresponding to $\BB$ and $\pi$ is the permutation corresponding to $\BB^i_j$, then $\sgn(\sigma) = \sgn(\pi)(-1)^{i + j - 2(k_1 + 1)} = \sgn(\pi)(-1)^{i + j}$. Thus $\sgn(\sigma) = (-1)^{i + j}\sgn(\pi) \iff \sgn(\pi) = (-1)^{i + j}\sgn(\sigma)$. 
    
    So,
    
    \begin{align*}
        a^i_j \det(\BB^i_j) &= a^i_j \sum_{\pi \in S_n} a^{\pi(1)}_1 ... \cancel{a^{\pi(i)}_j} ..., a^{\pi(n)}_n \sgn(\pi) \\
        &= a^i_j \sum_{\sigma \in S_n} a^{\pi(1)}_1 ... \cancel{a^{\pi(i)}_j} ..., a^{\pi(n)}_n (-1)^{i + j} \sgn(\sigma) \\
        &= (-1)^{i + j} a^i_j \det(\BB)
    \end{align*}
    
    Thus $a^i_j \det(\BB^i_j) = (-1)^{i + j} a^i_j \det(\BB) \iff \det(\BB) = (-1)^{i + j} a^i_j \det(\BB^i_j)$. Now sum all of the $\BB$'s (the diagonal shuffled matrices) to get $\det(\AA) = \sum_{j = 1}^{n} a^i_j \det(\AA^i_j)$.
\end{proof}
    
%\begin{theorem}
%    (Determinant of an upper triangular matrix).
%\end{theorem}

%\begin{theorem}
%    (Adjoint and Cramer's rule).
%\end{theorem}

\begin{defn}
    (Determinant of a linear function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces \textit{of the same dimension}, and let $E$ and $F$ be be bases for $V$ and $W$. We define the \textit{determinant of a linear function $\ff:V \rightarrow W$} to be the determinant of the matrix of $\ff$ relative to $E$ and $F$, $\det(\ff) := \det([\ff(E)]_F)$.
\end{defn}

\begin{remark}
    We have not yet shown that the determinant of a linear function $V \rightarrow V$ is well-defined; we have not shown that it doesn't on the basis chosen for $V$. We will see that this is the case soon.
\end{remark}

\begin{theorem}
\label{ch::antisymmetry::rmk::det_dual_invariant}
    (Determinant of a matrix is dual-invariant).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Consider also the dual $\ff:W^* \rightarrow V^*$ (recall Definition \ref{ch::appendix::defn::dual_transf_after_id}). Then $\det(\ff^*) = \det(\ff)$.
\end{theorem}

\begin{proof}
    Recall from\footnote{Technically, the equivalent conditions of the definition we reference only apply to linear functions $V \rightarrow V$. This is not an issue because $V$ and $W$ have the same dimension; if we want to be very formal, we can use the linear function $\widetilde{\ff}:V \rightarrow V$ that is obtained from $\ff$ by identifying $W \cong V$ with the identification that sends basis vectors of $W$ to basis vectors of $V$.} condition (3) of Definition \ref{ch::bilinear_forms_metric_tensors::defn::symmetric_linear_fn} that if $\AA$ is the matrix of $\ff$ relative to orthonormal bases $\hU_1$ and $\hU_2$, then the matrix of $\ff^*$ relative to the induced dual bases $\hU_2^*$ and $\hU_1^*$ is $\AA^\top$. Since the determinant of a matrix is transpose-invariant (recall Theorem \ref{ch::antisymmetry::thm::det_transpose_invariant},) we have $\det(\ff) = \det(\AA) = \det(\AA^\top) = \det(\ff^*)$.
\end{proof}

\begin{lemma}
\label{ch::antisymmetry::lemma:det_as_scale_factor}
    (Determinant as scale factor).

    Let $V$ and $W$ be vector spaces over the same field. Let $h$ be an alternating linear $k$-form. If $\ff:V \rightarrow W$ is a linear function, then $h(\vv_1, ..., \vv_k) = \det(\ff) h(\ff(\vv_1), ..., \ff(\vv_k))$.
\end{lemma}

\begin{proof}
     Because $h$ is multilinear and alternating, it is analogous to the determinant $\det(\ff(\ee_1), ..., \ff(\ee_n)) = \det([\ff(E)]_F) = \det(\ff)$. So, set $(a^i_j) = [\ff(E)]_F$, and then use essentially the same argument as was made to derive the permutation formula on the left side of the above. We obtain
    
    \begin{align*}
        h(\ff(\ee_1), ...,
        \ff(\ee_k)) 
        &= \sum_{\sigma \in S_k} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) h(\ee_1, ..., \ee_k)
        = \Big( \sum_{\sigma \in S_k} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) \Big) h(\ee_1, ..., \ee_k) \\
        &= \det([\ff(E)]_F) h(\ee_1, ..., \ee_k)
        = \det(\ff) h(\ee_1, ..., \ee_k).
    \end{align*}
    
   So, we have the statement on the basis $E$
   
   \begin{align*}
        h(\ff(\ee_1), ..., 
        \ff(\ee_k)) = \det(\ff) h(\ee_1, ..., \ee_k).
   \end{align*}
   
   Since $h$ is multilinear, we can extend this fact to apply to any list of vectors in $V$. This gives the lemma.
\end{proof}

\begin{theorem}
    (Product rule for determinants). 
    
    Let $V, W$ and $Z$ be finite-dimensional vector spaces of the same dimension, and consider linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Z$. Then $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$. Thus, if $\AA$ is an $m \times n$ matrix and $\BB$ is an $n \times p$ matrix, then $\det(\BB \AA) = \det(\BB) \det(\AA)$.
\end{theorem}

\begin{proof}
   Set $n := \dim(V) = \dim(W) = \dim(Z)$, and let $h$ be an alternating linear $k$-form. By the lemma, $\det(\gg \circ \ff)$ satisfies
   
   \begin{align*}
       h((\gg \circ \ff)(\vv_1), ..., (\gg \circ \ff)(\vv_n)) = \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) \text{ for all $\vv_1, ..., \vv_n \in V$}.
   \end{align*}
   
   Notice that the left side is
   
   \begin{align*}
       h\Big(\gg(\ff(\vv_1)), ..., \gg(\ff(\vv_n))\Big) = \det(\gg) h(\ff(\vv_1), ..., \ff(\vv_n)) =
       \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   Thus
   
   \begin{align*}
       \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) = \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   This is a statement on the tuple $(\vv_1, ..., \vv_n) \in V^{\times n}$. Extending this statement to a statement on any tuple $\TT \in V^{\times n}$, we have $\det(\gg \circ \ff) h(\TT) = \det(\gg) \det(\ff) h(\TT)$. Thus ${\det(\gg \circ \ff) - \det(\gg) \det(\ff)) h(\TT) = \mathbf{0}}$ for all $\TT \in V^{\times n}$. Since we can choose $h$ such that $h(\TT) \neq 0$ for some $\TT \in V^{\times n}$, this forces $\det(\gg \circ \ff) - \det(\gg) \det(\ff) = 0$, giving us $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$, as desired.
\end{proof}

\begin{theorem}
    (Determinant of an inverse function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Then $\det(\ff^{-1}) = \frac{1}{\det(\ff)}$.
\end{theorem}

\begin{proof}
   We have $\det(\ff \circ \ff^{-1}) = \det(\II) = 1$, and $\det(\ff \circ \ff^{-1}) = \det(\ff) \det(\ff^{-1})$ by the previous theorem, so $\det(\ff) \det(\ff^{-1}) = 1$.
\end{proof}

\newpage

\section{Orientation of finite-dimensional vector spaces}
\label{ch::antisymmetry::section::orientation}
%\begin{itemize}
%    \item \url{https://arxiv.org/pdf/1103.5263.pdf}
%    \item \url{https://en.wikipedia.org/wiki/Cartan\%E2\%80\%93Dieudonn\%C3\%A9_theorem}
%    \item \url{https://en.wikipedia.org/wiki/Rotor_(mathematics)#:~:text=A\%20rotor\%20is\%20an\%20object,the\%20Cartan\%E2\%80\%93Dieudonn\%C3\%A9\%20theorem).}
%    \item \url{https://en.wikipedia.org/wiki/Geometric_algebra#Rotating_systems}
%    \item \url{https://www.euclideanspace.com/maths/algebra/clifford/d4/transforms/index.htm}
%\end{itemize}

\textit{Orientation} is the mathematical formalization of the notions of ``clockwise'' and ``counterclockwise''; it is the notion which distinguishes different ``rotational configurations'' from each other.

Our discussion of orientation will be as follows. First, we define an \textit{orientation on a vector space} to be a choice of an orthonormal ordered basis. (We heavily rely on inner product spaces for their inner-product-induced orthonormality). This definition of orientation will only allow us to check the orientation of permutations of the chosen orthonormal basis, however. In order to give orientation to arbitrary ordered bases, we introduce rotations in $n$-dimensions, so that an arbitrary ordered basis can be given the orientation of a ``close-by'' permuted ordered basis.

After we finish the definition of orientation for inner product spaces, we end the subsection on oriented inner product spaces by presenting the fact that the determinant ``tracks'' orientation. This fact allows us to generalize the notion of orientation to finite dimensional vector spaces that may or may not have an inner product. Lastly, we show how the top exterior power of a finite-dimensional vector space can be used for the purposes of orientation.

[TO-DO: fix the terminology of the above]

\subsection*{First notions of orientation}

\subsubsection*{First notions of orientation in two dimensions}

\begin{defn}
    (Ordered basis).
    
    An \textit{ordered basis} for a vector space is a simply a tuple containing some ordering of the basis vectors for that space.
    
    For example, if $V$ is a 2-dimensional vector space and has basis $E = \{\ee_1, \ee_2\}$, then $(\ee_1, \ee_2)$ and $(\ee_2, \ee_1)$ are both ordered bases of $V$. We have $(\ee_1, \ee_2) \neq (\ee_2, \ee_1)$.
\end{defn}

We now discover a consequence of imposing that the bases under consideration be ordered.

\begin{deriv}
\label{ch::antisymmetry::deriv::ordered_bases_antisymmetry_intuition}
    (Intuition for the antisymmetry of ordered bases).
    
    Consider the plane $\R^2$, and consider also two permutations of the standard ordered basis $\sE = (\see_1, \see_2)$ for $\R^2$: $(\see_1, \see_2)$ and $(\see_1, -\see_2)$. (Draw these ordered bases out on paper). Notice that no matter how you rotate the entire second ordered basis (rotate each vector in the second ordered basis by the same amount), it is impossible to make all vectors from the second ordered basis simultaneously align with their counterparts from the first ordered basis. This is also impossible for the ordered bases $(\see_1, \see_2)$ and $(\see_2, \see_1)$. Finally, consider the ordered bases $(\see_1, \see_2)$ and $(-\see_2, \see_1)$ of $\R^2$. It \textit{is} possible to make each vector from the first ordered basis with its counterpart from the second ordered basis by rotating either the entire first ordered basis or the entire second ordered basis.
    
    What we have discovered is that \textit{swapping adjacent vectors in an ordered basis of two vectors produces an ordered basis that is} equivalent under rotation \textit{to the ordered basis obtained from the original by negating one of the vectors that have been swapped}. We refer to this fact as the \textit{antisymmetry of orthonormal ordered bases}.
\end{deriv}

We would now like to work towards a more precise statement of the antisymmetry of orthonormal ordered bases. The notion of \textit{rotational equivalence} is what will facilitate this formalization. Before we define rotational equivalence, however, we must formalize what a ``rotation'' is. The following definition accomplishes this.

\begin{defn}
\label{ch::antisymmetry::defn::2-rotation}
    ($2$-dimensional rotation).
    
    Let $V$ be a $2$-dimensional inner product space, and let $\hU$ be an orthonormal ordered basis for $V$.
    A \textit{$2$-dimensional rotation on $V$} is a linear function $\RR_\theta:V \rightarrow V$ whose matrix relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix},
        \text{ where $\theta \in [0, 2\pi)$}.
    \end{align*}
\end{defn}

\begin{defn}
    (Equivalence under rotation for $2$-dimensional inner product spaces).

    We define orthonormal ordered bases $\hU = (\huu_1, \huu_2)$ and $\hW = (\hww_1, \hww_2)$ of a 2-dimensional inner product space\footnote{Note, this is the first time we have required that the vector space under consideration be an inner product space. We need this constraint so that we can speak of orthonormal bases.} to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).

    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

\begin{theorem}
    \label{ch::antisymmetry::thm::antisymmetry_ordered_bases_2_dimensions}

    (Antisymmetry of orthonormal ordered bases for a $2$-dimensional inner product space).
    
    Now we see how the notion of rotational equivalence for $2$-dimensional inner product spaces formalizes the antisymmetry of orthonormal ordered bases. Let $\hU = (\huu_1, \huu_2)$ be an orthonormal ordered basis of a $2$-dimensional inner product space. When $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2} \}$, the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \pm
        \begin{pmatrix}
            0 & -1 \\
            1 & 0
        \end{pmatrix}.
    \end{align*}
    
    Computing $\RR_\theta(\hU) = \RR_\theta((\huu_1, \huu_2)) = (\RR_\theta(\huu_1), \RR_\theta(\huu_2))$ for $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$, we see that the following ordered bases are rotationally equivalent:
    
    \begin{align*}
        (\huu_2, \huu_1 ) &\sim (-\huu_1, \huu_2) \sim (\huu_1, - \huu_2) \\
        (\huu_1, \huu_2) &\sim (-\huu_2, \huu_1) \sim (\huu_2, -\huu_1)
    \end{align*}

    This is what we noticed in the informal discussion of Derivation \ref{ch::antisymmetry::deriv::ordered_bases_antisymmetry_intuition}.
\end{theorem}

\subsubsection*{First notions of orientation in $n$ dimensions}

To study orientation in $n$ dimensions, we generalize the key notion of rotational equivalence by ``extending'' $2$-dimensional rotations so that they can be applied to elements of $n$-dimensional inner product spaces. 

\begin{defn}
    (Extension of a function).

    Let $X$ and $Y$ be sets, let $X_1 \subset X$ be subset of $X$, and consider a function $f:X_1 \rightarrow Y$. The \textit{extension of $f$ to $X$} is the function $f_{\text{ext}}$ defined by

    \begin{align*}
        f_{\text{ext}}(x) =
        \begin{cases}
            f(x) & x \in X_1 \\
            x & x \notin X_1 
        \end{cases}.
    \end{align*}
    
    We will speak frequently of extensions of $2$-dimensional rotations. In fact, we will engage in a bit of abuse of notation: when $V$ is an inner product space, we will say ``the $2$-dimensional rotation $\RR_\theta$ on $V$'' to mean ``the extension of the $2$-dimensional rotation $\RR_\theta$ to $V$'', and use $\RR_\theta$ to denote $(\RR_\theta)_{\text{ext}}$.
    
    It's helpful to spell out a further detail. If $\RR_\theta$ is a $2$-dimensional rotation on a finite-dimensional inner product space and $\hU$ is an orthonormal basis of the space, then the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is    
    
    \begin{align*}
        [\RR_\theta(\hU)]_{\hU} =
        \kbordermatrix
        {
             & & & \text{$i$th column} &  & \text{$j$th column}   \\
             & 1 & \hdots & \cos(\theta) & 0 & -\sin(\theta) & 0 \\
             & 0 & & 0 & \vdots & 0 & \vdots \\
             & 0 & & \vdots & 1 & \vdots & \vdots \\
             & \vdots & & 0 & \vdots & 0 & \vdots \\
             & 0 & \hdots & \sin(\theta)  & 0 & \cos(\theta) & 1
        }.
    \end{align*}
    
    (The columns other than the $i$th and $j$th columns are the columns of the $n \times n$ identity matrix).
\end{defn}

Now that we are equipped with $2$-rotations that can be applied on finite-dimensional inner product spaces, we can generalize our definition of rotational equivalence to $n$ dimensions.

\begin{defn}
    (Equivalence under rotation).
    
    We define orthonormal ordered bases $\hU$ and $\hW$ of a finite-dimensional inner product space to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).
    
    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

\begin{defn}
    (Permutation acting on a tuple). 
    
    Let $X = (x_1, ..., x_n)$ be an $n$-tuple. Given a permutation $\sigma \in S_n$, we define $X^\sigma := (x_{\sigma(1)}, ..., x_{\sigma(n)})$. For example, if $E = (\ee_1, ..., \ee_n)$ is a basis of a finite-dimensional vector space, then $E^\sigma = (\ee_{\sigma(1)}, ..., \ee_{\sigma(n)})$.
\end{defn}

\begin{theorem}
    \label{ch::antisymmetry::thm::permutations_preserve_rotational_equivalence}

    (Permutations preserve rotational equivalence).
    
    If $\hU$ and $\hW$ are orthonormal ordered bases of an $n$-dimensional inner product space, then ${\hU \sim \hW \iff \hU^\sigma \sim \hW^\sigma}$ for all permutations $\sigma \in S_n$.
\end{theorem}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$ and $\hW = (\hww_1, ..., \hww_n)$. We have that $\hU \sim \hW$ iff there exists $\theta \in [0, 2\pi)$ such that $\hW = \RR_\theta(\hU)$, which is true iff $\hww_i = \RR_\theta(\huu_i)$ for all $i$. This is true iff for all $\sigma \in S_n$ we have $\hww_{\sigma(i)} = \RR_\theta(\huu_{\sigma(i)})$, which is equivalent to $\hU^\sigma \sim \hW^\sigma$.
\end{proof}

\begin{theorem}
    \label{ch::antisymmetry::thm::antisymmetry_ordered_bases_general}

    (Antisymmetry of orthonormal ordered bases for an $n$-dimensional inner product space).

    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. Similarly to what was done in Theorem \ref{ch::antisymmetry::thm::antisymmetry_ordered_bases_2_dimensions}, we use $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$ in the matrix relative to $\hU$ and $\hU$ of a $2$-dimensional rotation to obtain the formal characterization of the antisymmetry of orthonormal ordered bases.
    
    For any orthonormal ordered basis $\hU = (\huu_1, ..., \huu_n)$ of an $n$-dimensional inner product space, we have
    
    \begin{align*}
        (\huu_1, ..., \huu_i, ..., \huu_j, ..., \huu_n)
        \sim
        (\huu_1, ..., -\huu_j, ..., \huu_i, ..., \huu_n)
        \sim
        (\huu_1, ..., \huu_j, ..., -\huu_i, ..., \huu_n) \text{ for all $i, j \in \{1, ..., n\}$}.
    \end{align*}
\end{theorem}

This fundamental result leads to the following [...].

\begin{theorem}
    \label{ch::antisymmetry::thm::antisymmetry_ordered_bases_signs}

    (Orthonormal ordered bases of signed vectors).
    
    Let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis of an $n$-dimensional inner product space. If $s_1, ..., s_n \in \{-1, 1\}$, then

    \begin{align*}
        (s_1 \huu_1, ..., s_n \huu_n) \sim
        \begin{cases}
            (\huu_1, ..., \huu_n), & \text{the number of $s_i$ equal to $-1$ is $-1$ is even} \\
            (\huu_1, ..., \huu_{j - 1}, -\huu_j, \huu_{j + 1}, ..., \huu_n) \text{ for any $j \in \{1, ..., n\}$}, & \text{the number of $s_i$ equal to $-1$ is $-1$ is odd} 
        \end{cases}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\
    
    (Case: the number of $s_i$ equal to $-1$ is even). Since the number of $s_i$ equal to $-1$ is even, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. If we swap the vectors in each pair twice, then, as illustrated by the example $(-\huu_i, -\huu_j) \sim (\huu_j, -\huu_i) \sim (\huu_i, \huu_j)$, the coefficients on these vectors in the rotationally equivalent orthonormal ordered basis will be $1$, with the order of the vectors being the same as in the original pair. Thus $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_n)$.

    (Case: the number of $s_i$ equal to $-1$ is odd). Since the number of $s_i$ equal to $-1$ is odd, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all but one $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. Let $s_k \huu_k$ be the vector with $s_k = -1$ that is not a member of a pair. Applying the same reasoning as in the previous case, we see that $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ... \huu_n)$. Making use of the example $(-\huu_k, \huu_\ell) \sim (\huu_\ell, \huu_k) \sim (\huu_k, -\huu_\ell)$, we see that this orthonormal ordered basis is rotationally equivalent to $(\huu_1, \huu_2, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Thus, when the number of $s_i$ equal to $-1$ is odd, the orthonormal ordered basis $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Replace $\ell$ with $j$ to obtain the result.
\end{proof}

\begin{lemma}
    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. We have $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$. The sign of a permutation is $1$ iff the permutation consists of an even number of swaps. So, an equivalent statement to the theorem is: for all $n$, if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We prove this statement by induction on $n$.
    
    (Base case). Consider $\hU^\sigma$, where $\sigma = \tau \circ \pi$, and where $\pi, \tau$ are transpositions. 
    
    The permutation $\pi$ swaps some $i, j$. Using the example $(\huu_j, \huu_i) \sim (-\huu_i, \huu_j)$, we see \\ $\hU^\pi \sim (\huu_1, ..., \huu_{i - 1}, -\huu_i, \huu_{i + 1}, ..., \huu_n)$. Since this orthonormal ordered basis has only one basis vector with a coefficient of $-1$, it follows from the previous theorem that it is rotationally equivalent to every other such orthonormal ordered basis; in particular, it is rotationally equivalent to $\hW :=  (-\huu_1, ..., \huu_n)$.

    So far, we've shown $\hU^\pi \sim \hW$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::antisymmetry::thm::permutations_preserve_rotational_equivalence}), we may apply the permutation $\tau$ to each side of the rotational equivalence $\hU^\pi \sim \hW$ to obtain $(\hU^\pi)^\tau = \hU^{\tau \circ \pi}  = \hU^\sigma \sim \hW^\tau$.

    We now compute the right side of this rotational equivalence, $\hW^\tau$.
    
    \indent \indent (Case: $\tau$ swaps $1$ with some $\ell > 1$). By studying the example $(\huu_\ell, -\huu_1) \sim (\huu_1, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (\huu_1, ..., \huu_n) = \hU$.
    
    \indent \indent (Case: $\tau$ swaps $k \neq 1$ with some $\ell > k$). By studying the example $(\huu_\ell, \huu_k) \sim (-\huu_k, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (-\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ..., \huu_n)$. In this last orthonormal ordered basis, the number of basis vectors with a coefficient of $-1$ is even. It follows from the previous theorem that this last orthonormal ordered basis is rotationally equivalent to $\hU$.

    We've shown that $\hW^\tau = \hU$ for all $\tau \in S_n$. Therefore, we have $\hU^\sigma \sim \hW^\tau \sim \hU$, as desired.
    
    (Inductive case). Assume as the inductive hypothesis if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We need to prove that if $\sigma$ consists of $2(n + 1)$ transpositions, then $\hU^\sigma \sim \hU$.

    So, assume $\sigma$ consists of $2(n + 1) = 2n + 2$ transpositions. Then $\sigma = \tau \circ \pi$, where $\pi$ consists of $2n$ transpositions and $\tau$ consists of $2$ transpositions. By the inductive hypothesis, $\hU^\pi \sim \hU$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::antisymmetry::thm::permutations_preserve_rotational_equivalence}), then $(\hU^\pi)^\tau = \hU^{\tau \circ \pi} = \hU^\sigma \sim \hU^\tau$. Since $\tau$ consists of $2$ transpositions, then from the base case it follows that $\hU^\tau \sim \hU$. Thus $\hU^\sigma \sim \hU^\tau \sim \hU$, as desired.
\end{proof}

\begin{theorem}
    \begin{align*}
        \hU^\sigma \sim \hU^\pi \iff \sgn(\sigma) = \sgn(\pi) \text{ for all $\sigma, \pi \in S_n$}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    ($\impliedby$).
    
    The reverse implication is true when the following two statements are:
    
    \begin{enumerate}
        \item If $\sgn(\pi) = 1$, then $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
        \item If $\sgn(\pi) = -1$, then $\sgn(\sigma) = -1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
    \end{enumerate}

    We restate the previous lemma for convenience:

    \begin{itemize}
        \item $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
    \end{itemize}

    Now we prove (1) and (2).

    \begin{enumerate}
        \item If $\sgn(\sigma) = \sgn(\pi) = 1$, then $\hU^\sigma \sim \hU \sim \hU^\pi$ by the above bullet.
        \item Let $\pi$ be any odd permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi$, i.e. $\hU^{\sigma \circ \pi} \sim \hU^\pi$. Thus $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} = \hU^\pi$. In general, every odd permutation is of the form $\sigma \circ \pi$, where $\sigma$ is some even permutation. Thus, as $\sigma$ varies over the even permutations, the permutation $\tau := \sigma \circ \pi$ varies over the odd permutations. This makes the statement $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} \sim \hU^\pi$ equivalent to the statement $\sgn(\tau) = -1 \implies \hU^\tau \sim \hU^\pi$. Since the former statement is true, so is the later, as desired.
    \end{enumerate}

   ($\implies$).

    The forward implication is true when the following two statements are:

    \begin{enumerate}
        \item[3.] If $\sgn(\pi) = 1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.
        \item[4.] If $\sgn(\pi) = -1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = -1$ for all $\sigma \in S_n$.
    \end{enumerate}

    Before we can prove (3) and (4), we need to prove the following bullet point:

    \begin{itemize}
        \item $\hU \sim \hU^\sigma \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.
        \begin{itemize}
            \item (Proof). We show the contrapositive, $\sgn(\sigma) = -1 \implies \hU \nsim \hU^\sigma$ for all $\sigma \in S_n$. Because of (2), it suffices to show that $\hU \nsim \hU^\sigma$ for \textit{any} $\sigma$ with $\sgn(\sigma) = -1$.

            \textbf{Show $(\huu_1, \huu_2) \nsim (-\huu_1, \huu_2)$, and then that $(\huu_1, \huu_2, ..., \huu_n) \nsim (-\huu_1, \huu_2, ..., \huu_n)$ by induction.}
        \end{itemize}
    \end{itemize}

    Now we prove (3) and (4).
    
    \begin{enumerate}
        \item[3.] Let $\pi$ be any even permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$, i.e., $\hU^{\sigma \circ \pi} \sim \hU^\pi \implies \sgn(\sigma) = 1$. If we allow $\pi$ to vary over the even permutations, then since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the even permutations, and we obtain the statement $\hU^\tau \sim \hU^\pi \implies \sgn(\tau) = 1$ for all $\tau \in S_n$, as desired.
        \item[4.] The proof is analogous to that of (3). Begin the proof with ``Let $\pi$ be any odd permutation'', and use the fact that since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the odd permutations.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Orientation of permuted ordered bases).

    \textbf{LOOK OVER THIS. MAYBE EDIT IN LIGHT OF NEW PREVIOUS THEOREM}
    
    This means that there are only two equivalence classes\footnote{I find this relatively surprising. My intuition is that there would be something like $2^n$ or $n!$ equivalence classes of ``equivalence under rotation'' in $n$ dimensions, but nope! There are $2$ equivalence classes of ``equivalence under rotation'' for every $n$.} of ``equivalence under rotation''.
    
    We can now begin to set up the notion of orientation. An \textit{orientation for the $n$-dimensional inner product space $V$} is a choice of an orthonormal ordered basis $\hU$ for $V$. When $V$ is given the orientation $\hU$, then the \textit{orientation of a permutation of $\hU$ (relative to $\hU$)} is said to be \textit{positive} iff that permutation of $\hU$ is rotationally equivalent to $\hU$, and is said to be \textit{negative} otherwise. Per the previous paragraph, every permutation of $\hU$ is either positively oriented or negatively oriented relative to $\hU$.
\end{defn}

\begin{remark}
\label{ch::antisymmetry::rmk::formalization_ccw_cw}
    (The formalization of ``counterclockwise'' and ``clockwise'').
    
    At the beginning of this section, we said that orientation would formalize the notions of ``clockwise'' and ``counterclockwise''. This formalization has been achieved by the previous definition.
    
    A \textit{counterclockwise rotational configuration} is another name for the orientation given to $\R^3$ by the standard basis, \textit{when we use the normal} human \textit{convention} of drawing the ordered basis $\sE = (\see_1, \see_2, \see_3)$ such that $\sE$ can be rotated so that $\see_1$ points out of the page, $\see_2$ points to the right, and $\see_3$ points upwards on the page. In this visual convention, the direction of each basis vector corresponds to its position in $\sE$. Counterclockwise rotational configurations are also called ``right handed coordinate systems''.
    
    A \textit{clockwise rotational configuration} then corresponds to the ordered bases which are not rotationally equivalent to $\sE$. One such ordered basis, $(-\see_1, \see_2, \see_3)$, can be depicted using the visual convention just established by drawing $\see_1$ as pointing into the page (i.e. $-\see_1$ points out of the page), $\see_2$ as pointing to the right, and $\see_3$ as pointing upwards on the page. Clockwise rotational configurations are also called ``left handed coordinate systems''.
    
    We could have easily picked a different visual convention (i.e. a different permutation of in/out, left/right, up/down) to represent the ordering of the basis that is considered to orient the space.
\end{remark}

At this point, we need some definitions and facts about $n$-rotations before we complete our development of orientation.

\subsection*{Orientation in $n$ dimensions}

\begin{defn}
    ($n$-dimensional rotations by Euler angles).

    Let $V$ be an $n$-dimensional inner product space and let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis for $V$. An \textit{($n$-dimensional) Euler rotation} is a function $\RR_{\theta_1, ..., \theta_k}:V \rightarrow V$ of the form ${\RR_{\theta_1, ..., \theta_k} = \RR_{k, \theta_k} \circ ... \circ \RR_{1, \theta_1}}$, where, for each $i$, $\RR_{i, \theta_i}$ is the $2$-dimensional rotation by $\theta_i \in [0, 2\pi)$ on the subspace $\spann(\hU - \{\huu_i\})$. The angles $\theta_1, ..., \theta_k$ are called \textit{Euler angles}.
\end{defn}

\begin{theorem}
    All Euler rotations are orthogonal linear functions.
\end{theorem}

\begin{proof}
    This follows because $2$-dimensional rotations are orthogonal linear functions, and since a composition of orthogonal linear functions is another orthogonal linear function.
\end{proof}

% \begin{theorem}
%     Outline of proof that determinant tracks orientation:

%     \begin{itemize}
%         \item arbitrary ordered basis of $V$ -> orthonormal ordered basis of $V$ via Gram-Schmidt
%         \item orthonormal ordered basis of $V$ -> permutation of $\hU$ via Theorem \ref{ch::antisymmetry::thm::n_rot_acts_on_orthonormal_basis}
%         \item Theorem \ref{ch::antisymmetry::thm::n_rot_acts_on_orthonormal_basis}: any ordered basis is rotationally close to a permuted orthonormal basis
%     \end{itemize} 

%     ``To do so, we first choose $\alpha, \beta, \gamma$ such that $\RR(\hww_1) = \huu_1$.''    
% \end{theorem}

\begin{comment}
\begin{lemma}
    (Standard matrix of Euler rotation).

    Let $V$ be a $3$-dimensional inner product space and let $\hU$ be an orthonormal ordered basis for $V$. Consider a rotation $\RR_{\alpha, \beta, \gamma}$ by the Euler angles $\alpha, \beta, \gamma \in [0, 2\pi)$. The matrix of $\RR_{\alpha, \beta, \gamma}$ relative to $\hU$ and $\hU$ is the product of the standard matrices of $\RR_\alpha$, $\RR_\beta$, and $\RR_\gamma$, and is equal to

    \begin{align*}
        &\begin{pmatrix}
            \cos(\gamma) & -\sin(\gamma) & 0 \\ \sin(\gamma) & \cos(\gamma) & 0 \\
            0 & 0 & 1
        \end{pmatrix} 
        \begin{pmatrix}
            \cos(\beta) & 0 & \sin(\beta) \\
            0 & 1 & 0 \\
            -\sin(\beta) & 0 & \cos(\beta)
            \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & \cos(\alpha) & -\sin(\alpha) \\
            0 & \sin(\alpha) & \cos(\alpha)
        \end{pmatrix} \\
        = 
        &\begin{pmatrix}
            \cos(\gamma)\cos(\beta) & \cos(\gamma)\sin(\beta)\sin(\alpha)-\sin(\gamma)\cos(\alpha) & \cos(\gamma)\sin(\beta)\cos(\alpha)+\sin(\gamma)\sin(\alpha) \\
            \sin(\gamma)\cos(\beta) & \sin(\gamma)\sin(\beta)\sin(\alpha)+\cos(\gamma)\cos(\alpha) & \sin(\gamma)\sin(\beta)\cos(\alpha)-\cos(\gamma)\sin(\alpha) \\
            -\sin(\beta) & \cos(\beta)\sin(\alpha) & \cos(\beta)\cos(\alpha)
        \end{pmatrix}.
    \end{align*}
\end{lemma}

\begin{proof}
    Left as an exercise :)
\end{proof}
\end{comment}

\begin{lemma}
    (In three dimensions, there is an Euler rotation taking any nonzero vector to any other vector).

    Let $V$ be a $3$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, \huu_2, \huu_3)$ be an orthornormal ordered basis of $V$. If we can find angles $\alpha_1, \beta_1$ that rotate $\vv$ to align with $\huu_3$,

    \begin{align*}
        (\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3,
    \end{align*}
    
    and angles $\alpha_2, \beta_2$ that rotate $\huu_3$ to align with $\ww$, 

    \begin{align*}
        (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww,
    \end{align*}

    then the Euler rotation $\RR := \RR_{2, \beta_2} \circ \RR_{1, \alpha_2} \circ \RR_{2, \beta_1} \circ \RR_{1, \alpha_1}$ satisfies $\RR(\vv) = \ww$, the lemma is proven.

    \vspace{.5cm}

    First, we find $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    Letting $(v_1, v_2, v_3)^\top := [\vv]_{\hU}$, we have
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_1) & -\sin(\alpha_1) \\
                0 & \sin(\alpha_1) & \cos(\alpha_1)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\ v_2 \\ v_3
            \end{pmatrix}
        }_{[\vv]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \\
            v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) \\
            v_2 \sin(\alpha_1) + v_3 \cos(\alpha_1)
        \end{pmatrix}.
    \end{align*}

    $\alpha_1$ is such that the second component of the above is zero,

    \begin{align*}
        v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) = 0.
    \end{align*}
    
    One value of $\alpha_1$ that solves this equation is $\alpha_1 = \arctan(v_2/v_3)$. With this choice of $\alpha_1$, we have \\ ${\cos(\alpha_1) = v_3/\sqrt{v_2^2 + v_3^2}}$ and ${\sin(\alpha_1) = v_2/\sqrt{v_2^2 + v_3^2}}$. Thus the above is
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \begin{pmatrix}
            v_1 \\
            (v_2 v_3)/\sqrt{v_2^2 + v_3^2} - (v_3 v_2)/\sqrt{v_2^2 + v_3^2} \\
            (v_2^2 + v_3^2)/\sqrt{v_2^2 + v_3^2}
        \end{pmatrix}
        =
        \begin{pmatrix}
            v_1 \\
            0 \\
            \sqrt{v_2^2 + v_3^2}
        \end{pmatrix}.
    \end{align*}

    Now we have

    \begin{align*}
        [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} &=
        [\RR_{2, \beta_1}(\RR_{1, \alpha_1}(\vv))]_{\hU} = [\RR_{2, \beta_1}(\hU)]_{\hU} [\RR_{1, \alpha_1}(\vv)]_{\hU} \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_1) & 0 & -\sin(\beta_1) \\
                0 & 1 & 0 \\
                \sin(\beta_1) & 0 & \cos(\beta_1)
            \end{pmatrix}
        }_{[\RR_{2, \beta_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\
                0 \\
                \sqrt{v_2^2 + v_3^2}
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\vv)]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) \\
            0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1)
        \end{pmatrix}.
    \end{align*}

    $\beta_1$ is such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3 \iff [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} = ||\vv||\see_3$. So $\beta_1$ satisfies the following system of equations:

    \begin{align*}
        \begin{cases}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) = 0 \\
            0 = 0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1) = ||\vv||
        \end{cases}.
    \end{align*}

    One value of $\beta_1$ that solves the first equation is $\beta_1 = \arctan(v_1/ \sqrt{v_2^2 + v_3^2})$. 
    
    Since $\cos(\beta_1) = \sqrt{v_2^2 + v_3^2}/||\vv||$ and $\sin(\beta_1) = v_1/||\vv||$, the third equation is also satisfied:

    \begin{align*}
        \frac{v_1^2}{||\vv||} + \frac{\sqrt{v_2^2 + v_3^2}^2}{||\vv||} = \frac{v_1^2 + v_2^2 + v_3^2}{||\vv||} = \frac{||\vv||^2}{||\vv||} = ||\vv||.
    \end{align*}

    Thus we have found $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    \vspace{.5cm}

    Now, we find $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
    
    We have

    \begin{align*}
        [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU} &=
        [\RR_{2, \beta_2}(\hU)]_{\hU} [\RR_{1, \alpha_2}(\hU)]_{\hU} \huu_3 \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_2) & 0 & -\sin(\beta_2) \\
                0 & 1 & 0 \\
                \sin(\beta_2) & 0 & \cos(\beta_2)
            \end{pmatrix}
        }_{[\RR_{2, \beta_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_2) & -\sin(\alpha_2) \\
                0 & \sin(\alpha_2) & \cos(\alpha_2)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                0 \\ 0 \\ 1
            \end{pmatrix}
        }_{\huu_3} \\
        &=
        \begin{pmatrix}
            \cos(\beta_2) & 0 & -\sin(\beta_2) \\
            0 & 1 & 0 \\
            \sin(\beta_2) & 0 & \cos(\beta_2)
        \end{pmatrix}
        \begin{pmatrix}
            0 \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2)
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\cos(\alpha_2) \sin(\beta_2) \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2) \cos(\beta_2)
        \end{pmatrix}.
    \end{align*}

    \newcommand{\hw}{\hat{w}}

    $\alpha_2$ and $\beta_2$ are such that $\hww = (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) \iff [\hww]_{\hU} = [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU}$. Letting $\hw_i$ denote the $i$th component of $[\hww]_{\hU}$, we see $\alpha_2$ and $\beta_2$ must satisfy the following system of equations:

    \begin{align*}
        \begin{cases}
            \hw_1 = -\cos(\alpha_2) \sin(\beta_2) \\
            \hw_2 = -\sin(\alpha_2) \\
            \hw_3 = \cos(\alpha_2) \cos(\beta_2)
        \end{cases}.
    \end{align*}

    One value of $\alpha_2$ that solves the second equation is $\alpha_2 = -\arcsin(\hw_2)$. Dividing the first equation by the third, we have $\hw_1/\hw_3 = -\tan(\beta_2)$ and thus one value of $\beta_2$ that solves the first equation is $\beta_2 = \arctan(-\hw_1/\hw_3) = -\arctan(\hw_1/\hw_3)$.

    Thus we have found $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
\end{proof}

The previous lemma can easily be generalized so that it holds for all dimensions greater than or equal to $2$.

\begin{lemma}
    (There is an Euler rotation taking any nonzero vector to any other vector).
    
    Let $V$ be an $n$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    \mbox{} \\

    (Case: $n = 2$). The rotation $\RR_{\theta(\vv, \ww)}$ by $\theta(\vv, \ww)$ sends $\vv$ to $\ww$.
    
    (Case: $n = 3$). See the previous lemma.
    
    (Case: $n > 3$).
    
    \indent (Case: $n$ is even). [TO-DO]
    
    \indent (Case: $n$ is odd). [TO-DO]
\end{proof}

\begin{theorem}
\label{ch::antisymmetry::thm::n_rot_acts_on_orthonormal_basis}
     (Any ordered basis is ``rotationally close'' to a permuted orthonormal basis).
     
     Let $V$ be an finite-dimensional inner product space. If $\hW = (\hww_1, ..., \hww_k)$ and $\hU = (\huu_1, ..., \huu_k)$ are orthonormal ordered bases of subspaces of $V$, then there is an Euler rotation taking $\hW$ to an ordered basis that is rotationally equivalent to some permutation $\hU^\sigma$ of $\hU$. 
\end{theorem}

\begin{proof}
    By the previous lemma, we know there is an Euler rotation $\RR$ that satisfies $\RR(\hww_1) = \huu_1$. 
    
    We first prove that for all $i$ we have $\RR(\hww_i) = s_i \huu_i$, where $s_i \in \{-1, 1\}$. And we show this by using induction to show that for all $i$, $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    (Base case). We have $R(\hww_1) = \huu_1 = s_1 u_1$, where $s_1 = 1$.

    (Inductive case). Assume that $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    Since $\RR$ is an orthogonal linear function, it preserves orthonormality (\textbf{see Theorem [...]}), and so $(\RR(\hww_1), ..., \RR(\hww_k))$ is orthonormal. Thus, for each $j$, $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \cancel{\RR(\hww_j)}, ..., \RR(\hww_k)$. In particular, when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \RR(\hww_i)$. By the inductive hypothesis, $\RR(\hww_1) = s_1 \huu_1, ..., \RR(\hww_i) = s_i \huu_i$, where $s_1, ..., s_i \in \{-1, 1\}$. So when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $s_1 \huu_1, ..., s_i \huu_i$; it follows that each $\RR(\hww_j)$ with $j \geq i + 1$ is perpendicular to all of $\huu_1, ..., \huu_i$. This means that each $\RR(\hww_j)$ with $j \geq i + 1$ is in the orthogonal complement of $\spann((\huu_1, , \huu_i))$.
    
    This orthogonal complement is equal to $\spann((\huu_{i + 1}, , \huu_k)$. Therefore $\RR(\hww_{i + 1}), ..., \RR(\hww_k) \in \spann(\huu_{i + 1}, ..., \huu_k)$. Since $\RR$ preserves orthonormality (\textbf{again, see Theorem [...]}), then $\RR(\hww_{i + 1}), ..., \RR(\hww_k)$ are also orthonormal.
    
    Since $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1}, ..., \huu_k)$, then $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1} + c_{i + 2} \huu_{i + 2} + ... + c_k \huu_k$ for some scalars $c_{i + 1}, ..., c_k$. We claim that we must have $c_{i + 2} = ... = c_k = 0$; assume for contradiction that $c_{\ell_1}, ..., c_{\ell_p}$ are nonzero scalars with $\ell_1, ..., \ell_p > i + 1$. A quick inner product computation shows that $\RR(\hww_{i + 1})$ is not perpendicular to any of $\huu_{\ell_1}, ..., \huu_{\ell_p}$. Since $\RR(\hww_{i + 2}), ..., \RR(\hww_k)$ must all be perpendicular to $\RR(\hww_{i + 1})$, it follows that $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \notin \spann((\huu_{\ell_1}, ..., \huu_{\ell_p}))$. Thus $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \in \spann((\huu_{i + 2}, ..., \huu_k) - (\huu_{\ell_1}, ..., \huu_{\ell_p}))$, which is a subspace of dimension $|[i + 2, k] \cap \Z| - p$. On the other hand, since $(\RR(\hww_{i + 2}, ..., \RR(\hww_k))$ is an orthonormal basis, then $(\RR(\hww_{i + 2}), ..., \RR(\hww_k))$ span a $|[i + 2, k]|$-dimensional space. This is a contradiction; vectors cannot be in a space that has a lesser dimension than their span. Therefore, $c_{i + 2} = ... = c_k = 0$, and $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1}$, i.e., $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1})$. Since $\RR(\hww_{i + 1})$ is a unit vector, then $||\RR(\hww_{i + 1})|| = ||s_{i + 1} \huu_{i + 1}|| = |s_{i + 1}| = 1$. Thus $s_{i + 1} \in \{-1, 1\}$, and $\RR(\hww_{i + 1}) = s_{i + 1} \huu_{i + 1}$, as desired.

    \vspace{.25cm}

    Since $\RR(\hww_i) = s_i \huu_i$ for all $i$, where $s_i \in \{-1, 1\}$, then $\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k)$, where $s_1, ..., s_k \in \{-1, 1\}$. 

    If the number of $s_i$ equal to $-1$ is even, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = 1$, and if the number of $s_i$ equal to $-1$ is odd, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = -1$. (Recall Derivation     \ref{ch::antisymmetry::thm::antisymmetry_ordered_bases_signs}). We see that in either case, ${\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for some $\sigma \in S_k$. This proves the theorem.
\end{proof}

\subsubsection*{Completing the definition of orientation for finite-dimensional inner product spaces}

\begin{defn}
    (Orientation of arbitrary ordered bases).
    
    Let $V$ be an $n$-dimensional inner product space, and fix an orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ for $V$. We know how to ``orient'' ordered bases for $V$ that happen to be permutations of $\hU$. Now, we generalize the notion of orientation so that it applies to any orthonormal ordered basis of $V$.
    
    We define the \textit{orientation of an orthonormal ordered basis $E$ of $V$} that is not a permutation of $\hU$ to be the orientation of the unique permuted orthonormal basis $\hU^\sigma$, $\sigma \in S^n$, of $\hU$ for which there exists an $n$-rotation taking $E$ to $\hU^\sigma$.
    
    Then, we define the \textit{orientation of an arbitrary orthonormal ordered basis $E$ of $V$} to be the orientation of the unique orthonormal basis $\hU_E$ obtained from performing the Gram-Schmidt process on $E$ (see Theorem \ref{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}).
\end{defn}

\begin{theorem}
\label{ch::antisymmetry::thm::det_tracks_orientation}
    (The determinant tracks orientation). 
    
    Let $V$ be an $n$-dimensional inner product space with an orientation given by an orthonormal ordered basis $\hU$. Let $E = (\ee_1, ..., \ee_n)$ be any ordered basis (not necessarily orthonormal) of $V$. We have $\det([\EE]_{\hU}) > 0$ iff $E$ is positively oriented relative to $\hU$, and $\det([\EE]_{\hU}) < 0$ iff $E$ is negatively oriented relative to $\hU$.
\end{theorem}

\begin{proof}
   This proof has two overarching steps. First, we pass the definition of orientation for arbitrary ordered bases of $V$ to the definition of orthonormal ordered bases of $V$ by obtaining an orthonormal ordered basis $\hU_E$ from $E$. Then we pass the definition of orientation for orthonormal ordered bases of $V$ that are not permutations of $\hU$ to the definition of orientation for orthonormal ordered bases of $V$ that are permutations of $\hU$.
   
   To begin the first step, consider $\det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$, and perform Gram-Schmidt on $([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$. In the $i$th step of Gram-Schmidt, a linear combination of the vectors $[\ee_1]_{\hU}, ..., \cancel{[\ee_i]_{\hU}}, ..., [\ee_n]_{\hU}$ is added to $[\ee_i]_{\hU}$. Recall from Theorem \ref{ch::antisymmetry::thm::consequent_det_props} that the determinant is invariant under linearly combining input vectors into a different input vector. Therefore, performing Gram-Schmidt does not change the determinant. That is, if $\hU_E = (\hww_1, ..., \hww_n)$ is the orthonormal basis obtained by performing Gram-Schmidt on $E$, then 
   
   \begin{align*}
       \det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU}) = \det([\hww_1]_{\hU}, ..., [\hww_n]_{\hU})
       =
       \det([\hU_E]_{\hU}).
   \end{align*}
   
   In performing this first step of the proof, the determinant has stayed the same as we've passed from $E$ to $\hU_E$. We now show that the determinant continues to stay the same as we pass from  $\hU_E$ to some permutation $\hU^\sigma$ of $\hU$.
   
   Theorem \ref{ch::antisymmetry::thm::n_rot_acts_on_orthonormal_basis} says that there is a $n$-rotation $\RR$ taking $\hU_E$ to $\hU^\sigma$, for some $\sigma \in S_n$, and Theorem \ref{ch::antisymmetry::thm::n_dim_rot_det_1} guarantees that $\det(\RR) = 1$. Thus, since $\hU^\sigma = \RR(\hU_E)$, we have
   
   \begin{align*}
        \det([\hU_E]_{\hU}) 
        = \det([\RR(\hU_E)]_{\hU}) \det([\hU_E]_{\hU})
        = \det([(\RR \circ \II)(\hU_E)]_{\hU})
        = \det([\RR(\hU_E)]_{\hU})
        = \det([\hU^\sigma]_{\hU}).
   \end{align*}
   
   To conclude the proof, we will show that $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$; once we have shown this, we are done, since $\sgn(\sigma) \det([\hU]_{\hU}) = \sgn(\sigma) \det(\II) = \sgn(\sigma)$. Since any permutation is a composition of transpositions, then $\hU^\sigma$ can be obtained from $\hU$ by repeatedly swapping vectors in $\hU$. Whenever vectors are swapped in the determinant, the sign of the determinant is multiplied by $-1$. This accounts for the $\sgn(\sigma)$ factor in the equation $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$.
\end{proof}

\subsection*{Orientation of finite-dimensional vector spaces}

\label{ch::antisymmetry::orientation_finite_dim_vector_space}

The fact that the determinant tracks orientation is the main result of our discussion of orientation. Because determinants do not rely on the existence of an inner product, the determinant can be used to generalize the notion of orientation to any finite-dimensional vector space.

\begin{defn}
\label{ch::antisymmetry::defn::orientation_finite_dim_vector_space}
    (Orientation of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space (not necessarily an inner product space). An \textit{orientation on $V$} is a choice of ordered basis $E$ for $V$. (Notice here that $E$ is not necessarily orthonormal, because $V$ might not have an inner product!). If we have given $V$ the orientation $E$, then we say that an ordered basis $F$ of $V$ is \textit{positively oriented (relative to $E$)} iff $\det([\FF]_E) > 0$, and that $F$ is \textit{negatively oriented (relative to $E$)} iff $\det([\FF]_E) < 0$.
    
    A finite-dimensional vector space that has an orientation is called an \textit{oriented (finite-dimensional) vector space}.
\end{defn}
 
\begin{remark}
    (Antisymmetry of orthonormal ordered bases).
    
    Notice that we still have the previous antisymmetry of orthonormal ordered bases due to the antisymmetry of the determinant.
\end{remark}

As a last sidenote, the following theorem gives some justification as to why our definition of $n$-rotation was a good definition. (Strictly speaking, though, the justification is somewhat circular, since we used the notion of $n$-rotations to explain how the determinant- which is involved in the justification- tracks orientation).

\begin{theorem}
    If $V$ is an $n$-dimensional inner product space, then 
    
    \begin{align*}
        \{\text{$n$-rotations on $V$}\} = \{\text{orthogonal linear functions $V \rightarrow V$ with determinant 1}\}
    \end{align*}
\end{theorem}

\begin{proof}
    \scriptsize Proof idea is from jagr2808. 
    \fontsize{10pt}{12pt}\selectfont \\
    \indent ($\subseteq$). This is just Theorem \ref{ch::antisymmetry::thm::n_dim_rot_det_1}.
    
    \indent ($\supseteq$). If $\ff:V \rightarrow V$ is an orthogonal linear function, then $\ff$ sends an arbitrary orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ of $V$ to another orthonormal basis $\ff(\hU) = (\ff(\huu_1), ..., \ff(\huu_n))$.

    Let $\RR_1$ be the $2$-dimensional rotation sending $\ff(\huu_1)$ to $\huu_1$. Then $\RR_2 := \RR_1 \circ \ff:V \rightarrow V$ fixes $\huu_1$, so we can think of it as a function from $\spann(\huu_2, ..., \huu_n)$ to $\spann(\huu_2, ..., \huu_n)$. 
    
    We now use the above idea finitely many times. Define $\RR_{i + 1}$ to be the $2$-dimensional rotation sending ${(\RR_i \circ \RR_{i - 1})(\huu_i)}$ to $\huu_i$. We know that such a $2$-dimensional rotation always exists because $\det(\ff) = 1$ implies that $(\ff(\huu_1), ..., \ff(\huu_n))$ has the same orientation as $(\huu_1, ..., \huu_n)$. (Recall, having the same orientation involves ``rotational equivalence'' via $n$-rotations, which are compositions of $2$-dimensional rotations). By induction, $\RR_n \circ \ff$ is a composition of $2$-dimensional rotations.
    
    Thus, since $\ff = \RR_n^{-1} \circ (\RR_n \circ \ff)$, we see $\ff$ is a composition of the $2$-dimensional rotations $\RR_n^{-1}$ and $\RR_n^{-1} \circ \ff$.
    
    %If T is an orthogonal endomorphism then T maps the standard basis e_1, ..., e_n to an orthonormal basis u_1, ..., u_n.

    %Let R be the 2-rotation sending u_1 to e_1. Then RT is an endomorphism in SO(n) that fixes e_1, so we can think of it as an endomorphism in SO(n-1) on the orthogonal complement. By induction RT is the composition of 2-rotations. And T = R-1RT, so T is as well.
\end{proof}

\newpage

\chapter{The wedge product and exterior powers}

Earlier, we constructed tensor product spaces so that we could think of multilinear functions on vectors as linear functions of objects that appeared to be multilinear (tensors). In our study of the determinant, we've seen that multilinear alternating functions are also highly relevant. So, we now seek a way to represent these functions with linear functions. Following the pattern established with multilinear functions, we will represent multilinear alternating functions as linear functions of objects that appear to be multilinear and alternating. These objects will be called \textit{wedge products}. Just as tensor products use the notation $\otimes$, wedge products will use the notation $\wedge$.

\section*{The wedge product}

First, we remind the reader of the definition of an alternating function, and define notation for the vector space of all such functions.

\begin{defn}
    (Alternating function).

    If $V$ and $W$ are vector spaces, then a function $\ff:V^{\times k} \rightarrow W$ is \textit{alternating} iff \\ ${\ff(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_k) = -\ff(\vv_1, ..., \vv_j, ..., \vv_i, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$. That is, $\ff$ is alternating iff swapping inputs is equivalent to negating the output obtained by evaluating the original ordering of inputs.
    
    Equivalently, $\ff:V^{\times k} \rightarrow W$ is alternating iff
    ${\ff(\vv_{\sigma(1)}, ..., \vv_{\sigma(k)}) = \sgn(\sigma) \ff(\vv_1, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$ and $\sigma \in S_k$.
\end{defn}

\begin{defn}
    (Vector space of alternating functions).
    
    If $V$ and $W$ are vector spaces over a field $K$, then we use $(\alt \LLLL)(V^{\times k} \rightarrow W)$ to denote the vector space over $K$ of alternating functions $V^{\times k} \rightarrow W$ under the operations of function addition and function scaling.
\end{defn}

Now we define the wedge product, very similarly to how we defined the tensor product (recall Definition \ref{ch::motivated_intro::defn::tensor_product_space}).

\begin{defn}
    (Wedge product and exterior powers).
    
    Let $V$ be a finite-dimensional vector space over a field $K$. The \textit{$k$th exterior power of $V$}, $\underbrace{V \wedge ... \wedge V}_{\text{$k$ times}} = V^{\wedge k}$, is defined to be the vector space over $K$ whose elements are from the set $\underbrace{V \times ... \times V}_{\text{$k$ times}} = V^{\times k}$, where the elements are also subject to an equivalence relation $=$, which will be specified soon. We also denote a typical element of $\underbrace{V \wedge ... \wedge V}_{\text{$k$ times}} = V^{\wedge k}$ as $\vv_1 \wedge ... \wedge \vv_k$, rather than as $(\vv_1, ..., \vv_k)$. 
    
    The equivalence relation is defined by by the following conditions:
    
    \begin{enumerate}
        \item We require
        \begin{align*}
            \vv_1 \wedge ... \wedge \vv_{i - 1} &\wedge \vv_{i} \wedge \vv_{i + 1} ... \wedge \vv_k \\
            &+ \\
            \vv_1 \wedge ... \wedge \vv_{i - 1} &\wedge \vv_{j} \wedge \vv_{i + 1} ... \wedge \vv_k \\
            &= \\
            \vv_1 \wedge ... \wedge \vv_{i - 1} \wedge (\vv_{i} &+ \vv_{j}) \wedge \vv_{i + 1} ... \wedge \vv_k
        \end{align*}

        for all $\vv_1, ..., \vv_k \in V$, and
        
        \begin{align*}
            c (\vv_1 \wedge ... \wedge &\vv_i \wedge ... \wedge \vv_k ) \\
            &= \\
            \vv_1 \wedge ... \wedge (c &\vv_i) \wedge ... \wedge \vv_k,
        \end{align*}

        for all $\vv_1, ..., \vv_k \in V$ and $c \in K$, so that $\wedge$ looks like it is a multilinear function.

        \item We require

        \begin{align*}
            \vv_1 \wedge ... \wedge \vv_i \wedge &... \wedge \vv_j \wedge ... \wedge \vv_k \\
            &= \\
            -\vv_1 \wedge ... \wedge \vv_j \wedge &... \wedge \vv_i \wedge ... \wedge \vv_k,
        \end{align*}

        for all $\vv_1, ..., \vv_k \in V$, so that $\wedge$ looks like it is an alternating function.
    \end{enumerate}
\end{defn}

\begin{remark}
    (Bivectors, trivectors, blades).

    Elementary wedge products of the form $\vv_1 \wedge \vv_2$ are sometimes called ``bivectors'', elementary wedge products of the form $\vv_1 \wedge \vv_2 \wedge \vv_3$ are sometimes called ``trivectors'', and elementary wedge products of the form $\vv_1 \wedge ... \wedge \vv_k$ are sometimes called ``blades''.
\end{remark}

Already, we are able to see how wedge products allow us to represent multilinear alternating functions as linear functions.

\begin{theorem}
    (Universal property for exterior powers).
    
    Let $V$ and $W$ be vector spaces over the same field, and let $\ff:V^{\times k} \rightarrow W$ be an alternating bilinear function.]
    
    There exists a linear function $\widetilde{\ff}:V^{\wedge k} \rightarrow W$ uniquely corresponding to $\ff$ such that $\ff(\vv_1, ..., \vv_k) = \widetilde{\ff}(\vv_1 \wedge ... \wedge \vv_k)$.

    Equivalently, there are linear functions $\widetilde{\ff}:V^{\wedge k} \rightarrow W$ and $\gg:V^{\times k} \rightarrow V^{\wedge k}$, with $\gg$ defined on elementary tuples as $\gg(\vv_1, ..., \vv_k) := \vv_1 \wedge ... \wedge \vv_k$, and with $\widetilde{\ff}$ uniquely corresponding to $\ff$, such that $\ff = \widetilde{\ff} \circ \gg$.
\end{theorem}

\begin{proof}
    The proof is similar to the proof of the universal property of tensor product spaces (Theorem \ref{ch::lin_alg::thm::universal_prop_tensor_prod}). The only difference is that the maps we define in this proof are extended using antisymmetry and multilinearity, rather than just multilinearity.
\end{proof}

\begin{lemma}
    TO-DO:
    
    \begin{itemize}
        \item move proof of $\vv_1 \wedge ... \wedge \vv_k = \det(\ff) \ff(\vv_1) \wedge ... \wedge \ff(\vv_k)$ here
        \item show that $\wedge$ has similar properties to ``consequent properties of det'' underneath
        \item put a restatement of first bullet point in pushforward and pullback section
        \item this leaves one theorem from current ``wedge products and determinant'' section that should also be moved to pushforward and pullback section; it should go right below restatement of first bullet point in that section
    \end{itemize}

    also, maybe pushforward and pullback can go after ext powers as vector spaces of functions?
\end{lemma}

\begin{theorem}
    (Associativity of wedge product). 
    
    Let $V$ be a vector space. Then there are natural isomorphisms
    
    \begin{align*}
        (V \wedge V) \wedge V \cong V \wedge V \wedge V \cong V \wedge (V \wedge V).
    \end{align*}

    Because of this isomorphism, we will always assume $(\vv_1 \wedge \vv_2) \wedge \vv_3 = \vv_1 \wedge \vv_2 \wedge \vv_3 = \vv_1 \wedge (\vv_2 \wedge \vv_3)$ for all $\vv_1, \vv_2, \vv_3 \in V$.
\end{theorem}

\begin{proof}
    Replacing the symbol $\otimes$ with $\wedge$ in the proof of the associativity of the tensor product $\otimes$ from Theorem \ref{ch::motivated_intro::thm::tensor_product_associative} gives a proof of this theorem. 
\end{proof}

\begin{theorem}
    (Basis and dimension of exterior powers).
    
    Let $V$ be an $n$-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$. When $k \leq n$, then $\Lambda^k(V)$ is a vector space with basis
    
    \begin{align*}
        \{ \ee_{i_1} \wedge ... \wedge \ee_{i_k} \mid i_1, ..., i_k \in \{1, ..., n\} \text{ and } i_1 < ... < i_k \}.
    \end{align*}

    The number of basis elements here is equal to the number of length-$k$ strictly increasing subsequences of $(1, ..., n)$, which is $\binom{n}{k}$. Thus, when $V$ is $n$-dimensional and $k \leq n$, we have $\dim(\Lambda^k(V)) = \binom{n}{k}$.

    When $k > n$, then $\Lambda^k(V) = \{\mathbf{0}\}$.
\end{theorem}

\begin{proof}
    Use the seeming-multilinearity of $\wedge$, as was done for $\otimes$ in the proof of Theorem \ref{ch::motivated_intro::thm::basis_dim_tensor_product_space}, to show the proposed basis spans $\Lambda^k(V)$ for all $k$. It remains to show that $\Lambda^k(V) = \{\mathbf{0}\}$ when $k > n$ and that the proposed basis is linearly independent when $k \leq n$.

    (Case: $k > n$). When $k > n$, then every basis element is a wedge product in which at least one vector is repeated. Since a wedge product with a repeated vector is the zero wedge product, then when $k > n$ all vectors in the proposed basis are the zero wedge product, and the proposed basis is $\{\mathbf{0}\}$, which is not linearly independent. The proposed basis, $\{\mathbf{0}\}$, still spans $\Lambda^k(V)$, though, so we can conclude that $\Lambda^k(V) = \{\mathbf{0}\}$.

    (Case: $k \leq n$). Suppose 

    \begin{align*}
        \sum_{i_1 < ... < i_k} T^{i_1 ... i_k} \ee_{i_1} \wedge ... \wedge \ee_{i_k} = \mathbf{0}.
    \end{align*}

    Consider $T^{j_1 ... j_k}$, where $j_1, ..., j_k \in \{1, ..., n\}$ are arbitrary. In order for the wedge products in the above sum to be linearly independent, we must show $T^{j_1 ... j_k} = 0$.

    Let $j_{k + 1}, ..., j_n \in \{1, ..., n\}$ be ``complementary'' to $j_1, ..., j_k$, so that $\{j_1, ..., j_k, j_{k + 1}, ..., j_n\} = \{1, ..., n\}$. Taking the wedge product of the ``corresponding complementary wedge product'', $\ee_{j_{k + 1}} \wedge ... \wedge \ee_{j_n}$, with both sides of the previous equation and distributing, we have

    \begin{align*}
        \sum_{i_1 < \ldots < i_k} T^{i_1 \ldots i_k} \, \ee_{i_1} \wedge \ldots \wedge \ee_{i_k} \wedge \ee_{j_{k + 1}} \wedge \ldots \wedge \ee_{j_n} = \mathbf{0}.
    \end{align*}

    Any wedge product in the sum that contains duplicate vectors is the zero wedge product. The only wedge product that does not contain any duplicate vectors is the one in which $(i_1, ..., i_k) = (j_1, ..., j_k)$. Thus, we have

    \begin{align*}
        T^{j_1 ... j_k} \ee_{j_1} \wedge ... \wedge \ee_{j_k} \wedge \ee_{j_{k + 1}} \wedge ... \wedge \ee_{j_n} = \mathbf{0}.
    \end{align*}

    Since $E$ is linearly independent, then the above wedge product is nonzero (recall Theorem [will have a previous theorem about how wedge of vectors is zero iff the vectors are linearly dependent]). This forces $T^{j_1 ... j_k} = 0$, as desired. 
\end{proof}

\begin{comment}
\begin{proof}
    (Sergei Winitzki's approach).

    (Lemma, p. 52). For all $\phi \in V^*$, there exists a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$ such that $\phi(\ee_1) = 1$ and $\phi(\ee_i) = 0$ for all $i \geq 2$.

    Proof idea: interpret $\phi$ as $\epsilon^1$, where $E^* = \{\epsilon^1, ..., \epsilon^n\}$ is the dual basis for $V$ induced by some basis $E$ of $V$. Compute $E = F^{**}$.

    Proof of linear independence. Assume for contradiction that a linear combination of elements from the proposed basis is nontrivially zero:

    \begin{align*}
        \sum_{\ell = 1}^{\binom{n}{k}} c_\ell \ee_{i_{\ell 1}} \wedge &... \wedge \ee_{i_{\ell k}} = \mathbf{0},
    \end{align*}

     where not all of $c_1, ..., c_{\binom{n}{k}}$ are $0$. Let $\phi \in V^*$ and apply the interior product $\iota_\phi$ to the sum to obtain

     \begin{align*}
         \sum_{\ell = 1}^{\binom{n}{k}} \sum_{\sigma \in S_k} \sgn(\sigma) \phi(\ee_{i_{\ell \sigma(1)}}) \ee_{i_{\ell 2}} \wedge ... \wedge \ee_{i_{\ell r}} &= \mathbf{0}
     \end{align*}

     Without loss of generality, we can assume $c_1 \neq 0$. We also know from the lemma that there exists $\phi \in V^*$ such that $\phi(\ee_{i_{\ell 1}}) = 1$, with $\phi(\ee_j) = 0$ for all $j \neq i_{\ell 1}$. Using this $\phi$, we see

     \begin{align*}
         \sum_{\sigma \in S_k} \ee_{i_{\ell 2}} \wedge ... \wedge \ee_{i_{\ell r}} = \mathbf{0}.
     \end{align*}

     This contradicts the inductive hypothesis.
\end{proof}
\end{comment}

\begin{theorem}
\label{ch::antisymmetry::thm::fundamental_isos_exterior_pwrs}
    (Fundamental natural isomorphisms for exterior powers). 
    
    Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos} stated that there are natural isomorphisms
    
    \begin{align*}
        \LLLL(V_1 \times ... \times V_k \rightarrow W) &\cong \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W) \\
        \LLLL(V \rightarrow W) &\cong W \otimes V^* \\
        (V \otimes W)^* &\cong V^* \otimes W^* \\
        T_{p,q}(V) &\cong T_{q,p}(V^*)
    \end{align*}
    
    Analogously, there are natural isomorphisms
    
    \begin{empheq}[box = \fbox]{align*}
        (\alt \LLLL)(V_1 \times ... \times V_k \rightarrow W) &\cong (\alt \LLLL)(V_1 \wedge ... \wedge V_k \rightarrow W) \\
        (\alt \LLLL)(V \rightarrow W) &\cong W \wedge V^* \\
        (V \wedge W)^* &\cong V^* \wedge W^* \\
        \Lambda^k(V)^* &\cong \Lambda^k(V^*)
    \end{empheq}
\end{theorem}

\begin{proof}
     To show the first equation in the box, show that the function sending an alternating bilinear function to its unique linear counterpart defined on wedge product spaces (which is guaranteed to exist by the universal property for exterior powers) is a linear isomorphism. (This is what we did when we proved the coorresponding fact for tensor product spaces; those steps can essentially be repeated for this proof. See Theorem \ref{ch::motivated_intro::thm::multilin_fns_iso_lin_fns}). This proves the first equation for the case $k = 2$. The general result follows by induction.  
     
    To prove the second line, use a similar isomorphism as was presented at the end of Section \ref{ch::motivated_intro::sec::motivated_intro}, when we derived the natural isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$. That is, take an element $\ff \in (\alt \LLLL)(V \rightarrow W)$, decompose it into a linear combination of ``alternating elementary compositions'', and then send each alternating elementary composition $\ww \circ \phi \mapsto \ww \wedge \phi$. The formal check that this function is a linear isomorphism is essentially the same as the check described at the end of Section \ref{ch::motivated_intro::sec::motivated_intro}.
    
    The third line in the box is proved similarly as was in Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos}; the only difference is that it is necessary to extend with antisymmetry and bilinearity rather than just bilinearity. The fourth line follows from the third line.
\end{proof}

\section*{Construction of the wedge product}

\begin{defn}
    (Permutation of a tensor).
    
    Let $V_1, ..., V_k$ be vector spaces over the same field. Given a permutation $\sigma \in S_k$ and a tensor $\TT \in V^{\otimes k}$, we define the function $(\cdot)^\sigma:V^{\otimes k} \rightarrow V^{\otimes k}$ sending $\TT \mapsto \TT^\sigma$ by specifying its action on elementary tensors and extending linearly. We define
    
    \begin{align*}
        (\vv_1 \otimes ... \otimes \vv_k)^\sigma := \vv_{\sigma(1)} \otimes ... \otimes \vv_{\sigma(k)}.
    \end{align*}
\end{defn}

\begin{defn}
\label{ch::antisymmetry::defn::antisymmetric_tensor}
    (Antisymmetric tensor).
    
    Let $V$ be a vector space. A tensor $\TT \in V^{\otimes k}$ is \textit{antisymmetric} iff  $\TT^\sigma = -\TT$ for all transpositions $\sigma$.
\end{defn}

\begin{example}
    (Antisymmetric tensor of order $2$).

    Let $V_1$ and $V_2$ are vector spaces and $\vv_1, \vv_2 \in V$. The tensor $\TT = \vv_1 \otimes \vv_2 - \vv_2 \otimes \vv_1$ in $V^{\otimes 2}$ is antisymmetric, since for all $\sigma \in S_2 = \{\text{id}, (1 \spc 2)\}$ we have $\TT^\sigma = \sgn(\sigma) \TT$:

    \begin{align*}
        \TT^\id &= (\vv_1 \otimes \vv_2 - \vv_2 \otimes \vv_1)^{\id} = \vv_1 \otimes \vv_2 - \vv_2 \otimes \vv_1 = \TT = \sgn(\id) \TT, \\
        \TT^{(1 \spc 2)} &= (\vv_1 \otimes \vv_2 - \vv_2 \otimes \vv_1)^{(1 \spc 2)} = \vv_2 \otimes \vv_1 - \vv_1 \otimes \vv_2 = -(\vv_1 \otimes \vv_2 - \vv_2 \otimes \vv_1) = -\TT = \sgn((1 \spc 2)) \TT.
    \end{align*}
\end{example}

\begin{remark}
\label{ch::antisymmetry::rmk::alternating_antisymmetric}

    (``Antisymmetric'' vs. ``alternating'').
    
    Many authors refer to the ``antisymmetric tensors'' of Definition \ref{ch::antisymmetry::defn::antisymmetric_tensor} as ``alternating tensors''. Doing so is technically incorrect, because an ``alternating tensor'' is an element of a certain ``quotient algebra''. This quotient algebra is isomorphic to an algebra of antisymmetric tensors only when the characteristic of the field $K$ is infinity. (We have and will not define algebras or field characteristics). In other words, ``antisymmetric tensors'' are only the same as ``alternating tensors'' in certain special cases.
    
    Given the previous paragraph, one might think that ``alternating functions'' should really be called ``antisymmetric functions''. This is actually not the case. We are correct to define the functions of the above to be ``alternating functions'' because the alternating functions of the above definition correspond to alternating tensors (which, remember, are elements of a certain quotient algebra) via \textit{the universal property of the exterior algebra}. So, alternating functions share the same level of generality as alternating tensors (which we have not defined). Antisymmetric tensors, which we have defined, are what are ``more specific''.
    
    We have not defined what the exterior algebra is (and won't), but it is good to know this context.
\end{remark}

Now, we generalize the pattern shown in the previous example to define a function that produces antisymmetric tensors from arbitrary tensors. This function will be crucial to our construction of the wedge product.

\begin{defn}
    (Antisymmetrization of elements of tensor product spaces).
    
    Let $V$ be a vector space over the same field. We define ${\alt:V^{\otimes k} \rightarrow V^{\otimes k}}$ on elementary tensors and extend linearly; for an elementary tensor $\TT \in V^{\otimes k}$, we define
    
    \begin{align*}
        \alt(\TT) := \frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) \TT^\sigma.
    \end{align*}

    In the below, we give proof that $\alt(\TT)$ is antisymmetric for all $\TT \in V^{\otimes k}$.
    
    Note that $\TT$ is antisymmetric, then the sum argument is $\sgn(\sigma) \TT^\sigma = \sgn(\sigma)^2 \TT = \TT$, and so the sum is $k! \TT$. Thus, the division by $k!$ ensures that $\alt(\TT) = \TT$ when $\TT$ is an antisymmetric tensor.
\end{defn}

\begin{proof}
    We need to show that $\alt(\TT)$ is antisymmetric, i.e., that $\alt(\TT)^\pi = \sgn(\pi) \alt(\TT)$. We have
            
    \begin{align*}
        \alt(\TT)^\pi &= \Big(\sum_{\sigma \in S_k} \sgn(\sigma) (\TT^\sigma)\Big)^\pi = \sum_{\sigma \in S_k} \sgn(\sigma) (\TT^\sigma)^\pi = \sum_{\sigma \in S_k} \sgn(\sigma) \TT^{\pi \circ \sigma}.
    \end{align*}
            
    Since $S_k$ is closed under taking the inverse of a permutation (recall, a permutation in $S_k$ is a bijection on $\{1, ..., k\}$), then for every $\sigma \in S_k$ there is a $\tau \in S_k$ such that $\tau = \pi \circ \sigma \iff \sigma = \pi^{-1} \circ \tau$). So the sum becomes
            
    \begin{align*}
        \sum_{\tau \in S_k} \sgn(\pi^{-1} \circ \tau) \TT^\tau &= \sgn(\pi^{-1}) \sum_{\tau \in S_k} \sgn(\tau) \TT^\tau = \sgn(\pi) \alt(\TT),
    \end{align*}

    as desired.
\end{proof}

\begin{remark}
    (Alternating tensors exist).

    Since $\alt(\TT)$ is antisymmetric for all tensors $\TT$, we can conclude that alternating tensors exist.
\end{remark}

\begin{defn}
    (Wedge product).
    
    Let $V_1, ..., V_k$ be vector spaces over the same field. We define the \textit{wedge product} ${\wedge: V^{\otimes k} \otimes V^{\otimes k} \rightarrow \alt(V^{\otimes k} \otimes V^{\otimes k}})$ by $\TT \wedge \SS := \alt(\TT \otimes \SS)$.
\end{defn}

\begin{remark}
\label{ch::antisymmetry::rmk::exterior_product}
    (``Exterior product'').

    The wedge product is sometimes called the \textit{exterior product}. One may notice that this sounds thematically similar to ``outer product''.

    Recall that in Remark \ref{ch::motivated_intro::rmk::outer_products}, we explained that the relationship between the suggestive terms ``inner product'' and ``outer product'' is more coincidental than fundamental. The situation is much the same between ``outer product'' and ``exterior product'', as the only real connection between ``exterior product'' and ``outer product'' is that since the exterior product involves a tensor product of vectors, which can be identified with an outer product of vectors.
    
    In general, the terms ``inner product'', ``outer product'', and ``exterior product'' are more connected by coincidence than by a fundamental relationship.
\end{remark}

\begin{lemma}
    (Lemma for associativity of wedge product). 
    
    Let $V$ be a vector space, and consider $\TT, \SS \in V^{\otimes k}$. If $\alt(\TT) = \mathbf{0}$, then $\TT \wedge \SS = \mathbf{0} = \SS \wedge \TT$.
\end{lemma}

\begin{proof}
    \newcommand{\pit}{[\pi]_\tau}

    (This proof requires some abstract algebra. Understanding this proof is not really necessary to understand exterior powers, so you can take this theorem as an axiom if you want).

    Assume $\alt(\TT) = \mathbf{0}$. Let $\TT = \vv_{i_1} \otimes ... \otimes \vv_{i_{k}}$ and $\SS = \vv_{j_1} \otimes ... \otimes \vv_{j_k}$. We must show $\alt(\TT \otimes \SS) = \mathbf{0}$. 
    
    To do so, let $H$ be the subgroup of $S_{2k}$ whose elements fix all of $j_1, ..., j_k$, and consider the cosets ${\{H \sigma \mid \sigma \in S_{2k}\}}$ of $H$ in $S_{2k}$. Since these cosets partition $S_{2k}$, then
    
    \begin{align*}
        \alt(\TT \otimes \SS) &= \sum_{\pit \sigma \in \{\text{cosets}\}} \text{sgn}({\pit \sigma}) (\TT \otimes \SS)^{\pit \sigma} = \sum_{\sigma \in S_{2k}} \sum_{\pit \in H} \text{sgn}({\pit \sigma}) (\TT \otimes \SS)^{\pit \sigma} \\
        &= \sum_{\sigma \in S_{2k}} \Big(\sum_{\pit \in H} \text{sgn}({\pit}) (\TT \otimes \SS)^{\pit} \Big)^{\sigma}.
    \end{align*}
    
    Since $\pit \in H$, where $H$ is the subgroup of $S_{2k}$ whose elements fix all of $j_1, ..., j_{\ell}$, then $(\TT \otimes \SS)^{\pit} = \TT^{\pit} \otimes \SS$. With this, the innermost sum becomes
    
    \begin{align*}
        \sum_{\pit \in H} \text{sgn}({\pit}) (\TT \otimes \SS)^{\pit} 
        = \sum_{\pit \in H} \text{\sgn}({\pit}) \TT^{\pit} \otimes \SS
        = \Big(\sum_{\pit \in H} \text{sgn}({\pit}) \TT^{\pit}\Big) \otimes \SS.
    \end{align*}
    
    Now define $\pi \in S_{k}$ by $\pi = \tau^{-1} \pit \tau$, where $\tau = (i_1, ..., i_{k})$ (i.e. $\tau(i) = j_i$). Then the above is
    
    \begin{align*}
        \Big(\sum_{\pi S_{k}} \text{sgn}({\pi}) \TT^{\pi}\Big) \otimes \SS 
        = \alt(\TT) \otimes \SS 
        = \mathbf{0} \otimes \SS
        = \mathbf{0}.
    \end{align*}
    
    The last equality follows by the seeming-multilinearity of $\otimes$. 
\end{proof}

\begin{theorem}
\label{ch::antisymmetry::thm::wedge_associativity}
    (Wedge product is associative). 
    
    Let $V$ be a vector space. For all $\TT, \SS, \RR \in V^{\otimes k}$, we have $(\TT \wedge \SS) \wedge \RR = \TT \wedge (\SS \wedge \RR)$, and are therefore justified in denoting both as $(\TT \wedge \SS) \wedge \RR = \TT \wedge (\SS \wedge \RR) := \TT \wedge \SS \wedge \RR$.
\end{theorem}

\begin{proof}
    We will show $(\TT \wedge \SS) \wedge \RR = \alt(\TT \otimes \SS \otimes \RR)$; a similar argument shows $\TT \wedge (\SS \wedge \RR) = \alt(\TT \otimes \SS \otimes \RR)$.
    
    First, we have by definition of $\wedge$ that 
    
    \begin{align*}
        (\TT \wedge \SS) \wedge \RR = \alt((\TT \wedge \SS) \otimes \RR).
    \end{align*}
    
    Subtracting $\alt(\TT \otimes \SS \otimes \RR)$ from both sides and using linearity of $\alt$, we get that
    
    \begin{align*}
        (\TT \wedge \SS) \wedge \RR - \alt(\TT \otimes \SS \otimes \RR) = \alt((\TT \wedge \SS - \TT \otimes \SS) \otimes \RR) = (\TT \wedge \SS - \TT \otimes \SS) \wedge \RR.
    \end{align*}
    
    If we show $(\TT \wedge \SS - \TT \otimes \SS) \wedge \RR = \mathbf{0}$, then our claim is true. By the previous lemma, it suffices to show that $\alt(\TT \wedge \SS - \TT \otimes \SS) = \mathbf{0}$, since if this is true then $(\TT \wedge \SS - \TT \otimes \SS) \wedge \RR = \mathbf{0}$. And this the case: $\alt(\TT \wedge \SS - \TT \otimes \SS) = \alt(\TT \wedge \SS) - \alt(\TT \otimes \SS) = \TT \wedge \SS - \TT \wedge \SS = \mathbf{0}$, by linearity of $\alt$ and with use of the fact that $\TT \wedge \SS$ is antisymmetric.
\end{proof}

\begin{theorem}
    $\TT \wedge \SS := \alt(\TT \otimes \SS)$ satisfies the desired properties of the wedge product
\end{theorem}

\begin{proof}
    \textbf{TO-DO: review this}

   Property (1) follows by checking the definition $\TT \wedge \SS := \alt(\TT \otimes \SS)$. Property (2) was proved in Theorem \ref{ch::antisymmetry::thm::wedge_associativity}. Property (3) follows from the definition of $\alt$. Conditions (3) and (4) are logically equivalent, and conditions (3) and (5) are logically equivalent. 
\end{proof}

\section{Pushforward and pullback}

[segway]

\begin{defn}
\label{ch::antisymmetry::defn::pushforward_pullback}
    (Pushforwards and pullbacks).
    
    Let $V$ and $W$ be finite-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$ and its dual ${\ff^*:W^* \rightarrow V^*}$. We define the following \textit{pushforward} and \textit{pullback} maps on ${T_{k,0}(V) = V^{\otimes k}}$ and ${T_{0,k}(W) = (W^*)^{\otimes k}}$, respectively, extending each with the seeming-multilinearity of $\otimes$:
    
    \begin{align*}
        \vv_1 \otimes ... \otimes \vv_k \in T_{k,0}(V) &\overset{\otimes_{k, 0} \ff}{\longmapsto} \ff(\vv_1) \otimes ... \otimes \ff(\vv_k) \in T_{k,0}(W) \quad \text{(``pushforward on $T_{k,0}(V)$'')} \\
        \psi_1 \otimes ... \otimes \psi_k \in T_{0,k}(W) &\overset{\otimes_{0, k} \ff^*}{\longmapsto} \ff^*(\psi_1) \otimes ... \otimes \ff^*(\psi_k) \in T_{0,k}(V) \quad \text{(``pullback on $T_{0,k}(W)$'')}.
    \end{align*}
    
    We also define the following \textit{pushforward} and \textit{pullback} maps on $\Lambda^k(V) = V^{\wedge k}$ and $\Lambda^k(W^*) = (W^*)^{\wedge k}$, respectively, extending each with the seeming-multilinearity and antisymmetry of $\wedge$, for $k \leq n$:
    
    \begin{align*}
        \vv_1 \wedge ... \wedge \vv_k \in \Lambda^k(V) &\overset{\Lambda^k \ff}{\longmapsto} \ff(\vv_1) \wedge ... \wedge \ff(\vv_k) \in \Lambda^k(W) \quad \text{(``pushforward on $\Lambda^k(V)$'')} \\
        \psi^1 \wedge ... \wedge \psi^k \in \Lambda^k(W^*) &\overset{\Lambda^k \ff^*}{\longmapsto} \ff^*(\psi^1) \wedge ... \wedge \ff^*(\psi^k) \in \Lambda^k(V^*) \quad \text{(``pullback on $\Lambda^k(W^*)$'')}.
    \end{align*}
\end{defn}

\begin{remark}
\label{ch::antisymmetry::rmk::star_notation_pushforward_pullback}
    (Star notation for pushforward and pullback).
    
    The above notation of using $\otimes_{0, k} \ff:T_{k,0}(V) \rightarrow T_{k,0}(W)$ and $\Lambda^k \ff:\Lambda^k(V) \rightarrow \Lambda^k(W)$ for the pushforwards and using $\otimes_{k, 0} \ff^*$ and $\Lambda^k \ff^*$ for the pullbacks is nonstandard. It is more common to denote the pushforwards by ${\ff_*:T_{k,0}(V) \rightarrow T_{k,0}(W)}$ and ${\ff_*:\Lambda^k(V) \rightarrow \Lambda^k(W)}$ and the pullbacks by ${\ff^*:T_{0,k}(W) \rightarrow T_{0,k}(V)}$ and ${\ff^*:\Lambda^k(W) \rightarrow \Lambda^k(V)}$.
    
    Once the above definitions of pushforward and pullback are understood, this ``star notation'' can be useful. But it can be difficult to understand what the various pushforwards and pullbacks are if this notation is used from the start, due to the potential for confusing the dual $\ff^*:W^* \rightarrow V^*$ with either of the pullbacks ${\ff^*:T_{0,k}(W) \rightarrow T_{0,k}(V)}, {\ff^*:\Lambda^k(W) \rightarrow \Lambda^k(V)}$.
    
    We will use the star notation after we define a pullback of a differential form in Chapter \ref{ch::diff_forms}. Until then, we do not use the star notation.
\end{remark}

\section{Exterior powers as vector spaces of functions}

%\begin{defn}
%    (Contraction between dual exterior powers).
    
%    Let $V$ be an $n$-dimensional vector space over a field $K$, and consider the $k$th exterior power $\Lambda^k(V)$, as well as the $k$th exterior power $\Lambda^k(V^*$). Recall from Definition \ref{ch::bilinear_forms_metric_tensors::defn::tensor_contraction} that there is\footnote{In the referenced definition, we use $C$, rather than $C'$, to denote the bilinear form on $V$ and $V^*$. We have chosen to use $C'$ here for the bilinear form on $V$ and $V^*$ so that we can use $C$ to denote the bilinear function $\Lambda^k(V) \times \Lambda^k(V^*) \rightarrow K$ we describe.} a natural bilinear form $C'$ on $V$ and $V^*$ defined by $C'(\vv, \phi) = \phi(\vv)$. Using the bilinear form $C'$ on $V$ and $V^*$, we can produce a bilinear function $C:\Lambda^k(V) \times \Lambda^k(V^*) \rightarrow K$ defined on elementary wedges, and extended with antisymmetry and seeming-multilinearity, by

%    \begin{align*}
%        C(\vv_{\sigma(1)} \wedge ... \wedge \vv_{\sigma(k)}, \phi_{\pi(1)} \wedge ... \wedge \phi_{\pi(k)}) &:= \sgn(\pi \circ \sigma) C'(\vv_1, \phi_1) ... C'(\vv_k, \phi_k) \\
%        &= \sgn(\pi \circ \sigma) \phi_1(\vv_1) ... \phi_k(\vv_k).
%    \end{align*}
%\end{defn}

%\begin{remark}
%    Consider the title of the previous definition. In the previous definition, we describe a function $C:\Lambda^k(V) \times \Lambda^k(V^*) \rightarrow K$. Notice that because there is a natural isomorphism $\Lambda^k(V^*) \cong \Lambda^k(V)^*$ (recall Theorem \ref{ch::antisymmetry::thm::fundamental_isos_exterior_pwrs}), the function $C$ does induce a function $\widetilde{C}:\Lambda^k(V) \times \Lambda^k(V)^* \rightarrow K$, which is truly a contraction between ``dual exterior powers''. (In practice, we don't use $\widetilde{C}$, because we prefer to keep $\Lambda^k(V^*)$ ``as it is'').
%\end{remark}

When we first encountered $(p, q)$ tensors, we learned that $(p, q)$ tensors can be identified with multilinear functions; we saw that $T_{p,q}(V) \cong \LLLL((V^*)^{\times p} \times V^{\times q} \rightarrow K)$ is a natural isomorphism when $V$ is a finite-dimensional vector space. When $V_1, ..., V_k$ are finite-dimensional vector spaces, we similarly\footnote{$V_1^* \otimes ... \otimes V_k^* \cong (V_1 \otimes ... \otimes V_k)^* = \LLLL(V_1 \otimes ... \otimes V_k \rightarrow K) \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)$.} have the natural isomorphism ${V_1^* \otimes ... \otimes V_k^* \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)}$.

Wedge product spaces and exterior powers can also be interpreted as vector spaces of functions. We will need to interpret exterior powers as spaces of functions in the setting of differential forms. Our two goals in this subsection are to (1) present that a $k$-wedge of covectors in $V^*$ can act on $k$ vectors from $V$ to produce a scalar and to (2) give a new presentation of the pullback of a $k$-wedge of covectors.

To achieve our goals, we first formalize some notation about the alternative interpretations of tensor product spaces, $(p, q)$ tensors, wedge product spaces, and exterior powers.

\begin{deriv}
    (Tensor product that operates on tensors that are treated as functions).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces, and consider that $V_1^* \otimes ... \otimes V_k^* \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)$ naturally. Observe that the involvement of $\otimes$ in $V_1^* \otimes ... \otimes V_k^*$ induces a binary operation $\totimes$ on $\LLLL(V_1 \times ... \times V_k \rightarrow K)$. We construct $\totimes$ by explicitly constructing a natural isomorphism $V_1^* \otimes ... \otimes V_k^*  \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)$, and giving new notation to the output of this isomorphism. 
    
    The natural isomorphism $V_1^* \otimes ... \otimes V_k^* \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)$ is defined on the elementary tensor $\phi^1 \otimes ... \otimes \phi^k \in V_1^* \otimes ... \otimes V_k^*$, and extended with the seeming-multilinearity of $\otimes$ and the corresponding actual multilinearity of the newly-defined $\totimes$. The isomorphism sends
    
    %It is most illuminating to construct the isomorphism $V_1 \otimes ... \otimes V_k  \cong \LLLL(V_1 \times ... \times V_k \rightarrow K)$ by defining an isomorphism $T_{0,k}(V) \cong \LLLL((V^*)^{\times k} \rightarrow K)$. Once we have this isomorphism involving $k$, then we automatically have the full isomorphism involving $p, q$, since the identification $V \cong V^{**}$ provides a way to turn the isomorphism $T_{0,k}(V) \cong \LLLL((V^*)^{\times k} \rightarrow K)$ into an isomorphism $T_{k,0}(V) \cong \LLLL(V^{\times k} \rightarrow K)$; we can ``piece together'' the two isomorphisms involving $k$ to obtain the full isomorphism involving $p, q$. We construct the isomorphism this way to emphasize the action of elements of $V^*$ on elements of $V^*$ and the action of elements of $V \cong V^{**}$ on elements of $V^*$.
    
    \begin{align*}
        \phi^1 \otimes ... \otimes \phi^k \mapsto \phi^1 \totimes ... \totimes \phi^k,
    \end{align*}
    
    where $\phi^1 \totimes ... \totimes \phi^k:V^{\times k} \rightarrow K$ is the multilinear function defined by
    
    \begin{align*}
        (\phi^1 \totimes ... \totimes \phi^k)(\vv_1, ..., \vv_k) := \phi^1(\vv_1) ... \phi^k(\vv_k).
    \end{align*}
    
    Note, we have essentially reused the idea of the natural isomorphism from the third bullet point of Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos} (this isomorphism is discussed in the proof of the referenced theorem).
\end{deriv}

\begin{defn}
    (Tensor product spaces as vector spaces of functions).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces over a field $K$. We define $V_1^* \totimes ... \totimes V_k^*$ to be the vector space spanned by elements of the form $\phi^1 \totimes ... \totimes \phi^k$, where $\phi^i \in V_i^*$. Thus, 
    
    \begin{align*}
        V_1^* \totimes ... \totimes V_k^* = \LLLL(V_1 \times ... \times V_k \rightarrow K).
    \end{align*}
    
    In analogy to the above, we also define $V_1 \totimes ... \totimes V_k := \LLLL(V_1^* \times ... \times V_k^* \rightarrow K)$.
\end{defn}

\begin{theorem}
    (Dual distributes over $\totimes$).
    
    Let $V$ and $W$ be finite-dimensional vector spaces. Then we have the natural isomorphism
    
    \begin{align*}
        (V \totimes W)^* \cong
        V^* \totimes W^*
    \end{align*}
\end{theorem}

\begin{proof}
    By taking the dual of each side, we see that an equivalent fact is $(V^* \totimes W^*)^* \cong V \totimes W$. This is indeed the case, as we have $(V^* \totimes W^*)^* = \LLLL(V \times W \rightarrow K)^* \cong \LLLL(V \otimes W \rightarrow K)^* = (V \otimes W)^{**} \cong (V^* \otimes W^*)^* = \LLLL(V^* \otimes W^* \rightarrow K) \cong \LLLL(V^* \times W^* \rightarrow K) \cong V \totimes W$.
\end{proof}

\begin{theorem}
    (Natural isomorphisms relating $\otimes$ and $\totimes$).
     
    For finite-dimensional vector spaces $V$ and $W$ over a field $K$, we have $V^* \totimes W^* \cong V^* \otimes W^*$ naturally. It follows from taking the dual of each side of this natural isomorphism and applying that $V \totimes W \cong V \otimes W$ naturally. Induction then gives that $V^{\totimes k} \cong V^{\otimes k}$.
\end{theorem}

\begin{proof}
   We prove the first statement: we have $V^* \totimes W^* = \LLLL(V \times W \rightarrow K) \cong \LLLL(V \otimes W \rightarrow K) = (V \otimes W)^* \cong V^* \otimes W^*$.
\end{proof}

\begin{defn}
    ($(p, q)$ tensors as functions).
    
    We define $\tT_{p,q}(V) := (V^*)^{\totimes p} \totimes V^{\totimes q}$. Note that, due to the previous theorem, there is a natural isomorphism $T_{p,q}(V) \cong \tT_{p,q}(V)$.
\end{defn}

\begin{deriv}
    (Alternization of elements of $V_1 \totimes ... \totimes V_k$).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces, and consider the wedge product space $V_1^* \wedge ... \wedge V_k^*$. Due to the existence of the $\alt$ function on $V_1^* \wedge ... \wedge V_k^*$, the isomorphism $V_1^* \otimes ... \otimes V_k^* \cong V_1^* \totimes ... \totimes V_k^*$ induces a function $\talt:V_1^* \totimes ... \totimes V_k^* \rightarrow V_1^* \totimes ... \totimes V_k^*$:
    
    \begin{align*}
        \talt(\TT) = \frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) \TT^\sigma,
    \end{align*}
    
    where $(\cdot)^\sigma$ is a permutation on elements of $V_1^* \totimes ... \totimes V_k^*$ induced by a permutation on elements of $V_1^* \totimes ... \totimes V_k^*$. To be extra clear, $(\cdot)^\sigma:V_1^* \totimes ... \totimes V_k^* \rightarrow V_1^* \totimes ... \totimes V_k^*$ is defined on elementary ``tensors'', and extended with multilinearity, by $(\phi^1 \totimes ... \totimes \phi^k)^\sigma = \phi^{\sigma(1)} \totimes ... \totimes \phi^{\sigma(k)}$.
\end{deriv}

\begin{deriv}
    (Wedge product that operates on tensors that are treated as functions).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces. The wedge product ${\wedge:(V_1^* \otimes ... \otimes V_k^*)^{\times 2} \rightarrow \alt((V_1^* \otimes ... \otimes V_k^*)^{\otimes 2})}$ induces a wedge product ${\twedge: (V_1 \totimes ... \totimes V_k^*)^{\times 2} \rightarrow \talt((V_1^* \totimes ... \totimes V_k^*)^{\totimes 2})}$ defined by $\TT \twedge \SS = \talt(\TT \totimes \SS)$. The wedge product $\twedge$ is an multilinear alternating function because $\wedge$ is antisymmetric and appears multilinear.
\end{deriv}

\begin{defn}
    (Wedge product spaces as vector spaces of functions).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces over a field $K$. We define $V_1^* \twedge ... \twedge V_k^*$ to be the vector space spanned by elements of the form $\phi^1 \twedge ... \twedge \phi^k$, where $\phi^i \in V_i^*$. Thus,
    
    \begin{align*}
        V_1^* \twedge ... \twedge V_k^* = (\alt \LLLL)(V_1 \times ... \times V_k \rightarrow K).
    \end{align*}
    
    We also define $V_1 \twedge ... \twedge V_k := \LLLL(V_1^* \times ... \times V_k^* \rightarrow K)$.
\end{defn}

\begin{theorem}
    (Natural isomorphisms relating $\wedge$ and $\twedge$).
     
    For finite-dimensional vector spaces $V$ and $W$, we have $V^* \twedge W^* \cong V^* \wedge W^*$ naturally. It follows from taking the dual of each side of this natural isomorphism that $V \twedge W \cong V \wedge W$ naturally. Induction then gives that $V^{\twedge k} \cong V^{\wedge k}$.
\end{theorem}

\begin{proof}
    To prove the first statement, send elementary tensors to elementary tensors via the linear function $\phi \twedge \psi \mapsto \phi \wedge \psi$, and extend this function with linearity and antisymmetry. The rest follows easily.
\end{proof}

\begin{defn}
    (Exterior powers as vector spaces of functions).
    
    Let $V$ be a finite-dimensional vector space over a field $K$. Because the previous definition shows \\ ${\Lambda^k(V^*) = (V^*)^{\wedge k} \cong (V^*)^{\twedge k}}$ naturally, we define $\tLambda^k(V^*) := (V^*)^{\twedge k} = (\alt \LLLL)(V^{\times k} \rightarrow K)$.
\end{defn}

%\begin{defn}
%    (Action of an element of $\Lambda^k(V^*)$ on an element of $\Lambda^k(V)$).
    
%    Let $V$ be an $n$-dimensional vector space over a field $K$. Due to the existence of the contraction between dual exterior powers, an element of $\Lambda^k(V^*)$ can be thought of having a natural action on elements of $\Lambda^k(V)$. To formalize this, we will define the following notation for $\phi^1 \wedge ... \wedge \phi^k \in \Lambda^k(V^*)$.
    
%    \begin{align*}
%        (\phi^{\pi(1)} \wedge ... \wedge \phi^{\pi(k)})(\vv_{\sigma(1)}, ..., \vv_{\sigma(k)}) &:= C(\vv_{\sigma(1)} \wedge ... \wedge \vv_{\sigma(k)}, \phi_{\pi(1)} \wedge ... \wedge \phi_{\pi(k)}) \\
%        &= \sgn(\pi \circ \sigma) C'(\vv_1, \phi_1) ... C'(\vv_k, \phi_k) \\
%        &= \sgn(\pi \circ \sigma) \phi_1(\vv_1) ... \phi_k(\vv_k).
%    \end{align*}
%\end{defn}

We have now laid out the landscape for the interpretation of tensor product spaces and wedge product spaces as vector spaces of functions. In doing so, we described explicitly how $\totimes$ acts on multilinear functions to produce a multilinear function. To complete this section, we show how $\twedge$ acts on multilinear alternating functions to produce an multilinear alternating function, and present the pullback of elements of exterior powers in the context where exterior powers are thought of as vector spaces of functions.

\begin{lemma}
\label{ch::antisymmetry::lemma::pushforward_on_dual}

    (Pushforward on the dual is multiplication by $\det(\phi^i(\ee_j))$).
    
    Let $V$ be an $n$-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the induced dual basis for $V^*$. Since the pushforward on the top exterior power is multiplication by the determinant (recall Theorem \ref{ch::antisymmetry::rmk::top_pushforward_det}), then we have
    
    \begin{align*}
        \phi^1 \wedge ... \wedge \phi^n = \det(\gg) \phi^{\ee_1} \wedge ... \wedge \phi^{\ee_n}
        \text{ for all  $\phi^1, ..., \phi^n \in V^*$}
    \end{align*}

    where $\gg:V^* \rightarrow V^*$ is the linear function that is defined on basis vectors by $\gg(\phi^{\ee_i})) = \phi^i$. The matrix $[\ff(E^*)]_{E^*}$ of $\ff$ relative to $E^*$ and $E^*$ is $\begin{pmatrix} [\phi^1]_{E^*} & \hdots & [\phi^1]_{E^*} \end{pmatrix}$, so we have
    
    \begin{align*}
        \phi^1 \wedge ... \wedge \phi^n &= \det \begin{pmatrix} [\phi^1]_{E^*} & \hdots & [\phi^1]_{E^*} \end{pmatrix} \phi^{\ee_1} \wedge ... \wedge \phi^{\ee_n} \\
        &= \det(\phi^i(\ee_j)) \phi^{\ee_1} \wedge ... \wedge \phi^{\ee_n} \text{ for all $\phi^1, ..., \phi^n \in V^*$}.
    \end{align*}
    
    To obtain the last expression, recall the fact that for any $\phi \in V^*$ and basis $F^*$ of $V^*$ induced by a basis $F = \{\ff_1, ..., \ff_n\}$ of $V$, we have $([\phi]_{F^*})_i = \phi(\ff_i)$ (see Theorem \ref{ch::bilinear_forms_metric_tensors::thm::coords_vector_dual_vector}).
\end{lemma}

\begin{lemma}
\label{ch::antisymmetry::lemma::action_dual_wedge_dual_basis}
    (Action of dual $n$-wedge on dual basis).
    
    Let $V$ be an $n$-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the induced dual basis for $V^*$. Consider an element $\phi^1 \twedge ... \twedge \phi^n \in \tLambda^n(V^*)$. We have
    
    \begin{align*}
        (\phi^{\ee_1} \twedge ... \twedge \phi^{\ee_n})(\ee_1, ..., \ee_n) = 1.
    \end{align*} 
\end{lemma}

\begin{proof}
    Using a similar argument to the one that showed the permutation formula for the determinant on the $\phi^{\ee_i}$, we have
    
    \begin{align*}
        \phi^{\ee_1} \twedge ... \twedge \phi^{\ee_n} = \talt(\phi^{\ee_1} \twedge ... \totimes \phi^{\ee_n}) 
        = \sum_{\sigma \in S_n} \sgn(\sigma) \phi^{\ee_{\sigma(1)}} \totimes ... \totimes \phi^{\ee_{\sigma(n)}}.
    \end{align*}
            
    Therefore 
    
    \begin{align*}
         (\phi^{\ee_1} \twedge ... \twedge \phi^{\ee_n})(\ee_1, ..., \ee_n) 
         = \Big( \sum_{\sigma \in S_n} \sgn(\sigma) \phi^{\ee_{\sigma(1)}} \totimes ... \totimes \phi^{\ee_{\sigma(n)}} \Big) (\ee_1, ..., \ee_n) 
         = \sum_{\sigma \in S_n} \Big( \sgn(\sigma) ( \phi^{\ee_{\sigma(1)}} \totimes ... \totimes \phi^{\ee_{\sigma(n)}})(\ee_1, ..., \ee_n) \Big). 
    \end{align*}
    
    Now we focus on the inner term, $(\phi^{\ee_{\sigma(1)}} \totimes ... \totimes \phi^{\ee_{\sigma(n)}})(\ee_1, ..., \ee_n)$. By definition of $\totimes$, we have
            
    \begin{align*}
        (\phi^{\ee_{\sigma(1)}} \totimes ... \totimes \phi^{\ee_{\sigma(n)}})(\ee_1, ..., \ee_n) 
        =
        \phi^{\ee_{\sigma(1)}}(\ee_1) ... \phi^{\ee_{\sigma(n)}}(\ee_n) 
    \end{align*}
            
    Since $\epsilon^{\sigma(i)}(\ee_j) = \delta^{\sigma(i)}{}_j$, the only permutation $\sigma \in S_n$ for which the above expression is nonzero is the identity permutation $i$; when $\sigma = i$, the above is $1$. Thus, we have
    
    \begin{align*}
        (\phi^{\ee_1} \twedge ... \twedge \phi^{\ee_n})(\ee_1, ..., \ee_n) = \sgn(i) \cdot 1 = 1 \cdot 1 = 1. 
    \end{align*}
\end{proof}

\begin{theorem}
\label{ch::antisymmetry::thm::action_dual_k_wedge_on_vectors}
    (Action of dual $n$-wedge on vectors).
    
    Let $V$ be an $n$-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the induced dual basis for $V^*$. Consider an element $\phi^1 \twedge ... \twedge \phi^n \in \tLambda^n(V^*)$. We have
    
    \begin{align*}
        (\phi^1 \twedge ... \twedge \phi^n)(\ee_1, ..., \ee_n) = \det(\phi^i(\ee_j)).
    \end{align*}
    
    By extending with antisymmetry and multilinearity, we have
    
    \begin{align*}
        \boxed
        {
            (\phi^1 \twedge ... \twedge \phi^n)(\vv_1, ..., \vv_n) = \det(\phi^i(\vv_j)) \text{ for all $\vv_1, ..., \vv_n \in V$}
        }
    \end{align*}
    
    The action of $\phi^1 \twedge ... \twedge \phi^n$ on $\vv_1, ..., \vv_n$ can be interpreted as follows. First notice that the above is nonzero only when $\vv_1, ..., \vv_n$ are linearly independent, i.e., when $E := \{\vv_1, ..., \vv_n\}$ is a basis\footnote{Quick proof that $(\text{linearly independent}) \implies (\text{basis})$ in this scenario. If $E := \{\vv_1, ..., \vv_n\}$ is not a basis for $V$, then $E$ is a linearly independent set of $n$ vectors that doesn't span $V$, so we need to add more linearly independent vectors to $E$ in order to obtain a basis; that is, $\dim(V) > n$, which is a contradiction.}. Thus, when $\vv_1, ..., \vv_n$ are linearly independent, we can interpret $\phi^i(\vv_j)$ in the style of Lemma \ref{ch::antisymmetry::lemma::pushforward_on_dual} as $\phi^i(\vv_j) = ([\phi^i]_{E^*})^j$, where ${E^* := \{\phi^{\vv_1}, ..., \phi^{\vv_n}\}}$ is the dual basis for $V^*$ induced by the basis $E$ for $V$:
    
    \begin{align*}
        (\phi^1 \twedge ... \twedge \phi^n)(\vv_1, ..., \vv_n) = \det \begin{pmatrix} [\phi^1]_{E^*} & \hdots & [\phi^1]_{E^*} \end{pmatrix} \text{ when $\vv_1, ..., \vv_n$ are linearly independent}.
    \end{align*}
\end{theorem}

\begin{proof}
   The first of the previous two lemmas implies $\phi^1 \twedge ... \twedge \phi^n = \det(\ff^*) \phi^{\ee_1} \twedge ... \twedge \phi^{\ee_n}$. (We have just put $\sim$'s over the $\wedge$'s of the first lemma). Combine this with the second of the previous two lemmas.
\end{proof}

So, we have shown how a $k$-wedge of elements from $V^*$, when treated as a function, can act on vectors from $V$. Lastly, we present the pushforward and pullback of elements of exterior powers when exterior powers are interpreted to be spaces of functions. (We are interested in the pullback, but present the pushforward for completeness).

\subsubsection{Pushforward and pullback on vector spaces of functions}

\begin{deriv}
    (Pushforward on $\tT_{k,0}(V)$)
    
    Let $V$ and $W$ be $n$-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$. Definition \ref{ch::antisymmetry::defn::pushforward_pullback} defined the pushforward $\otimes_{k, 0} \ff: T_{k,0}(V) \rightarrow T_{k,0}(W)$ by
    
    \begin{align*}
        \vv_1 \otimes ... \otimes \vv_k \overset{\totimes_{k,0} \ff}{\longmapsto} \ff(\vv_1) \otimes ... \otimes \ff(\vv_k).
    \end{align*}
    
    The natural isomorphisms $T_{k,0}(V) \cong \tT_{k,0}(V)$ and $T_{k,0}(W) \cong \tT_{k,0}(W)$ induce a pushforward function ${\totimes_{k,0} \ff:\tT_{k,0}(V) \rightarrow \tT_{k,0}(W)}$ defined by
    
    \begin{align*}
        \vv_1 \totimes ... \totimes \vv_k \overset{\totimes_{k,0} \ff}{\longmapsto} \ff(\vv_1) \totimes ... \totimes \ff(\vv_k).
    \end{align*}
    
    Notice that by definition of $\totimes$, we have $ \ff(\vv_1) \totimes ... \totimes \ff(\vv_k) = (\ff^{\totimes k})(\vv_1, ..., \vv_i)$. Therefore
    
    \begin{align*}
        \vv_1 \totimes ... \totimes \vv_k \overset{\totimes_{k,0} \ff}{\longmapsto} \ff^{\totimes k}(\vv_1, ..., \vv_k).
    \end{align*}
\end{deriv}

\begin{deriv}
\label{ch::antisymmetry::deriv::pullback_on_tT0k}
    (Pullback on $\tT_{0,k}(W)$).
    
    Let $V$ and $W$ be $n$-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$. Definition \ref{ch::antisymmetry::defn::pushforward_pullback} defined the pullback $\otimes_{k, 0} \ff^*: T_{0,k}(W) \rightarrow T_{0,k}(V)$ by
    
    \begin{align*}
        \psi^1 \totimes ... \totimes \psi^k \overset{\Lambda^k \ff^*}{\longmapsto} \ff^*(\psi^1) \totimes ... \totimes \ff^*(\psi^k).
    \end{align*}
    
    The natural isomorphisms $T_{0,k}(W) \cong \tT_{0,k}(W)$ and $T_{0,k}(V) \cong \tT_{0,k}(V)$ induce a pullback function ${\totimes_{0,k} \ff^*:\tT_{0,k}(W) \rightarrow \tT_{0,k}(V)}$ defined by
    
    \begin{align*}
        \psi^1 \totimes ... \totimes \psi^k \overset{\totimes_{0,k} \ff^*}{\longmapsto} \ff^*(\psi^1) \totimes ... \totimes \ff^*(\psi^k) = (\psi^1 \circ \ff) \totimes ... \totimes (\psi^k \circ \ff)
    \end{align*}
    
    It can be checked using the definition of $\totimes$ that $(\psi^1 \circ \ff) \totimes ... \totimes (\psi^k \circ \ff) = (\psi^1 \totimes ... \totimes \psi^k) \circ \ff$. Therefore
    
    \begin{align*}
        \psi^1 \totimes ... \totimes \psi^k \overset{\totimes_{0,k} \ff^*}{\longmapsto}
        (\psi^1 \totimes ... \totimes \psi^k) \circ \ff.
    \end{align*}
    
    The above is a statement on the elementary tensor $\psi^1 \totimes ... \totimes \psi^k$. Extending this statement using the multilinearity of $\totimes$, we have
    
    \begin{align*}
        \TT \in \tT_{0,k}(W) \overset{\totimes_{0,k} \ff^*}{\longmapsto} \TT \circ \ff \in \tT_{0,k}(V) \text{ for all $\TT \in \tT_{0,k}(W)$}.
    \end{align*}
    
    Since $\TT \circ \ff = \ff^*(\TT)$, this is equivalent to
    
    \begin{align*}
        \TT \in \tT_{0,k}(W) \overset{\totimes_{0,k} \ff^*}{\longmapsto} \ff^*(\TT) \in \tT_{0,k}(V) \text{ for all $\TT \in \tT_{0,k}(W)$}.
    \end{align*}
    
    Since elements of $\tT_{0,k}(V)$ act on $k$ vectors from $V$, we now ask, how does $\otimes_{0, k} \ff^*$ act on $k$ vectors from $V$? Well,
    
    \begin{align*}
        \otimes_{0, k} \ff^*(\vv_1, ..., \vv_k) = \ff^*(\TT)(\vv_1, ..., \vv_k) = (\TT \circ \ff)(\vv_1, ..., \vv_k) = \TT(\ff(\vv_1), ..., \ff(\vv_k)).
    \end{align*}
    
    Thus $\otimes_{0, k} \ff^*$ acts on $k$ vectors from $V$ by
    
    \begin{align*}
        \boxed
        {
            \otimes_{0, k} \ff^*(\vv_1, ..., \vv_k) = \TT(\ff(\vv_1), ..., \ff(\vv_k))
        }
    \end{align*}
\end{deriv}

\begin{deriv}
    (Pushforward on $\tLambda^k(V)$).
    
    Let $V$ and $W$ be $n$-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$. Definition \ref{ch::antisymmetry::defn::pushforward_pullback} defined the pushforward $\Lambda^k \ff:\Lambda^k(V) \rightarrow \Lambda^k(W)$ by
    
    \begin{align*}
        \vv_1 \wedge ... \wedge \vv_k \overset{\Lambda^k \ff}{\longmapsto} \ff(\vv_1) \wedge ... \wedge \ff(\vv_k).
    \end{align*}
    
    Equivalently,
    
    \begin{align*}
        \alt(\vv_1 \otimes ... \otimes \vv_k) \overset{\Lambda^k \ff}{\longmapsto} \alt(\otimes_{k, 0} \ff(\vv_1 \otimes ... \otimes 
        \vv_k)).
    \end{align*}
    
    Since the pushforward $\otimes_{k, 0}:T_{k,0}(V) \rightarrow T_{k,0}(W)$ induces the pushforward $\totimes_{k,0}:\tT_{k,0}(V) \rightarrow \tT_{k,0}(W)$, then we obtain a pushforward $\tLambda^k \ff:\tLambda^k(V) \rightarrow \tLambda^k(W)$ defined by
    
    \begin{align*}
        \vv_1 \twedge ... \twedge \vv_k = \talt(\vv_1 \totimes ... \totimes \vv_k) \overset{\tLambda^k \ff}{\longmapsto} \talt(\totimes_{k,0} \ff(\vv_1 \totimes ... \totimes \vv_k)) = \ff^{\twedge k}(\vv_1, ..., \vv_k).
    \end{align*}
    
    That is,
    
    \begin{align*}
        \vv_1 \twedge ... \twedge \vv_k \overset{\tLambda^k \ff}{\longmapsto} \ff^{\twedge k}(\vv_1, ..., \vv_k).
    \end{align*}
\end{deriv}

\begin{deriv}
\label{ch::antisymmetry::deriv::pullback_on_exterior_pwr_of_actual_fns}
    (Pullback on $\tLambda^k(W^*)$).
    
    Let $V$ and $W$ be $n$-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$. Definition \ref{ch::antisymmetry::defn::pushforward_pullback} defined the pullback $\Lambda^k \ff^*:\Lambda^k(W^*) \rightarrow \Lambda^k(V^*)$ by
    
    \begin{align*}
        \psi^1 \wedge ... \wedge \psi^k \overset{\Lambda^k \ff^*}{\longmapsto} \ff^*(\psi^1) \wedge ... \wedge \ff^*(\psi^k).
    \end{align*}
    
    Equivalently,
    
    \begin{align*}
        \alt(\psi^1 \otimes ... \otimes \psi^k) \overset{\Lambda^k \ff^*}{\longmapsto} \alt(\otimes_{0, k} \ff^*(\psi^1 \otimes ... \otimes \psi^k)).
    \end{align*}
    
    Since the pullback $\otimes_{0, k} \ff^*:T_{0,k}(W) \rightarrow T_{0,k}(V)$ induces the pullback $\totimes_{0,k} \ff^*:\tT_{0,k}(W) \rightarrow \tT_{0,k}(V)$, then we obtain a pullback $\tLambda^k \ff^*:\tLambda^k(W^*) \rightarrow \tLambda^k(V^*)$ defined by
    
    \begin{align*}
        \psi^1 \twedge ... \twedge \psi^k = \talt(\psi^1 \totimes ... \totimes \psi^k) \overset{\tLambda^k \ff^*}{\longmapsto}
        &\talt(\totimes_{0,k} \ff^*(\psi^1 \totimes ... \totimes \psi^k)) \\ 
        &= \talt((\psi^1 \totimes ... \totimes \psi^k) \circ \ff) = \talt(\psi^1 \totimes ... \totimes \psi^k) \circ \ff
        = (\psi^1 \twedge ... \twedge \psi^k) \circ \ff.
    \end{align*}
    
    (The equality between the rightmost expression of the first line and the leftmost expression of the second line uses the fact that $\totimes \ff^*(\TT) = \ff^*(\TT)$, which is presented in Derivation \ref{ch::antisymmetry::deriv::pullback_on_tT0k}. The validity of the second equals sign in the second line has not been proven, but it is quickly checked).
    
    Overall, the above line reads
    
    \begin{align*}
        \psi^1 \twedge ... \twedge \psi^k \overset{\tLambda^k \ff^*}{\longmapsto} (\psi^1 \twedge ... \twedge \psi^k) \circ \ff.
    \end{align*}
    
    Extending with the multilinearity and alternatingness of $\twedge$, we extend this statement to
    
    \begin{align*}
        \TT \in \Lambda^k(W^*) \overset{\tLambda^k \ff^*}{\longmapsto} \TT \circ \ff = \ff^*(\TT) \in \Lambda^k(V^*).
    \end{align*}
    
    Therefore, if $\TT \in \tLambda^k(W^*)$, then $(\tLambda^k \ff^*)(\TT) \in \tLambda^k(V^*)$ acts on $k$ vectors from $V$ by
    
    \begin{align*}
        \boxed
        {
            \Big((\tLambda^k \ff^*)(\TT)\Big)(\vv_1, ..., \vv_k) = \TT(\ff(\vv_1), ..., \ff(\vv_k))
        }
    \end{align*}
    
    In appearance, it seems that $(\tLambda^k \ff^*)(\TT)$ acts on $\vv_1, ..., \vv_k$ exactly as does $\totimes_{0,k} \ff^*$, since $\totimes_{0,k} \ff^*(\vv_1, ..., \vv_k) = \TT(\ff(\vv_1), ..., \ff(\vv_k))$. The distinction between the two definitions is that $\tLambda^k \ff^*$ acts on multilinear alternating functions $\TT$, while $\totimes_{0,k} \ff^*$ acts on multilinear (not necessarily alternating) functions $\TT$.
\end{deriv}

\newpage

\section{Wedge products and the determinant}

Given that the determinant is a multilinear alternating function, and the wedge product looks like a multilinear alternating function, there must be some fundamental connection between the two concepts. We present that connection now.

\begin{deriv}
\label{ch::antisymmetry::rmk::top_pushforward_det}
    (The pushforward on the top exterior power is multiplication by the determinant).
    
    Let $V$ and $W$ be $n$-dimensional vector spaces over a field $K$, let $\ff:V \rightarrow W$ be a linear function, and consider $\Lambda^n(V)$.
    
    We follow an argument similar to that given in the proof of Lemma \ref{ch::antisymmetry::lemma:det_as_scale_factor}\footnote{Unfortunately, we can't directly use the result of said lemma to avoid writing what is a bit of a redundant derivation because doing so requires machinery we yet have.}. Let $E = \{\ee_1, ..., \ee_n\}$ be a basis of $V$, and consider
    
    \begin{align*}
        \ff(\ee_1) \wedge ... \wedge \ff(\ee_n).
    \end{align*}
    
    Because $\Lambda^n(\ff)$ is multilinear and alternating, the wedge product is analogous to the determinant $\det(\ff(\ee_1), ..., \ff(\ee_n)) = \det([\ff(E)]_F) = \det(\ff)$. So, set $(a^i_j) = [\ff(E)]_F$, and then use essentially the same argument as was made to derive the permutation formula on the left side of the above. We obtain
    
    \begin{align*}
        \ff(\ee_1) \wedge ... \wedge 
        \ff(\ee_n) 
        &= \sum_{\sigma \in S_n} \Big( a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) \ee_1 \wedge ... \wedge \ee_n \Big)
        = \Big( \sum_{\sigma \in S_n} a^1_{\sigma(1)} ... a^n_{\sigma(n)} \sgn(\sigma) \Big) \ee_1 \wedge ... \wedge \ee_n \\
        &= \det([\ff(E)]_F) (\ee_1 \wedge ... \wedge \ee_n)
        = \det(\ff) (\ee_1 \wedge ... \wedge \ee_n).
    \end{align*}
    
   So, we have the statement on the basis $E$
   
   \begin{align*}
        \ff(\ee_1) \wedge ... \wedge 
        \ff(\ee_n) = \det(\ff) (\ee_1 \wedge ... \wedge \ee_n).
   \end{align*}
   
   Using the seeming-multilinearity of $\wedge$, we can extend this fact to apply to any list of vectors in $V$:
   
   \begin{align*}
       \boxed
       {
        \ff(\vv_1) \wedge ... \wedge \ff(\vv_n) = \det(\ff) ( \vv_1 \wedge ... \wedge \vv_n ) \text{ for all $\vv_1, ..., \vv_n \in V$}
       }
   \end{align*}
   
   We can explicitly involve the pullback $\Lambda^n \ff$ and write the above as
   
   \begin{align*}
       (\Lambda^n \ff)(\vv_1 \wedge ... \wedge \vv_n) = \det(\ff) ( \vv_1 \wedge ... \wedge \vv_n ) \text{ for all $\vv_1, ..., \vv_n \in V$}.
   \end{align*}
   
   Thus, $\Lambda^n(\ff)$ is multiplication by $\det(\ff)$.
\end{deriv}

\begin{theorem}
\label{ch::antisymmetry::rmk::top_pullback_det}

    (The pullback on the top exterior power is multiplication by the determinant).
    
    Let $V$ and $W$ be $n$-dimensional vector spaces, and consider a linear function $\ff:V \rightarrow W$. Consider additionally the dual $\ff^*:W^* \rightarrow V^*$ (recall Definition \ref{ch::appendix::defn::dual_transf_after_id}). Then, applying the previous theorem and using that $\det(\ff) = \det(\ff^*)$, we see that the pullback $\Lambda^n \ff^*$ on the top exterior power satisfies
    
    \begin{align*}
        (\Lambda^n \ff^*)(\ww_1 \wedge ... \wedge \ww_n) = \det(\ff) ( \ww_1 \wedge ... \wedge \ww_n ) \text{ for all $\ww_1, ..., \ww_n \in V$}.
    \end{align*}
    
    That is,
    
    \begin{align*}
        \boxed
        {
            \ff^*(\ww_1) \wedge ... \wedge \ff^*(\ww_n) = \det(\ff)(\ww_1 \wedge ... \wedge \ww_n)
        }
    \end{align*}
\end{theorem}

\section{Wedge products and orientation}

We now show how the top exterior power of a vector space can be used to describe orientation. 

\begin{theorem}
\label{ch::antisymmetry::defn::orientation_with_top_degree_wedges}
    (Orientation with top degree wedges).
    
    Let $V$ be a finite-dimensional vector space with an orientation $E = (\ee_1, ..., \ee_n)$. All positively oriented bases of $V$ are scalar multiples of $E$, and all negatively oriented bases of $V$ are scalar multiples of $-E$, where $-E = E^\sigma$ for some $\sigma$ with $\sgn(\sigma) < 0$.
    
    Notice, we can identify $E = (\ee_1, ..., \ee_n)$ with $\ee_1 \wedge ... \wedge \ee_n \in \Lambda^n(V)$, because the antisymmetry of orthonormal ordered bases is manifested in elements of $\Lambda^n(V)$ due to the antisymmetry of $\wedge$. Once one has noticed this, it is a natural next step to check that the union of the sets of positively oriented and negatively oriented ordered bases, when considered under the operations of ``basis addition'' and multiplication by a scalar, is a vector space that is isomorphic to $\Lambda^n(V)$.
    
    Therefore, another way to give an orientation to a finite-dimensional vector space is to choose an element of $\Lambda^n(V)$. The fact that the pushforward on the top exterior power is multiplication by the determinant (recall Theorem \ref{ch::antisymmetry::rmk::top_pushforward_det}) plays nicely into this interpretation: once an orientation $\ee_1 \wedge ... \wedge \ee_n$ of $V$ has been chosen, then we have $\ff_1 \wedge ... \wedge \ff_n = \det(\ff) \ee_1 \wedge ... \wedge \ee_n$, where $\ff$ is the linear function $V \rightarrow V$ sending $\ee_i \mapsto \ff_i$.
\end{theorem}

\section{The cross product}
\label{ch::antisymmetry::section::cross_product}

At the end of Chapter \ref{ch::lin_alg}, we presented an explanation for the dot product that remedies two common problematic ways to explain the dot product. The cross product also comes with two common pedagogical problems. We describe and remedy those here.

The first pedagogical problem with the cross product is that the complicated algebraic formula for the cross product is rarely explained. The formula is $\vv \times \ww = (v^2 w^3 - w^3 v^2) \see_1 - (v^1 w^3 - v^3 w^1) \see_2`+ (v^1 w^2 - v^2 w^1) \see_3$, where $v^i := [\vv]_\sE^i$ and $w^i := [\ww]_\sE^i$.

The second problem is that the ``right hand rule'' is never explicitly formalized. One common ``explanation'' for the right hand rule goes as follows: ``you can use a `left hand rule' if you want to, but then you'll have to account for a minus sign''. This is a true statement, but it only relates the ``right hand rule'' with the ``left hand rule''- it does not explain the fundamental reason why a right hand rule or left hand rule would emerge in the first place. The ``right hand rule'' is really a consequence of conventions about orientation.

\begin{comment}
\begin{lemma}
\label{ch::lin_alg::thm::dot_prod_cancelable}
    (The algebraic dot product on $\R^n$ is positive definite, and therefore cancelable).
    
    The algebraic dot product on $\R^n$ is \textit{positive definite}; that is, it satisfies the property ${(\vv \cdot \vv = 0 \iff \vv = \mathbf{0})}$.
    
    As a consequence, we have the fact that, when $\vv \in \R^n$ is nonzero and $\vv_1, \vv_2 \in \R^n$, then ${((\vv_1 \cdot \vv = \vv_2 \cdot \vv) \implies \vv_1 = \vv_2)}$.
\end{lemma}

\begin{proof}
   We first show that the dot product on $\R^n$ satisfies the property $(\vv \cdot \vv = 0 \iff \vv = \mathbf{0})$. The reverse implication follows immediately. For the forward implication, let $\vv \in \R^n$, and suppose $\vv \cdot \vv = 0$; we must show $\vv = \mathbf{0}$. We have $\vv \cdot \vv = \sum_{i = 1}^n ([\vv]_\sE)_i^2$. Each term in the sum is a nonnegative number. Therefore, the sum is only zero if all terms in the sum are zero, so $([\vv]_\sE)_i = 0$ for each $i$, which means $\vv = \mathbf{0}$.
   
   Now we show that when $\vv \in \R^n$ is nonzero and $\vv_1, \vv_2 \in \R^n$, then $((\vv_1 \cdot \vv = \vv_2 \cdot \vv) \implies \vv_1 = \vv_2)$. If $\vv_1 \cdot \vv = \vv_2 \cdot \vv$, then $(\vv_1 - \vv_2) \cdot \vv = \mathbf{0}$ by the bilinearity of $\cdot$. Since $\vv \neq \mathbf{0}$, then we must have $\vv_1 - \vv_2 = \mathbf{0}$ due to the positive definiteness of $\cdot$. That is, $\vv_1 = \vv_2$.
\end{proof}
\end{comment}

\begin{deriv}
    (Cross product). 
    
    In Section \ref{ch::lin_alg::section::dot_product}, we presented the dot product by starting with an intuitive geometric definition of the dot product and building onto that definition. If we were to define the cross product in the same manner, we would start with this geometric definition:
    
    \vspace{.25cm}
    
    \begin{addmargin}[3em]{2em}
        The cross product of $\vv_1, \vv_2 \in \R^3$ is the vector $(\vv_1 \times \vv_2) \in \R^3$ such that

        \begin{itemize}
            \item $||\vv_1 \times \vv_2||$ is the area of the parallelogram spanned by $\vv_1$ and $\vv_2$
            \item $\widehat{\vv_1 \times \vv_2}$ is the vector perpendicular to $\vv_1$ and $\vv_2$ such that $\{\vv_1, \vv_2, \vv_1 \times \vv_2\}$ is positively oriented (this condition is equivalent to the ``right hand rule'')
        \end{itemize}
    \end{addmargin}
    
    Starting with the above definition is better than most non-explanations of the cross product, but is still problematic, as doing so begs the question, ``Why is it natural for some vector to satisfy these two conditions?''. A better way to introduce the cross product is to stumble across a vector that satisfies these two conditions, and then define that vector to be $\vv_1 \times \vv_2$. This is what we will do.

    Consider the linear function $\ff_{\vv_1,\vv_2}:\R^3 \rightarrow \R$ defined by $\ff_{\vv_1,\vv_2}(\vv) = \det(\vv_1, \vv_2, \vv)$. Since ${\ff_{\vv_1,\vv_2}:\R^3 \rightarrow \R}$, then $\ff_{\vv_1,\vv_2}$ must be represented by a $1 \times 3$ matrix: $\ff_{\vv_1, \vv_2}(\vv) = \vv^\top \vv$ for some $\cc \in \R^3$. We define the \textit{cross product of $\vv_1$ and $\vv_2$} to be this $\cc$. That is, we define the cross product of $\vv_1, \vv_2 \in \R^3$ to be the unique vector $(\vv_1 \times \vv_2) \in \R^3$ such that
    
    \begin{align*}
        (\vv_1 \times \vv_2) \cdot \vv = \det(\vv_1, \vv_2, \vv) \text{ for all $\vv \in \R^3$}.
    \end{align*}
\end{deriv}

\begin{remark}
    (The cross product is unique).

    We have not justified that the above definition of the cross product defines a unique vector. We quickly do so now.
\end{remark}

\begin{proof}
    Suppose for contradiction that there exist $\cc, \dd$ with $\cc \neq \dd$ such that $\cc \cdot \vv = \det(\vv_1, \vv_2, \vv)$ for all $\vv \in \R^3$ and $\dd \cdot \vv = \det(\vv_1, \vv_2, \vv)$ for all $\vv \in \R^3$. Subtracting these equations, we have $(\cc - \dd) \cdot \vv = 0$. Since the dot product [is positive definite], we must have $\cc - \dd = \mathbf{0}$, i.e., $\cc = \dd$, which contradicts $\cc \neq \dd$.
\end{proof}

We now prove that $\vv_1 \times \vv_2$ satisfies the previously mentioned geometric properties.

\begin{theorem}
    (Magnitude, direction of cross product).
    
    The cross product $\vv_1 \times \vv_2$ of  $\vv_1, \vv_2 \in \R^3$ is such that:
    
    \begin{itemize}
        \item $||\vv_1 \times \vv_2||$ is the area of the parallelogram spanned by $\vv_1$ and $\vv_2$
        \item $\widehat{\vv_1 \times \vv_2}$ is the vector perpendicular to $\vv_1$ and $\vv_2$ such that $\{\vv_1, \vv_2, \vv_1 \times \vv_2\}$ is positively oriented
    \end{itemize}
\end{theorem}

\begin{proof}
    \hspace{0mm} \\
    \begin{itemize}
        \item We have $(\vv_1 \times \vv_2) \cdot \vv = ||\proj(\vv \rightarrow \vv_1 \times \vv_2)|| \spc ||\vv_1 \times \vv_2||$. By the definition of the cross product, we have $||\proj(\vv \rightarrow \vv_1 \times \vv_2)|| \spc ||\vv_1 \times \vv_2|| = \det(\vv_1, \vv_2, \vv)$, so $||\vv_1 \times \vv_2|| = \frac{\det(\vv_1, \vv_2, \vv)}{||\proj(\vv \rightarrow \vv_1 \times \vv_2)||}$. This is equal to the volume of the parallelapiped spanned by $\vv_1, \vv_2$ and $\vv$ divided by the height of the same parallelapiped, which is the same as the area of the base of the parallelapiped, i.e., the area of the parallelogram spanned by $\vv_1$ and $\vv_2$.
        \item Consider $\vv_i$, $i \in \{1, 2\}$. We have $(\vv_1 \times \vv_2) \cdot \vv_i = \det(\vv_1, \vv_2, \vv_i)$. Since $\vv_i \in \{\vv_1, \vv_2\}$, then two vectors in the determinant are the same. This implies the determinant is zero.
    \end{itemize}
\end{proof}

\subsection*{The below is in-progress}

\begin{defn}
    (Signed counterclockwise angle).
    
    $\theta_{\hat{\nn}}(\vv_1, \vv_2)$ for full formal notation, where $\hat{\nn}$ is one of the two vectors perpendicular to $\vv_1$, $\vv_2$; $\theta(\vv_1, \vv_2)$ for more colloquial
    
    we have $\theta_{\hat{\nn}}(\vv_1, \vv_2) = 2\pi - \theta_{-\hat{\nn}}(\vv_1, \vv_2)$
\end{defn}

\begin{theorem}
    (Geometric formula for magnitude of the cross product).
    
    The magnitude of the cross product $\vv_1 \times \vv_2$ of $\vv_1, \vv_2 \in \R^3$ is
    
    \begin{align*}
        ||\vv_1 \times \vv_2 || = ||\vv_1|| \spc ||\vv_2|| \spc |\sin(\theta(\vv_1, \vv_2))|,
    \end{align*}
    
    where $\theta$ is the signed counterclockwise angle from $\vv_1$ to $\vv_2$.
\end{theorem}

\begin{proof}
   Basic trigonometry shows this result. The absolute value around $\sin(\theta(\vv_1, \vv_2))$ ensures the RHS is always nonnegative.
\end{proof}

\begin{deriv}
    (Right hand rule).
        
    Let $\vv_1, \vv_2 \in \R^3$, and consider the ordered basis $\{\vv_1, \vv_2, \hat{\nn}\}$, where $\hat{\nn}$ is either one of the two vectors perpendicular to $\vv_1$ and $\vv_2$. (So $\hat{\nn}$ is either $\widehat{\vv_1 \times \vv_2}$ or $-\widehat{\vv_1 \times \vv_2}$).
    
    Since $\{\vv_1, \vv_2, \vv_1 \times \vv_2\}$ is positively oriented, $\widehat{\vv_1 \times \vv_2} = \hat{\nn}$ iff $\{\vv_1, \vv_2, \hat{\nn}\}$ is positively oriented. And $\{\vv_1, \vv_2, \hat{\nn}\}$ is positively oriented iff $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (0, \pi)$.
    
    So $\widehat{\vv_1 \times \vv_2} = \hat{\nn}$ iff $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (0, \pi)$. By similar reasoning, $\widehat{\vv_1 \times \vv_2} = -\hat{\nn}$ iff $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (\pi, 2\pi)$.
    
    In all:
    
    \begin{itemize}
        \item $\widehat{\vv_1 \times \vv_2} = \hat{\nn}$ iff $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (0, \pi)$
        \item $\widehat{\vv_1 \times \vv_2} = -\hat{\nn}$ iff $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (\pi, 2\pi)$
    \end{itemize}
    
    Think of $\hat{\nn}$ as designating which side of the plane spanned by $\vv_1$ and $\vv_2$ is the ``top'' side. Then the above two bullet points tell us that the cross product $\vv_1 \times \vv_2$ points ``up'' (i.e. to the top side) when $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (0, \pi)$ and points ``down'' (i.e. to the bottom side) when $\theta_{\hat{\nn}}(\vv_1, \vv_2) \in (0, 2\pi)$.
    
    %The key idea of the right hand rule: \textit{the ``user'' can decide which of the two possible options $\hat{\nn}$ is.} That is, pick a side of the plane from which you want to view $\vv_1$ and $\vv_2$. (Since your line-of-sight vector is $-\hat{\nn}$, picking a side of the plane corresponds to choosing $\hat{\nn}$). Then make use of the above two bullet-points.
        
    %Remark: if one switches which side of the plane is designated as ``top'', then they have really just set $\hat{\nn}_{\text{after}} = - \hat{\nn}_{\text{before}}$. The right hand rule will still work because negating $\hat{\nn}$ corresponds to negating the counterclockwise signed angle; the new counterclockwise signed angle is $\theta_{\text{after}}(\vv_1, \vv_2) = 2 \pi - \theta_{\text{before}}(\vv_1, \vv_2)$.
\end{deriv}

\begin{theorem}
    (Cyclic pattern with the cross product).
    
     $\ee_{\sigma(1)} \times \ee_{\sigma(2)} = \sgn(\sigma) \ee_{\sigma(3)}$
     
     so $\see_1 \times \see_2 = \see_3$, $\see_2 \times \see_3 = \see_1$, $\see_3 \times \see_1 = \see_2$
\end{theorem}

\begin{deriv}
    (Cross product is Hodge dual of wedge product).
    
    $\Lambda^2(V) \cong V$ iff $\dim(\Lambda^2(V)) = \dim(V)$ iff $\binom{\dim(V)}{2} = \dim(V)$ iff $\dim(V) = 3$. In all, $\Lambda^2(V) \cong V$ iff $\dim(V) = 3$. This shows in particular that $\Lambda^2(\R^3)$ and $\R^3$ are isomorphic.
    
    One isomorphism $\perp:\Lambda^2(\R^3) \rightarrow \R^3$ is defined by $\perp(\see_{\sigma(1)} \wedge \see_{\sigma(2)}) = \sgn(\sigma) \see_{\sigma(3)}$. We call $\perp$ the \textit{Hodge dual}.
    
    
    \begin{align*}
        \perp(\vv_1 \wedge \vv_2)
        = \perp\Big( \sum_{i = 1}^n \Big( ([\vv_1]_\sE)_i \see_i \Big) \wedge \sum_{i = 1}^n \Big( ([\vv_2]_\sE)_i \see_i \Big) \Big)
        = ...
        = \vv_1 \times \vv_2
    \end{align*}
\end{deriv}

The previous theorem motivates us to ask if $\perp(\vv_1 \wedge ... \wedge \vv_{n - 1})$ satisfies a condition that generalizes the defining property of the cross product. The following theorem provides the answer(``yes!'').

\begin{theorem}
    (Hodge dual generalizes cross product).

    Let $\vv_1, ..., \vv_{n - 1} \in \R^n$. Then $\perp(\vv_1 \wedge ... \wedge \vv_{n - 1}) \in \R^n$ is the unique vector satisfying
    
     \begin{align*}
         \perp(\vv_1 \wedge ... \wedge \vv_{n - 1}) \cdot \vv = \det(\vv_1, ..., \vv_{n - 1}, \vv) \text{ for all $\vv \in \R^n$}.
     \end{align*}
\end{theorem}

\begin{proof}
        Note that \textit{if} the condition is satisfied, then uniqueness easily follows, since the function $\vv \mapsto \det(\vv_1, ..., \vv_{n - 1}, \vv)$, being a linear function, has a unique matrix.
        
        We now show that the condition is satisfied. $(\vv_1 \wedge ... \wedge \vv_{n - 1}) \mapsto \perp(\vv_1 \wedge ... \wedge \vv_{n - 1}) \cdot \vv$ and $(\vv_1 \wedge ... \wedge \vv_{n - 1}) \mapsto \det(\vv_1, ..., \vv_{n - 1}, \vv)$ are both multilinear alternating functions, so they must be scalar multiples of each other\footnote{In Section \ref{ch::antisymmetry::determinant}, we proved that the determinant is the unique alternating linear $k$-form function sending $\sE \mapsto 1$. An arbitrary alternating linear $k$-form $f:(K^n)^{\times n} \rightarrow K$ is such that $\frac{1}{c}f$ satisfies $(\frac{1}{c}f)(\see_1, ..., \see_n) = 1$, so such an $f$ is $f = c \det$.}. Thus, to show the condition, it suffices to show that the two functions agree on a particular input. We show $\perp(\see_1, ..., \see_{n - 1}) \cdot \vv = \det(\see_1, ..., \see_{n - 1}, \vv)$ for all $\vv \in \R^n$. This condition is logically equivalent to the condition ``$\see_n \cdot \vv = \det(\see_1, ..., \see_{n - 1}, \vv)$ for all $\vv \in \R^n$'', which is in turn logically equivalent to ``$\see_n \cdot \vv = \det(\see_1, ..., \see_{n - 1}, \vv)$ for all $\vv \in \sE$, where $\sE$ is the standard basis for $\R^n$''.  When $\vv = \see_i \in \sE$, we have $\see_n \cdot \see_i = \delta^i{}_n = \det(\see_1, ..., \see_{n - 1}, \see_i)$, so this last condition (and thus all of them) are true.
\end{proof}

\begin{remark}
    (Interpretation of $k$-wedges as oriented volumes).
\end{remark}

We can actually define the Hodge dual in a more general context.

\begin{defn}
    (Hodge dual).
    
    Let $V$ be a finite-dimensional vector space with a metric tensor $g$. We define the \textit{Hodge dual (induced by $g$)} to be the function $\perp:\Lambda^k(V) \rightarrow \Lambda^{n - k}(V)$ defined by
    
    \begin{align*}
        \perp(\huu_{i_1} \wedge ... \wedge \huu_{i_k}) &:= \sgn(\sigma) \spc \huu_{i_{k + 1}} \wedge ... \wedge \huu_{i_{k + (n - k)}}, \\ &\text{ where } \{i_{k + 1}, ..., i_{k + (n - k)} \} = \{1, ..., n\} - \{i_1, ..., i_k\} \text{ and } \sigma = (i_1, ..., i_n).
    \end{align*}
    
    [Extending with multilinearity and alternatingness], we see that
    
    \begin{align*}
        \perp(\vv_{i_1} \wedge ... \wedge \vv_{i_k}) &= \sgn(\sigma) \det\Big((g(\vv_{i_k}, \vv_{j_\ell}))\Big) \spc \vv_{i_{k + 1}} \wedge ... \wedge \vv_{i_n}, \\ &\text{ where } \{i_{k + 1}, ..., i_n\} = \{1, ..., n\} - \{i_1, ..., i_k\} \text{ and } \sigma = (i_1, ..., i_n).
    \end{align*}
    
    In the above, $\det \Big( (\langle \vv_{i_k}, \vv_{j_\ell} \rangle) \Big)$ denotes the determinant of the matrix with $k\ell$ entry $\langle \vv_{i_k}, \vv_{j_\ell} \rangle$, and is called the \textit{Gram determinant}. Interestingly enough, the function $\widetilde{g}$ that sends $(\vv_{i_1} \wedge ... \vv_{i_k}, \ww_{i_1} \wedge ... \ww_{i_k})$ to the Gram determinant is a metric tensor on $\Lambda^k(V)$. We can use this new metric tensor to restate the above as
    
    \begin{align*}
        \perp \vv = \perp(\vv_{i_1} \wedge ... \wedge \vv_{i_k}) &= \sgn(\sigma) \widetilde{g}(\vv, \vv) \spc \vv_{i_{k + 1}} \wedge ... \wedge \vv_{i_n}, \\ &\text{ where } \{i_{k + 1}, ..., i_n\} = \{1, ..., n\} - \{i_1, ..., i_k\} \text{ and } \sigma = (i_1, ..., i_n).
    \end{align*}
    
    Most authors use $*$ to denote the Hodge dual, and the Hodge dual is often known as the ``Hodge star''. We've chosen not to use this notation so as to avoid confusion with the $*$ that appears when notating dual vector spaces.
\end{defn}

\begin{remark}
    The above definition of the Hodge dual depends on a basis, so we do not yet know that the Hodge dual is well-defined.
\end{remark}

\begin{theorem}
    (Double Hodge dual).
    
    Let $V$ be a finite-dimensional vector space with metric tensor $g$. The Hodge dual $\perp:\Lambda^k(V) \rightarrow \Lambda^{n - k}(V)$ satisfies $\perp \perp \vv = (-1)^{k(n - k)} \vv$.
\end{theorem}

\begin{proof}
    It suffices to prove the above when $\vv$ is an elementary wedge, $\vv = \vv_{i_1} \wedge ... \wedge \vv_{i_k}$. Using Definition ..., we have $\perp \vv = \sgn(\sigma) g(\vv, \vv) \vv_{i_{k + 1}} \wedge ... \wedge \vv_{i_n}$, where $\sigma = (i_1, ..., i_n)$. Applying $\perp$ again, we have $\perp \perp \vv = \sgn(\tau) \sgn(\sigma) \vv_{i_1} \wedge ... \wedge \vv_{i_k} = \sgn(\tau) \sgn(\sigma) \vv$, where $\tau = (i_{k + 1}, ..., i_n, i_1, ..., i_k)$. The key is to notice that $\sgn(\tau) = (-1)^{k(n - k + 1)} \sgn(\sigma)$, since, to obtain $\sigma$ from $\tau$, we need to move each of the $k$ indices $i_1, ..., i_k$ past the $n - k$ many indices $i_{k + 1}, ..., i_n$. Thus, we have $\perp \perp \vv = (-1)^{k(n - k + 1)} \sgn(\sigma)^2 \hvv = (-1)^{k(n - k + 1)} \hvv$.
\end{proof}

\begin{theorem}
    Let $V$ be an $n$-dimensional vector space with metric tensor $g$. For all $k \leq n$ is a metric tensor $\widetilde{g}$ on $\Lambda^k(V)$ defined on elementary wedges by     $\widetilde{g}(\vv_1 \wedge ... \wedge \vv_k, \ww_1 \wedge ... \wedge \ww_k) = \det(g(\vv_i, \ww_i))$.
\end{theorem}

\begin{theorem}
    Let $V$ be an $n$-dimensional vector space with metric tensor $g$. If $k \leq n$, then for all $\omega, \eta \in \Lambda^k(V)$ we have

    \begin{align*}
        \omega \wedge (\perp \eta) = \widetilde{g}(\omega, \eta) \omega_{\text{vol}},
    \end{align*}
    
    where $\omega_{\text{vol}}$ is the volume form on $V$ and $\widetilde{g}$ is the metric tensor on $\Lambda^k(V)$ from the previous definition.
    
    Therefore, the Hodge dual is basis-independent and thus well-defined.
\end{theorem}

