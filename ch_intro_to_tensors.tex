\chapter{Introduction to tensors}
\label{ch::motivated_intro}

In this chapter, we introduce the idea of a \textit{tensor}, since tensors underpin differential forms. There are two key ideas that we must formalize before we define what a tensor is.

One of the ideas is that of a ``multilinear element''. Recall that elements of vector spaces (vectors) can be thought of as ``linear elements''. Thus, we can say ``linear functions respect the decomposition of linear elements''. After defining the notion of \textit{multilinear function}, we will define the notion of ``multilinear elements''. Multilinear elements will be objects that (in a sense) are respected by multilinear functions. Formally, mutlilinear elements will be elements of \textit{tensor product spaces}. 

There are two main contributions of tensor product spaces to the overarching theory of tensors: tensor product spaces formalize the structure of how ``multilinear things'' behave, and they allow multilinear functions to be identified with linear functions. Tensor product spaces do not account for the entire theory of tensors, though, even though the name might make you think this. One more key idea, described below, is required.

The second key idea in the theory of tensors is to think of linear functions as vectors- that is, as elements of vector spaces. We achieve this by decomposing linear functions into linear combinations of simpler linear functions. Most introductory linear algebra classes approach this idea by proving the fact that the set of $m \times n$ matrices form a vector space. We take this idea and run with it to discover \textit{dual spaces}. Soon after, we utilize the two key ideas, (1) tensor product spaces and (2) dual spaces, to prove the theorem which underlies the definition of a \textit{$(p, q)$ tensor}.

\section{Multilinear functions and tensor product spaces}

\begin{defn}
    (Multilinear function).
    
    Let $V_1, ..., V_k, W$ be vector spaces over a field $K$. We say a function $\ff:V_1 \times ... \times V_k \rightarrow W$ is \textit{$k$-linear}, or \textit{multilinear}, iff for all $\vv_1 \in V_1, ..., \vv_i \in V_i, ..., \vv_k \in V_k$, the function $\ff_i:V_i \rightarrow W$ defined by $\ff_i(\vv_1, ..., \vv_i, ..., \vv_k) = \ff(\vv_1, ..., \vv_i, ..., \vv_k)$ is linear. In other words, $\ff$ is multilinear iff it is ``linear in each argument''. 
    
    Note that a $1$-linear function is simply a linear function. A $2$-linear function is called a \textit{bilinear function}.
\end{defn}

\begin{example}
    (Example of a multilinear function). 
    
    The dot product on $\R^n$ is a bilinear function on $\R^n \times \R^n$.
\end{example}

\begin{defn}
    (Vector space of multilinear functions).
    
    If $V_1, ..., V_k, W$ are vector spaces over a field $K$, then we use $\LLLL(V_1 \times ... \times V_k \rightarrow W)$ to denote the vector space over $K$ of $k$-linear functions $V_1 \times ... \times V_k \rightarrow W$ under the operations of function addition and function scaling.
    
    Notice, the case $k = 1$ is consistent with our definition in Theorem \ref{ch::lin_alg::thm::vector_space_linear_functions} of $\LLLL(V_1 \rightarrow W)$ as being the set of linear functions $V_1 \rightarrow W$.
\end{defn}

\begin{proof}
     The proof that $\LLLL(V_1 \times ... \times V_k \rightarrow W)$ is indeed a vector space is left as an exercise.
\end{proof}

We know from our study of linear algebra that the decomposition of vectors is respected by linear functions (see Remark \ref{ch::lin_alg::rmk::linear_means_vector}). Now, having just been introduced to the notion of a \textit{multilinear} function. Natural questions are then: ``What objects do multilinear functions respect? Could these be thought of as 'multilinear elements'?`` 

The following definition gives the answer and formalizes this notion. ``Multilinear elements'' will be called \textit{tensors}.

\newpage

\begin{defn}
\label{ch::motivated_intro::defn::tensor_product_space}
    \smallcite{book_SM}{307} (Tensor product space).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces over a field $K$. The \textit{tensor product space} $V_1 \otimes ... \otimes V_k$ is defined to be the vector space over $K$ whose elements are from the set $V_1 \times ... \times V_k$, where the elements are also subject to an equivalence relation $=$, which will be specified soon. We also denote a typical element of $V_1 \otimes ... \otimes V_k$ as $\vv_1 \otimes ... \otimes \vv_k$, rather than as $(\vv_1, ..., \vv_k)$. 
    
    The equivalence relation is defined by the condition that
    
    \begin{align*}
        \vv_1 \otimes ... \otimes \vv_{i - 1} &\otimes \vv_{i} \otimes \vv_{i + 1} ... \otimes \vv_k \\
        &+ \\
        \vv_1 \otimes ... \otimes \vv_{i - 1} &\otimes \vv_{j} \otimes \vv_{i + 1} ... \otimes \vv_k \\
        &= \\
        \vv_1 \otimes ... \otimes \vv_{i - 1} \otimes (\vv_{i} &+ \vv_{j}) \otimes \vv_{i + 1} ... \otimes \vv_k
    \end{align*}

    for all $\vv_1, ..., \vv_k \in V$, and
    
    \begin{align*}
        c (\vv_1 \otimes ... &\otimes \vv_i \otimes ... \otimes \vv_k ) \\
        &= \\
        \vv_1 \otimes ... &\otimes (c \vv_i) \otimes ... \otimes \vv_k
    \end{align*}

    for all $\vv_1, ..., \vv_k \in V$ and $c \in K$. This is so that $\otimes$ looks like it is a multilinear function.
    
    Elements of tensor product spaces are called ``tensors''.
\end{defn}

% \begin{remark}
%     (Tensor terminology). 
    
%     Some authors use the word ``tensor'' to mean ``$(p, q)$ tensor''. (We have not defined $(p, q)$ tensors yet, but we will in Definition \ref{ch::motivated_intro::defn::pq_tensor}). We will use the word ``tensor'' to either mean an element of a tensor product space or a $(p, q)$ tensor, but we only do this when the meaning is clear from context.
% \end{remark}

We will soon see that tensors truly are multilinear elements, as their decompositions are respected by multilinear functions, but first we need to address some basics.

\begin{defn}
    (Elementary tensor). 
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces, and consider the tensor product space $V_1 \otimes ... \otimes V_k$. An element of $V_1 \otimes ... \otimes V_k$ that is of the form $\vv_1 \otimes ... \otimes \vv_k$ is called an \textit{elementary tensor}. Intuitively, an elementary tensor is a tensor that does not need to be expressed as linear combination of two or more other nonzero tensors. An element of $V_1 \otimes ... \otimes V_k$ that is not an elementary tensor is called a \textit{nonelementary tensor}.
\end{defn}

\begin{remark}
    (Extending with linearity).

    Let $V_1, ..., V_k, W$ be finite-dimensional vector spaces. Suppose we define a function $\ff:V_1 \otimes ... \otimes V_k \rightarrow W$ on elementary tensors, and suppose we desire a linear function $\gg$ that agrees with $\ff$ on elementary tensors. There does indeed exist\footnote{For an idea on how to prove this, revisit Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}.} a unique such $\gg$.
    
    Because of this, we can say things such as ``we \textit{impose} that $\ff$ be linear'', or ``we \textit{extend} the definition of $\ff$ \textit{with linearity}'', to indicate the definition ``$\ff(\TT) := \gg(\TT)$ when $\TT$ is a nonelementary tensor''.
\end{remark}

Now we present the property of tensors that motivated their definition.

\begin{theorem}
\label{ch::lin_alg::thm::universal_prop_tensor_prod}
    (Universal property of the tensor product).

    Let $V_1, ..., V_k, W$ be vector spaces, and let $\ff:V_1 \times ... \times V_k \rightarrow W$ be a multilinear function. 
    
    There exists a linear function $\widetilde{\ff}:V_1 \otimes ... \otimes V_k \rightarrow W$ uniquely corresponding to $\ff$ such that $\ff(\vv_1, ..., \vv_k) = \widetilde{\ff}(\vv_1 \otimes ... \otimes \vv_k)$.

    Equivalently, there are linear functions $\widetilde{\ff}:V_1 \otimes ... \otimes V_k \rightarrow W$ and $\gg:V_1 \times ... \times V_k \rightarrow V_1 \otimes ... \otimes V_k$, with $\gg$ defined on elementary tuples as $\gg(\vv_1, ..., \vv_k) := \vv_1 \otimes ... \otimes \vv_k$, and with $\widetilde{\ff}$ uniquely corresponding to $\ff$, such that $\ff = \widetilde{\ff} \circ \gg$.
\end{theorem}

\begin{proof}
    We define $\widetilde{\ff}$ on elementary tensors by $\widetilde{\ff}(\vv_1 \otimes ... \otimes \vv_k) := \ff(\vv_1, ..., \vv_k)$ and extend with linearity.
\end{proof}

\begin{remark}
    (The sense in which multilinear functions respect multilinear elements).
        
   This theorem tells us how multilinear elements are respected by multilinear functions: if $\ff$ is a multilinear function and $\TT = \sum_{i_1, ..., i_k} T_{i_1 ... i_k} \vv_{i_1} \otimes ... \otimes \vv_{i_k}$ is a multilinear element (a tensor), then, since $\widetilde{\ff}$ is linear, it respects the decomposition of $\TT$. So, if we squint, we can imagine that the multilinear function $\ff$ is acting on $\TT$ and preserving its decomposition by means of the linear function $\widetilde{\ff}$.
\end{remark}

Using the previous theorem to define a natural linear isomorphism of vector spaces yields the following result.

\begin{theorem}
\label{ch::motivated_intro::thm::multilin_fns_iso_lin_fns}
    (Multilinear functions are naturally identified with linear functions on tensor product spaces).
    
    Let $V_1, ..., V_k, W$ be finite-dimensional vector spaces. Then the vector space of multilinear functions ${V_1 \times ... \times V_k \rightarrow W}$ is naturally isomorphic to the vector space of linear functions ${V_1 \otimes ... \otimes V_k \rightarrow W}$:
    
    \begin{align*}
        \LLLL(V_1 \times ... \times V_k \rightarrow W) \cong \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W).
    \end{align*}
\end{theorem}

\begin{proof}
    To construct a linear isomorphism $\FF:\LLLL(V_1 ... \times V_k \rightarrow W) \mapsto \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W)$, we define $\FF(\ff) = \widetilde{\ff}$, where $\widetilde{\ff}$ is from Theorem \ref{ch::lin_alg::thm::universal_prop_tensor_prod}. We need to show that $\FF$ is a linear bijection. 
    
    First, we find an explicit expression for $\widetilde{\ff}$. Since $\ff = \widetilde{\ff} \circ \gg$ and since $\gg$ has a minimal kernel, then $\gg$ is invertible and $\widetilde{\ff} = \ff \circ \gg^{-1}$.

    Now we can show that $\FF$ is linear. The map is linear because, given vector spaces $Y, Z, W$, the function $\circ$ which composes linear functions, ${\circ:\LLLL(Y \rightarrow Z) \times \LLLL(Z \rightarrow W) \rightarrow \LLLL(Y \rightarrow W)}$, is a bilinear map. (Check this fact for yourself. Other consequences of this are explored in Derivation \ref{ch::intro_to_tensors::deriv::compos_linear_map_with_contract}). 
    
    To show $\FF$ is bijective, since it is a map between vector spaces of the same dimension, it suffices to show that it is one-to-one, i.e., that it has a minimal kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}). This is clearly the case because if $\FF(\ff) = \widetilde{\ff} = \mathbf{0}$, then $\ff = \widetilde{\ff} \circ \gg = \mathbf{0} \circ \gg = \mathbf{0}$.
\end{proof}

[segway. we've now learned the key facts we've set out to learn about tensors, but there are still some basic/crucial things to learn]

\begin{theorem}    \label{ch::motivated_intro::thm::tensor_product_associative}
    (Associativity of the tensor product). 
    
    Let $V_1, V_2, V_3$ be finite-dimensional vector spaces. Then there are natural linear isomorphisms
    
    \begin{align*}
        (V_1 \otimes V_2) \otimes V_3 \cong V_1 \otimes V_2 \otimes V_3 \cong V_1 \otimes (V_2 \otimes V_3).
    \end{align*}
    
    That is, these spaces are ``the same'', since an element of one can ``naturally'' be identified as an element of the other. (See Definition \ref{ch::lin_alg::defn::linear_iso} for a discussion of linear isomorphisms). These identifications are ``natural'' in the sense that they do not depend on a choice of basis (see Definition \ref{ch::lin_alg::defn::natural_iso}).

    Because of this isomorphism, we will always assume $\vv_1 \otimes (\vv_2 \otimes \vv_3) = \vv_1 \otimes \vv_2 \otimes \vv_3 = (\vv_1 \otimes \vv_2) \otimes \vv_3$ for all $\vv_1 \in V_1, \vv_2 \in V_2, \vv_3 \in V_3$.
\end{theorem}

\begin{proof}
    We construct a linear isomorphism $\FF:V_1 \otimes V_2 \otimes V_3 \rightarrow (V_1 \otimes V_2) \otimes V_3 \rightarrow V_3$; construction of a linear isomorphism $V_1 \otimes V_2 \otimes V_3 \rightarrow V_1 \otimes (V_2 \otimes V_3)$ is similar.

    We define $\FF$ on elementary tensors by $\vv_1 \otimes \vv_2 \otimes \vv_3 \overset{\FF}{\mapsto} (\vv_1 \otimes \vv_2) \otimes \vv_3$ and extend with linearity.
    
    $\FF$ is onto, as, since any ${\SS \in (V_1 \otimes V_2) \otimes V_3}$ is a linear combination of elementary tensors, $\SS = \sum_{i, j, k} T_{ijk} (\vv_i \otimes \vv_j) \otimes \vv_k$, then if $\TT := \sum_{i,j,k} T_{ijk} \vv_i \otimes \vv_j \otimes \vv_k$ we have $\FF(\TT) = \SS$.

    %\footnote{We could use dimensional justifications to prove one-to-oneness for this theorem about tensors, but an analogous approach will not work for the wedge product, since, when studying the wedge product, associatvitiy has to come before dimension. So, we will use an approach here that is easily adapted to the wedge product.}

    To show $\FF$ is one-to-one, it suffices to show that it has a left inverse (recall Theorem \ref{ch::logic_pf_fns::thm::one_to_one_iff_left_inverse}). We define a left inverse $\LL:(V_1 \otimes V_2) \otimes V_3 \rightarrow V_1 \otimes V_2 \otimes V_3$ on elementary tensors by $(\vv_1 \otimes \vv_2) \otimes \vv_3 \overset{\LL}{\mapsto} \vv_1 \otimes \vv_2 \otimes \vv_3$ and extend with linearity. This $\LL$ is indeed a left inverse on elementary tensors: $\LL(\FF(\vv_1 \otimes \vv_2 \otimes \vv_3) = \LL((\vv_1 \otimes \vv_2) \otimes \vv_3) = \vv_1 \otimes \vv_2 \otimes \vv_3$. It easily follows that it is a left inverse on nonelementary tensors.
\end{proof}

\begin{remark}
    (Noncommutativity of the tensor product).

    It is easy to show that if $V_1$ and $V_2$ are finite-dimensional vector spaces, then there is a natural linear isomorphism $V_1 \otimes V_2 \cong V_2 \otimes V_1$. Even though this is the case, we will \textit{not} assume $\vv_1 \otimes \vv_2 = \vv_2 \otimes \vv_1$ for all $\vv_1 \in V_1, \vv_2 \in V_2$. If we did assume this, then it would follow that $\vv_{i_1} \otimes ... \otimes \vv_{i_k}$ is the same tensor for all $i_1, ..., i_k \in \{1, ..., n\}$, and from this, it would follow that the universal property of the tensor product only applies to \textit{symmetric} multilinear functions\footnote{A multilinear function $\ff:V_1 \times ... \times V_k \rightarrow W$ is \textit{symmetric} iff $\ff(\vv_{i_1}, ..., \vv_{i_k})$ is the same for all $i_1, ..., i_k \in \{1, ..., n\}$.}, which destroys the usefulness of the multilinear element notion.

    We've justified the choice to use the natural linear isomorphism from the previous theorem, and the choice to ignore the one from this remark. The overall process that deems what natural linear isomorphisms should be used, though, seems vague and unformalized.
\end{remark}

\begin{theorem}
\label{ch::motivated_intro::thm::basis_dim_tensor_product_space}
    (Basis and dimension of a tensor product space). 
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces with bases $E_1, ..., E_k$, respectively, where $E_i = \{\ee_{i1}, ..., \ee_{in_i}\}$, and where $\dim(V_i) = n_i$. Then $V_1 \otimes ... \otimes V_k$ is a $n_1 n_2 ... n_k$ dimensional vector space with basis
    
    \begin{align*}
        \bigcup_{i = 1}^k \bigotimes_{\vv \in E_i} \vv = 
        \{ \ee_{1i_1} \otimes ... \otimes \ee_{ki_k} \mid i_k \in \{1, ..., n_k\} \}.
    \end{align*}
\end{theorem}

\begin{proof}      
      To show that this set spans $V_1 \otimes ... \otimes V_k$, it suffices to show it spans the set of elementary tensors in $V_1 \otimes ... \otimes V_k$, since any tensor in $V_1 \otimes ... \otimes V_k$ is a linear combination of elementary tensors. For an elementary tensor $\vv_1 \otimes ... \otimes \vv_k \in V_1 \otimes ... \otimes V_k$, we have $\vv_1 \otimes ... \otimes \vv_k = \sum_{i_1, ..., i_k} ([\vv_1]_{E_1})^{i_1} ... ([\vv_k]_{E_k})^{i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k}$ by the seeming-multilinearity of $\otimes$.
      
      To show linear independence, assume that $\sum_{i_1, ..., i_k} T_{i_1 ... i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k} = \mathbf{0}$. We must show that $T_{i_1 ... i_k} = 0$ for all $i_1, ..., i_k$. Note that a tensor in $V_1 \otimes ... \otimes V_k$ is the zero tensor iff\footnote{($\implies$). We have $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k = 0 \cdot (\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k) = \mathbf{0}$. ($\impliedby$). Suppose $\sum_{i_1, ..., i_k} T_{i_1 ... i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k}$ is the zero tensor. By the previous direction, $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k$ is also the zero tensor. So the two are equal.} it is an elementary tensor of the form $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k$ for some $i$. Due to the linear independence of the bases $E_1, ..., E_k$, it is impossible to obtain a $\mathbf{0}$ in any position of the tensor product unless all $T_{i_1 ... i_k}$ are $0$.
\end{proof}

\newpage

\section{Introduction to $(p, q)$ tensors}
\label{ch::motivated_intro::sec::motivated_intro}

Now we will derive the theorem which generalizes the two key notions (thinking of linear functions as vectors and ``multilinear elements'') discussed at the beginning of the chapter. Since we now have familiarity with the second key notion, then hopefully ``accidentally'' discovering how the first key notion is involved and formalizing it as we go is not too ambitious.

The theorem we will discover is that when $V$ and $W$ are finite-dimensional vector spaces, there is a natural linear isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$, where $V^*$ is the \textit{dual vector space} to $V$. We can see that the two key ideas (the first being thinking of linear functions as vectors and the second being ``multilinear elements'') are represented in this theorem with formal notation: the theorem includes a dual vector space $V^*$, which (we will see) indicates that thinking of linear functions as vectors is involved, and it also includes the tensor product $\otimes$, which indicates that multilinear structure is involved.

We first need a lemma.

\begin{lemma}
    (Standard basis matrices).

    Let $K$ be a field, and consider the standard bases $\sF = \{ \sff_1, ..., \sff_m\}$ and $\sE = \{\see_1, ..., \see_n\}$ for $K^m$ and $K^n$. The $m \times n$ matrix whose only nonzero entry is a $1$ in the $ith$ row and $j$th column is equal to $\sff_i \see_j^\top$. For example, if $m = 4$ and $n = 3$, we have

    \begin{align*}
        \sff_3 \see_2 = 
        \begin{pmatrix}
            0 \\
            0 \\
            1 \\
            0
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 & 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0
        \end{pmatrix}.
    \end{align*}
\end{lemma}

Now we begin our investigation.

\begin{deriv}
\label{ch::motivated_intro::deriv::decomposition_of_a_linear_function}
    (Decomposition of a linear function).

    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces over a field $K$, and consider the matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$.
    
    Let $a_{ij}$ be the $ij$ entry of $[\ff(E)]_F$. Making use of the lemma, we have

    \begin{align*}
        [\ff(E)]_F = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \sff_i \see_j^\top.
    \end{align*}

    Applying the linear isomorphism $\FF^{-1}$ between $m \times n$ matrices and linear functions $V \rightarrow W$ from Theorem \ref{ch::lin_alg::thm::linear_functions_matrices_isomorphism} to this equation causes  \begin{itemize}
        \item $[\ff(E)]_F$ to be replaced with $\ff$
        \item matrix multiplication to be replaced by function composition $\circ$
        \item $\see_j^\top$ to be replaced the element $\phi^{\see_j}$ of $\LLLL(V \rightarrow K)$ whose matrix relative to $E$ is $\see_j^\top$ (so $\phi^{\ee_j}(\vv) = \see_j^\top [\vv]_E$)
        \item $\sff_i$ to be replaced by the element $\tsff_i$ of $\LLLL(K \rightarrow W)$ whose standard matrix is $\sff_i$ (so $\tsff_i(c) = c\sff_i$)
    \end{itemize}

    So, we have

    \begin{align*}
        \ff = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \tsff_i \circ \phi^{\ee_i},
    \end{align*}

    where $\phi^{\ee_i} \in \LLLL(V \rightarrow K)$ and $\tsff_i \in \LLLL(K \rightarrow W)$.
\end{deriv}

\newpage

Since we have seen $\LLLL(V \rightarrow K)$ and $\LLLL(K \rightarrow W)$ have fundamental roles in the above decomposition, we make the following definitions.

\begin{defn}
\label{ch::motivated_intro::defn::dual_space_1}
    (Dual vector space).
    
    Let $V$ be a vector space over a field $K$. The \textit{dual vector space} $V^*$ to $V$ is the vector space $V^* := \LLLL(V \rightarrow K)$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::iso_V_to_linear_functions_K_V}
    (Natural identification of $W$ with $\LLLL(K \rightarrow W)$).

    Let $W$ be a vector space over a field $K$. We define a natural linear isomorphism $\sim:W \rightarrow \LLLL(K \rightarrow W)$ by $\sim(\ww) = (c \mapsto c\ww)$, and use the notation $\tww := \sim(\ww)$. That is, we have $\tww(c) = c\ww$.
\end{defn}

Using these definitions together with the result from the end of the above derivation yields the following theorem.

\begin{theorem}
\label{ch::motivated_intro::thm::decomposition_of_a_linear_function}
    (Decomposition of a linear function).
    
    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces. Any linear function $\ff:V \rightarrow W$ can be decomposed as
    
    \begin{align*}
        \ff = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \tww_i \circ \phi_j \text{ for some $a_{ij} \in K$},
    \end{align*}
    
    where $\phi_j \in V^*$, $\ww_i \in W$, and where we recall from the previous definition that $\tww_i(c) = c\ww_i$.
\end{theorem}

\begin{proof}
    See the previous derivation.
\end{proof}

One final ``big leap'' will complete our discovery. Recall, our original goal was to show ${\LLLL(V \rightarrow W) \cong W \otimes V^*}$. We've seen that dual spaces (i.e. $V^*$) have become involved. Tensor product spaces (i.e. $\otimes$) have not, yet, but will now.

\begin{deriv}
\label{ch::motivated_intro::deriv::linear_functions_V_W_iso_W_Vstar}
    ($\LLLL(V \rightarrow W) \cong W \otimes V^*$ naturally).
    
    We know from the previous theorem that $\ff \in \LLLL(V \rightarrow W)$ decomposes as a sum of linear functions of the form $\tww \circ \phi$, where $\phi \in V^*$ and $\ww \in W$. The idea is to take advantage of this decomposition and define a linear isomorphism $\FF:\LLLL(V \rightarrow W) \rightarrow W \otimes V^*$ that sends ${(\tww \circ \phi) \in \LLLL(V \rightarrow W)}$ to the elementary tensor $\ww \otimes \phi \in W \otimes V^*$:
    
    \begin{align*}
        \underbrace{\tww \circ \phi}_{\in \LLLL(V \rightarrow W)} \overset{\FF}{\longmapsto} \underbrace{\ww \otimes \phi}_{\in W \otimes V^*}.
    \end{align*}
    
    We need to show that $\FF$ is a linear bijection. Ultimately, this will be the case because $\circ$ is a bilinear map, because $\ww \mapsto \tww$ is a linear isomorphism, and because $\otimes$ appears to be bilinear.
    
    First, we show $\FF$ is linear on \textit{elementary compositions}, which we define to be linear functions of the form $(\ww \circ \phi) \in \LLLL(V \rightarrow W)$, where $\phi \in V^*$ and $\ww \in W$. These elementary compositions are similar to elementary tensors in the sense that they are not fundamentally a linear combination of two or more other elements of the form $\ww \circ \phi$.
    
    So, we need to show that
    
    \begin{align*}
        \FF(\ff_1 + \ff_2) &= \FF(\ff_1) + \FF(\ff_2) \\
        \FF(c\ff) &= c\FF(\ff),
    \end{align*}
    
    for all elementary compositions $\ff_1, \ff_2, \ff \in \LLLL(V \rightarrow W)$ and scalars $c \in K$.
    
    More explicitly, we need $\FF$ to satisfy
    
    \begin{align*}
        \tww_1 \circ \phi + \tww_2 \circ \phi 
        &\overset{\FF}{\longmapsto}
        \ww_1 \otimes \phi + \ww_2 \otimes \phi
        \\
        \tww \circ \phi_1 + \tww \circ \phi_2 
        &\overset{\FF}{\longmapsto}
        \ww \otimes \phi_1 + \ww \otimes \phi_2
        \\
        c (\tww \circ \phi)
        &\overset{\FF}{\longmapsto}
        c (\ww \otimes \phi),
    \end{align*}
    
    for all $\ww, \ww_1, \ww_2 \in W$, $\phi, \phi_1, \phi_2 \in V^*$, and $c \in K$.
    
    The above is achieved because $\circ$ is bilinear\footnote{The fact that $\circ$ is bilinear might seem rather abstract. It may be helpful to note that a familiar consequence of $\circ$ being bilinear is the fact that matrix multiplication distributes over matrix addition. So, for example, $\AA(\BB + \CC) = \AA \BB + \AA \CC$)}, because $\ww \mapsto \tww$ is a linear isomorphism, and because $\otimes$ is bilinear:
    
    \begin{align*}
        \tww_1 \circ \phi + \tww_2 \circ \phi 
        = (\tww_1 + \tww_2) \circ \phi
        = \widetilde{(\ww_1 + \ww_2)} \circ \phi
        &\overset{\FF}{\longmapsto}
        (\ww_1 + \ww_2) \otimes \phi = \ww_1 \otimes \phi + \ww_2 \otimes \phi
        \\
        \tww \circ \phi_1 + \tww \circ \phi_2 
        = \tww \circ (\phi_1 + \phi_2)
        &\overset{\FF}{\longmapsto}
        \ww \otimes (\phi_1 + \phi_2)
        = \ww \otimes \phi_1 + \ww \otimes \phi_2
        \\
        c (\tww \circ \phi)
        = (c \tww) \circ \phi
        &\overset{\FF}{\longmapsto}
        (c \ww) \otimes \phi
        = c (\ww \otimes \phi).
    \end{align*}
    
    So, $\FF$ is linear on elementary compositions. We now \textit{impose} that it be linear on nonelementary compositions to ensure its action on any defined $\ff \in \LLLL(V \rightarrow W)$ is defined, as such an $\ff$ is a linear combination of elementary compositions. This ``shows'' that $\FF$ is linear for any $\ff \in \LLLL(V \rightarrow W)$.
    
    The bijectivity of $\FF$ now follows easily. $\FF$ is onto because any nonelementary tensor coorresponds to a ``nonelementary composition'', i.e., a linear combination of elementary compositions. $\FF$ is one-to-one because it is one-to-one when restricted to elementary compositions; the linearity of $\FF$ implies that this extends to ``nonelementary compositions''. These are the main ideas of how to prove bijectivity; the explicit check is left to the reader.
\end{deriv}

So, we have proved the following theorem.

\begin{theorem}
\label{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}
    ($\LLLL(V \rightarrow W) \cong W \otimes V^*$ naturally). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces. Then there is a natural linear isomorphism
    
    \begin{align*}
        \LLLL(V \rightarrow W) \cong W \otimes V^*.
    \end{align*}
    
    This isomorphism is natural because it does not depend on a choice of basis. (See Definition \ref{ch::lin_alg::defn::natural_iso}).
\end{theorem}

\begin{proof}
    See the previous derivation.
\end{proof}

\begin{remark}
    (The two key ideas). 
    
    Now that we have gone through the derivation, we can specifically see how the two key ideas- thinking of linear functions as vectors and ``multilinear elements''- have manifested.
    
    We thought of the linear function $\ff:V \rightarrow W$ as a vector when we decomposed it into a linear combination of ``elementary compositions''. The notion of dual spaces allowed us to further abstract away the component $\phi \in V^* = \LLLL(V \rightarrow K)$ in the ``elementary composition'' $\ww \circ \phi$.
    
    In order distill ``elementary compositions'' $\ww \circ \phi$ down into objects which express the key aspects of their bilinear structure, we used the seeming-bilinearity of $\otimes$ to think of these elementary compositions as ``multilinear elements'' (speficially, we thought of them as ``bilinear elements'').
\end{remark}

Here is a quick aside that isn't entirely relevant to our immediate study of tensors. When we defined inner products (Definition \ref{ch::lin_alg::defn::inner_product}), we gave motivation for why inner products are called ``inner products'', and said that we would later explain the relevance of ``outer products''. Here is that explanation. 

\begin{defn}
\label{ch::motivated_intro::rmk::outer_products}
    (Outer products).

    Let $K$ be a field. We define the \textit{outer product of $\ww \in K^m$ and $\vv \in K^n$} to be the matrix $\ww \vv^\top \in K^{m \times n}$. 
    
    Now let $V$ and $W$ be finite-dimensional vector spaces. Throughout this chapter, we've seen that matrices of the form $\ww \vv^\top$ are identified with linear functions of the form $\ww \circ \phi \in \LLLL(V \rightarrow W)$, where $\phi \in V^*$, and that those linear functions are identified with tensors of the form $\ww \otimes \phi \in W \otimes V^*$. Thus the significance of outer products is that they are fundamentally at play whenever tensors are involved.
    
    Since when $\vv \in K^n$ and $\ww \in K^m$, the matrix $\ww \vv^\top$ can be identified with the tensor $\ww \otimes \vv \in W \otimes V$, then the tensor product $\otimes$ is unfortunately sometimes referred to as ``the outer product''. Some authors even take advantage of the fact that for $\vv \in K^n$ and $\ww \in K^n$ the function $(\vv, \ww) \mapsto \ww \vv^\top$ is bilinear, and define the tensor product on $K^n$ so that it actually is the outer product: ``$\ww \otimes \vv := \ww \vv^\top$''.

    Note: there is no sort of fundamental duality between inner products (Definition \ref{ch::lin_alg::defn::inner_product}) and outer products. The two topics are relatively unrelated.

    There is one last definition whose name sounds like it is related to inner products and outer products, and that is the \textit{exterior product of vectors}. We discuss this in Remark \ref{ch::antisymmetry::rmk::exterior_product}.
\end{defn}

\newpage

\section{Dual vector spaces}

Dual vector spaces are crucial to the theorem $\LLLL(V \rightarrow W) \cong W \otimes V^*$ because they allow us to think of linear functions as vectors. We now restate the definition of a dual space and make some additional remarks.

\begin{defn}
\label{ch::motivated_intro::defn::dual_space_2}
    (Dual vector space).
    
    Let $V$ be a vector space over a field $K$. The \textit{dual vector space} $V^*$ to $V$ is the vector space $V^* := \LLLL(V \rightarrow K)$.

    Elements of $V^*$ have various names. They may be called \textit{dual vectors}, \textit{covectors}, \textit{linear functionals}, or even \textit{linear $1$-forms} (not to be mistaken with the notion of a \textit{differential} 1-form, which we have not yet defined).
\end{defn}

\begin{defn}
\label{ch::motivated_intro::defn::covariance_contravariance}
    (Covariance and contravariance).
    
    Let $V$ be a vector space. For reasons that will be explained later, in Remark \ref{ch::intro_to_tensors::rmk::covar_contarvar_real_meaning}, dual vectors (elements of $V^*$) are said to be \textit{covariant vectors}, or \textit{covectors}, and vectors (elements of $V$) are said to be \textit{contravariant vectors}.
    
    The coordinates of a covariant vector relative to a basis are indexed by lower subscripts; contrastingly, covaraint vectors themselves are indexed by upper subscripts. So, for example, we would write a linear combination of covariant vectors as $c_1 \phi^1 + ... + c_n \phi^n$.
    
    Contravariant vectors and their coordinates follow the opposite conventions. Coordinates of contravariant vectors are indexed by upper subscripts, and contravariant vectors themselves are indexed by lower subscripts. We would write a linear combination of contravariant vectors as $c^1 \vv_1 + ... + c^n \vv_n$.
    
    The deeper meaning behind covariance and contravariance will be explained in Remark \ref{ch::intro_to_tensors::rmk::covar_contarvar_real_meaning}.
\end{defn}

\subsection*{Bases for dual spaces}

\begin{deriv}
\label{ch::motivated_intro::deriv::induced_dual_basis}
    (Induced dual basis).
    
    Let $V$ be a finite-dimensional vector space and let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$. We can discover a basis for $V^*$ by decomposing the matrix corresponding to an element of $V^*$.
    
    Specifically, any element ${\phi \in V^* = \LLLL(V \rightarrow K)}$ is represented by its matrix $\phi(E)$ relative to $E$ (recall Derivation \ref{ch::lin_alg::deriv::matrix_relative_to_bases}),
    
    \begin{align*}
        \phi(E)
        =
        \begin{pmatrix} 
            \phi(\ee_1) & \hdots & \phi(\ee_n)
        \end{pmatrix}.
    \end{align*}
    
    We we express $\phi(E)$ as a linear combination of ``basis'' row matrices:
    
    \begin{align*}
        \phi(E) = \sum_{i = 1}^n \phi(\ee_i) \see_i^\top.
    \end{align*}
    
    Applying the linear isomorphism $\FF^{-1}$ between $1 \times n$ matrices and linear functions $V \rightarrow K$ from Theorem \ref{ch::lin_alg::thm::linear_functions_matrices_isomorphism} to this equation causes  \begin{itemize}
        \item $\phi(E)$ to be replaced with $\phi$
        \item matrix multiplication to be replaced by function composition $\circ$
        \item $\see_i^\top$ to be replaced the element $\phi^{\see_i}$ of $\LLLL(V \rightarrow K)$ whose matrix relative to $E$ is $\see_i^\top$; that is, $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$
    \end{itemize}

    Therefore
    
    \begin{align*}
        \phi = \sum_{i = 1}^n \phi(\ee_i) \phi^{\ee_i},
    \end{align*}

    where $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$.
    
    We see that any $\phi \in V^*$ is a linear combination of $\phi^{\ee_1}, ..., \phi^{\ee_n}$, so $\phi^{\ee_1}, ..., \phi^{\ee_n}$ span $V^*$. Since $\see_1^\top, ..., \see_n^\top$ are linearly independent, and as $\FF^{-1}$ is a linear isomorphism, then $\phi^{\ee_1}, ..., \phi^{\ee_n}$ are also linearly independent (recall Theorem \ref{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence}). Thus, the set ${E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}}$ is a basis for $V^*$.
    
    Since $E^*$ depends on $E$, we call $E^*$ the \textit{dual basis for $V^*$ induced by $E$}.
\end{deriv}

\begin{theorem}
\label{ch::motivated_intro::deriv::dim_dual_space}
    (Dimension of dual space to a finite-dimensional vector space).
    
    If $V$ is a finite-dimensional vector space, then $\dim(V^*) = \dim(V)$.
\end{theorem}

\begin{proof}
    In the previous derivation, we started with the assumption that $V$ is $n$-dimensional, and eventually saw that the dual basis induced by a choice of basis for $V$ also contains $n$ elements.
\end{proof}

\begin{theorem}
\label{ch::motivated_intro::thm::characterizations_of_induced_dual_basis}
    (Characterizations of an induced dual basis).
     
    Let $V$ be a finite-dimensional vector space. If $E = \{\ee_1, ..., \ee_n\}$ is a basis for $V$, then the dual basis $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ for $V^*$ induced by $E$ is characterized by the following equivalent conditions:
             
    \begin{enumerate}
        \item $\phi^{\ee_i}$ is the element of $V^*$ whose matrix $\phi^{\ee_i}(E)$ relative to $E$ is $\phi^{\ee_i}(E) = \see_i^\top$. That is, $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$.
        \item $\phi^{\ee_i}(\vv) = ([\vv]_E)^i$.
        \item $\phi^{\ee_i}(\ee_j) = \delta_{ij}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    The first and second items are obviously equivalent. We show that the second condition is equivalent to the third condition; we show $(\phi^{\ee_i}(\vv) = ([\vv]_E)^i \iff \phi^{\ee_i}(\ee_j) = \delta_{ij})$. To prove the forward direction, substitute $\vv = \ee_j$. For the reverse direction, we have $\phi^{\ee_i}(\vv) = \sum_{j = 1}^n ([\vv]_E)^j \phi(\ee_j) = \sum_{j = 1}^n ([\vv]_E)^j \delta_{ij} = ([\vv]_E)^i$.
\end{proof}

\begin{remark}
\label{ch::motivated_intro::rmk::unnatural_iso_V_V*}
    (An ``unnatural'' isomorphism $V \cong V^*$).
    
    Suppose we've chosen a basis $E = \{\ee_1, ..., \ee_n\}$ for $V$, so that we have the induced dual basis ${E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}}$ for $V^*$. We can define a linear isomorphism $V \rightarrow V^*$ that is defined on basis vectors by $\ee_i \mapsto \phi^{\ee_i}$.
    
    This isomorphism is \textit{not} natural (see Definition \ref{ch::lin_alg::defn::natural_iso}) because it depends on the basis $E$ for $V$. Additionally, the function $\ee_i \mapsto \phi^{\ee_i}$ is onto but not one-to-one when $V$ is finite-dimensional, because when $V$ is infinite-dimensional the cardinality of $V^*$ is strictly greater than the cardinality of $V$.    
\end{remark}

\begin{remark}
    (We don't always have to choose induced bases).
    
    We don't have to pick a basis of $V$ to pick a basis for $V^*$. Derivation \ref{ch::motivated_intro::deriv::induced_dual_basis} showed that when $V$ is finite-dimensional, then $V^*$ is finite-dimensional. Therefore, when $V$ is finite-dimensional, we can pick an \textit{arbitrary} basis for $V^*$.
\end{remark}

\subsection*{The double dual}

The following theorem explains why $V^* = \LLLL(V \rightarrow K)$ is called the ``dual vector space'' to $V$.
\begin{theorem}
\label{ch::motivated_intro::thm::V_iso_double_dual}
    ($V \cong V^{**}$ naturally). 
    
    Let $V$ be a finite-dimensional vector space. The function $\vv \mapsto \Phi_\vv$, where $\Phi_\vv:V^* \rightarrow K$ is the element of $(V^*)^* = V^{**}$ defined by $\Phi_\vv(\phi) = \phi(\vv)$, is a natural linear isomorphism $V \rightarrow V^{**}$. Thus $V \cong V^{**}$ naturally.

    This theorem tells us that not only is $V^*$ the dual space to $V$, but that, since $(V^*)^* \cong V$ naturally, it is also true in some sense that $V$ is the dual space to $V^*$.
\end{theorem}

\begin{proof}
    Define $\FF:V \rightarrow V^{**}$ by $\FF(\vv) = \Phi_\vv$. We need to show that $\FF$ is linear, one-to-one, and onto. Checking linearity is straightforward; $\FF$ is linear regardless of the dimensionality of $V$. As for showing that $\FF$ is bijective, recall from Theorem \ref{ch::motivated_intro::deriv::dim_dual_space} that since $V$ is finite-dimensional we have $\dim(V) = \dim(V^{**})$. Thus, to show that $\FF$ is bijective it suffices, to show that $\FF$ has a minimal kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}).
    
    We have the following: if $\FF(\vv) = \mathbf{0}$, then $\Phi_\vv$ is the zero function, and so $\Phi_\vv(\phi) = \phi(\vv) = 0$ for all $\phi \in V^*$. One would think that this most recent statement implies $\vv = \mathbf{0}$, and this is indeed the case, because the contrapositive of $((\forall \phi \in V^* \spc \phi(\vv) = 0) \implies \vv = \mathbf{0})$, which is $(\vv \neq \mathbf{0} \implies (\exists \phi \in V^* \spc \phi(\vv) \neq 0))$, is clearly true: if $\vv \neq \mathbf{0}$, then for any basis $E$ of $V$, some component $([\vv]_E)^i \neq 0$ of $\vv$ is nonzero, and thus the element of $V^*$ defined by $\vv \mapsto ([\vv]_E)^i$ sends $\vv$ to a nonzero scalar, as desired.
\end{proof}

For more on this theory of duality, see Derivation \ref{ch::appendix::deriv::function_input_duality} in the appendix.

\newpage

\subsection*{Corresponding elements of dual spaces induced by bases}

\begin{defn}
    (Corresponding elements of dual spaces induced by bases).
    
    Let $V$ be a finite-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$, let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the dual basis for $V^*$ induced by $E$, and let $E^{**} = \{\Phi_{\phi^{\ee_1}}, ..., \Phi_{\phi^{\ee_n}}\}$ be the dual basis for $V^{**}$ induced by $E^*$. Additionally, let $\FF:V \rightarrow V^*$ be the isomorphism that sends $\ee_i \mapsto \phi^{\ee_i}$ and let $\GG:V^* \rightarrow V^{**}$ be the isomorphism that sends $\phi^{\ee_i} \mapsto \Phi_{\phi^{\ee_i}}$. We define the notation $\phi^\vv := \FF(\vv)$ and $\Phi_\phi := \GG(\phi)$.
\end{defn}

\begin{theorem}
    \label{ch::motivated_intro::thm::Phiphiv_eq_Phiv}
    ($\Phi_{\phi^\vv} = \Phi_\vv$).

    Assume the hypotheses of the previous definition. We have $\Phi_{\phi^\vv} = \Phi_\vv$, where $\Phi_\vv$ is defined by $\Phi_\vv(\phi) := \phi(\vv)$ (this is the definition for $\Phi_\vv$ given in Theorem \ref{ch::motivated_intro::thm::V_iso_double_dual}).
\end{theorem}

\begin{proof}
    To prove the theorem, we check that $\GG(\FF(\vv)) = \Phi_\vv$, where $\Phi_\vv(\phi) := \phi(\vv)$, holds for all $\vv \in V$. Since $\vv = \sum_{i = 1}^n ([\vv]_E)^i \ee_i$, we have $\FF(\vv) = \sum_{i = 1}^n ([\vv]_E)^i \phi^{\ee_i}$ and $\GG(\FF(\vv)) = \sum_{i = 1}^n ([\vv]_E)^i \Phi_{\phi^{\ee_i}}$. Thus, $\Big(\GG(\FF(\vv))\Big)(\phi) = \sum_{i = 1}^n \Big( ([\vv]_E)^i \Phi_{\phi^{\ee_i}}(\phi) \Big)$. Applying Theorem \ref{ch::motivated_intro::thm::characterizations_of_induced_dual_basis}, we have $([\vv]_E)^i = \phi^{\ee_i}(\vv)$ and $\Phi_{\phi^{\ee_i}}(\phi) = ([\phi]_{E^*})^i$, so \\ ${\sum_{i = 1}^n \Big( ([\vv]_E)^i \Phi_{\phi^{\ee_i}}(\phi) \Big) = \sum_{i = 1}^n \Big( \phi^{\ee_i}(\vv) ([\phi]_{E^*})^i \Big) = \sum_{i = 1}^n \Big( ([\phi]_{E^*})^i \phi^{\ee_i}(\vv) \Big) = \Big(\sum_{i = 1}^n ([\phi]_{E^*})^i \phi^{\ee_i}\Big)(\vv) = \phi(\vv) = \Phi_\vv(\phi)}$.
\end{proof}

\begin{theorem}
    \label{ch::intro_to_tensors::thm:vv_E_eq_phi_vv_Estar}
    
    ($[\vv]_E = [\phi^\vv]_{E^*}$).
    
    Assume the hypotheses of the previous definition. We have $[\vv]_E = [\phi^\vv]_{E^*}$ for any $\vv \in V$.
\end{theorem}

\begin{proof}
    The theorem is true because $\FF(\ee_i) = \phi^{\ee_i}$. A more explicit check is left to the reader.
\end{proof}

\begin{remark}
    (The misleading star notation for dual vectors).
    
    Some authors use $\vv^*$ to denote $\phi^\vv$, while also using $\{\ee_1^*, ..., \ee_n^*\}$ to denote an \textit{arbitrary} basis of $V^*$. This notation is misleading because it is suggestive of the false equation $\ee_i^* = \ee_i^*$, where the first $*$ is from the definition of $\vv^*$ and where the second $*$ is part of a basis vector $\ee_i^*$. The equation is false because it is equivalent to the following claim: ``if $\{\epsilon^1, ..., \epsilon_n\}$ is an arbitrary basis of $V^*$ then $\phi^{\ee_i} = \epsilon^i$'', which is false because it is possible to pick a basis for $V^*$ that is not the induced dual basis.
\end{remark}

\subsection*{The dual transformation}

\begin{defn}
\label{ch::motivated_intro::defn::dual_transf}
    (Dual transformation).
    
    Let $V$ and $W$ be finite-dimensional vector spaces, and let $\ff:V \rightarrow W$ be a linear function.
    The \textit{dual transformation of $\ff$}, also called the \textit{transpose of $\ff$}, is the linear function $\ff^*:W^* \rightarrow V^*$ defined by $\ff^*(\psi) = \psi \circ \ff$.
\end{defn}

The following theorem reveals why the dual transformation is sometimes referred to as the transpose.

\begin{theorem}
    (Dual transformation is represented by transpose matrix).

    Let $V$ and $W$ be finite-dimensional vector spaces with bases $E$ and $F$, and let $E^*$ and $F^*$ be the induced dual bases for $V^*$ and $W^*$. Consider a linear function $\ff:V \rightarrow W$. Recall from Derivation \ref{ch::lin_alg::deriv::matrix_relative_to_bases} that $[\ff(E)]_F$ denotes the matrix of $\ff:V \rightarrow W$ relative to $E$ and $F$. The matrix $[\ff^*(F^*)]_{E^*}$ of $\ff^*:W^* \rightarrow V^*$ relative to $F^*$ and $E^*$ is $[\ff^*(F^*)]_{E^*} = [\ff(E)]_F^{\top}$.
\end{theorem}

\begin{proof}
    Let $E = \{\ee_1, ..., \ee_n\}$, $F = \{\ff_1, ..., \ff_m\}$, $E^* = \{\epsilon^1, ..., \epsilon^n\}$, $F^* = \{\delta^1, ..., \delta^n\}$. We will show that the $ij$ entry of $[\ff^*(F^*)]_{E^*}$ is the $ji$ entry of $[\ff(E)]_F$.
    
    The $j$th column of $[\ff^*(F^*)]_{E^*}$ is $[\ff^*(\delta^j)]_{E^*}$. The $i$th entry of this column is $([\ff^*(\delta^j)]_{E^*})_i$. By Theorem \ref{ch::intro_to_tensors::thm::coords_vector_dual_vector}, we have $([\ff^*(\delta^j)]_{E^*})_i = \ff^*(\delta^j)(\ee_i)$. We have $\ff^*(\delta^j)(\ee_i) = (\delta^j \circ \ff)(\ee_i) = \delta^j(\ff(\ee_i)) = ([\ff(\ee_i)]_F)_j$, which is the $ji$ entry of $[\ff(E)]_F$, i.e., the $ij$ entry of $[\ff(E)]_F^\top$.
\end{proof}

\begin{theorem}
     (Dual of a composition).
     
     Let $V, W$, and $Y$ be finite-dimensional vector spaces, and let $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ be linear functions. Then $(\gg \circ \ff)^* = \ff^* \circ \gg^*$. 
     
     This fact is what underlies the fact $(\AA \BB)^\top = \BB^\top \AA^\top$, which tells how to transpose a matrix-matrix product.
\end{theorem}

\begin{proof}
    For all $\psi \in Y^*$ we have $(\gg \circ \ff)^*(\psi) = \psi \circ (\gg \circ \ff) = (\psi \circ \gg) \circ \ff = \gg^*(\psi) \circ \ff$, where $\gg^*(\psi) \in V^*$. Then for all $\psi \in Y^*$ we have $\gg^*(\psi) \circ \ff = \ff^*(\gg^*(\psi)) = (\ff^* \circ \gg^*)(\psi)$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::rmk::det_dual_invariant}
    (Determinant is dual-invariant).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Consider also the dual $\ff:W^* \rightarrow V^*$ (recall Definition \ref{ch::appendix::defn::dual_transf_after_id}). Then $\det(\ff^*) = \det(\ff)$.
\end{theorem}

\begin{proof}
    Recall from\footnotemark condition (3) of Definition \ref{ch::bilinear_forms_metric_tensors::defn::symmetric_linear_fn} that if $\AA$ is the matrix of $\ff$ relative to orthonormal bases $\hU_1$ and $\hU_2$, then the matrix of $\ff^*$ relative to the induced dual bases $\hU_2^*$ and $\hU_1^*$ is $\AA^\top$. Since the determinant of a matrix is transpose-invariant (recall Theorem \ref{ch::lin_alg::thm::det_transpose_invariant},) we have $\det(\ff) = \det(\AA) = \det(\AA^\top) = \det(\ff^*)$.

    \footnotetext
    {
        Technically, the equivalent conditions of the definition we reference only apply to linear functions $V \rightarrow V$. This is not an issue because $V$ and $W$ have the same dimension; if we want to be very formal, we can use the linear function $\widetilde{\ff}:V \rightarrow V$ that is obtained from $\ff$ by identifying $W \cong V$ with the identification that sends basis vectors of $W$ to basis vectors of $V$.
    }
\end{proof}

\section{$(p, q)$ tensors}

Before our detour into the world of dual spaces, we derived Theorem \ref{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}, and learned that when $V$ and $W$ are finite-dimensional vector spaces, there is a natural linear isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$. One implication of this theorem is of particular interest: the vector space of linear functions $V \rightarrow V$ is naturally isomorphic to $V \otimes V^*$. The characterization of linear maps $V \rightarrow V$ as elements of $V \otimes V^*$ lends itself to generalization; we can generalize the notion of linear function by ``tensor-producting in'' more copies of $V$ and $V^*$! This is the idea behind the following definition.

\begin{defn}
\label{ch::motivated_intro::defn::tensor_space_on_a_vector_space}
    (Tensor space on a vector space).
    
    Let $V$ be a vector space. We define a \textit{tensor space on $V$} to be a vector space of the form $V_1 \otimes ... \otimes V_k$, where each $V_i$ is either $V$ or $V^*$.
    
    The \textit{type} of a tensor space on $V$ is the sequence of positive integer superscripts and subscripts constructed in the pattern of the following examples\footnote{It is possible to give a formal definition of this notion of ``type'', but any such definition will be verbose and not particularly illustrative of the concept.}$^{,}$ \footnote{This convention goes against Definition \ref{ch::motivated_intro::defn::covariance_contravariance}, which sets the convention that vectors are associated with subscripts and covectors are associated with superscripts. This is necessary because we want the pattern of subscripts and superscripts in a tensor's type to mirror the pattern of subscripts and superscripts in a tensor's coordinates.}, in which     superscripts correspond to $V$ and subscripts correspond to $V^*$:
    
    \begin{itemize}
        \item The type of $V \otimes V^* \otimes V$ as a tensor space on $V$ is $^1{}_1{}^1$.
        \item The type of $V^* \otimes (V)^{\otimes 3}$ as a tensor space on $V$ is $_1{}^3$.
        \item The type of the tensor space $(V)^{\otimes 2} \otimes V^* \otimes V$ as a tensor space on $V$ is $^2{}_1{}^1$.
    \end{itemize}
    
    The \textit{type} of a tensor on $V$ is the type of the tensor space on $V$ of which the tensor is a member.
\end{defn}

\begin{defn}
\label{ch::motivated_intro::defn::pq_tensor_coords}
    (Coordinates of a tensor).

    Let $V$ be a finite-dimensional vector space over a field $K$ with basis $E = \{\ee_1, ..., \ee_n\}$, let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$, and let $T = V_1 \otimes ... \otimes V_k$ be a tensor space on $V$. The \textit{coordinates of a tensor $\TT \in T$ relative to $E$ and $E^*$} are the scalars in $K$ indexed by $i_1, ..., i_k$ such that
    
    \begin{itemize}
        \item $i_\ell$ is a superscript iff $V_\ell = V$ and $i_\ell$ is a subscript iff $V_\ell = V^*$.
        \item When we write both subscripts and superscripts at the same notational height, and denote the scalar indexed by $i_1, ..., i_k$ by $T(i_1, ..., i_k)$, we have 
        
        \begin{align*}
            \TT = \sum_{i_1, ..., i_k \in \{1, ..., n\}} T(i_1, ..., i_k) \spc \ww(i_1) \otimes ... \otimes \ww(i_k),
        \end{align*}
        
        where $\ww(i_\ell) = \ee_{i_\ell}$ iff $V_\ell = V$ and $\ww(i_\ell) = \epsilon^{i_\ell}$ iff $V_\ell = V^*$.
    \end{itemize}
    
    For example, the coordinates of a tensor $\SS \in V^* \otimes V \otimes V^*$ are the $S_{i_1}{}^{i_2}{}_{i_3} \in K$ such that
    
    \begin{align*}
        \SS = \sum_{i_1, i_2, i_3 \in \{1, ..., n\}} S_{i_1}{}^{i_2}{}_{i_3} \spc \epsilon^{i_1} \otimes \ee_{i_2} \otimes \epsilon^{i_3}.
    \end{align*}
    
    % \begin{align*}
    %     \TT = \sum_{\substack{i_1 ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \spc \ee_{i_1} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \epsilon^{j_q}.
    % \end{align*}
    
    Each coordinate of a tensor can be thought of as occupying a position in a           ``multidimensional matrix'', where each $i_k$ is associated with an orthogonal axis. (E.g., if a tensor's type is $^1{}_1$ or $_1{}^1$, then that tensor's coordinates are stored in a two-dimensional matrix).
\end{defn}

\begin{defn}
    \label{ch::motivated_intro::defn::pq_tensor}
    
    ($(p, q)$ tensor).
    
    Let $V$ be a vector space. We define the vector space $T_{p,q}(V)$ of \textit{$(p, q)$ tensors on $V$} to be the tensor space on $V$ of type $^p{}_q$. That is, the vector space $T_{p,q}(V)$ of $(p, q)$ tensors on $V$ is defined to be $T_{p,q}(V) := V^{\otimes p} \otimes (V^*)^{\otimes q}$.
\end{defn}

Recall, the below theorem is what motivated our definition of tensors in the first place.

\begin{theorem}
    ($(1, 1)$ tensors are naturally identified with linear functions).
    
    Since $T_{1,1}(V) = V \otimes V^*$ and $V \otimes V^* \cong \LLLL(V \rightarrow V)$ naturally, then $T_{1,1}(V) \cong \LLLL(V \rightarrow V)$ naturally. 
\end{theorem}

\begin{theorem}
    (Coordinates of $(p, q)$ tensors).

    Let $V$ be a vector space over a field $K$, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$. Since $(p, q)$ tensors are of type $^p{}_q$, then the coordinates of $\TT \in T_{p,q}(V)$ relative to $E$ and $E^*$ are of the form $T^{i_1 ... i_p}{}_{j_1 ... j_q} \in K$, with all of the superscripts occurring before all of the subscripts. Using coordinates to express $\TT$, we have

     \begin{align*}
        \TT = \sum_{\substack{i_1, ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \ee_{i_k} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \epsilon^{j_q}.
    \end{align*}
\end{theorem}

\begin{example}
\label{ch::motivated_intro::thm::linear_fn_cooresp_to_1_1_tensor}
    (Linear function corresponding to a $(1, 1)$ tensor).

    Let $V$ be a vector space, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$. Any $(1, 1)$ tensor on $V$, $\TT \in T_{1,1}(V)$, can be expressed as

    \begin{align*}
        \TT = \sum_{i,j \in \{1, ..., n\}} T^i{}_j \ee_i \otimes \epsilon^j.
    \end{align*}

    Recall from Derivation \ref{ch::motivated_intro::deriv::linear_functions_V_W_iso_W_Vstar} that $(1, 1)$ tensors are naturally identified with elements of $\LLLL(V \rightarrow V)$ via the linear isomorphism $\GG:T_{1,1}(V) = V \otimes V^* \rightarrow \LLLL(V \rightarrow V)$ defined on elementary tensors by $\GG(\ee_i \otimes \epsilon^j) := \widetilde{\ee}_i \circ \epsilon^j$, where $\widetilde{\ee}_i \in \LLLL(K \rightarrow V)$ is defined by $\widetilde{\ee}_i(c) = c\ee_i$. Thus, the corresponding element of $\LLLL(V \rightarrow V)$ is

    \begin{align*}
        \GG(\TT) = \sum_{i,j \in \{1, ..., n\}} T^i{}_j \widetilde{\ee}_i \circ \epsilon^j.
    \end{align*}
\end{example}

\begin{remark}
    ($\delta^{ij}$ vs. $\delta^i{}_j$ vs. $\delta^i{}_j$).
    
    One may be confused when they consider that there are three ways to write the Kronecker delta: $\delta^{ij}, \delta^i{}_j$, and $\delta_{ij}$. The difference between these functions of $i$ and $j$ is straightforward: $\delta^{ij}$ is used to denote coordinates of a $(2, 0)$ tensor, $\delta^i{}_j$ is used to denote coordinates of a $(1, 1)$ tensor, and $\delta_{ij}$ is used to denote coordinates of a $(0, 2)$ tensor. All three functions of $i$ and $j$ are defined, as usual, to be $1$ when $i = j$ and $0$ otherwise.
\end{remark}

\begin{defn}
    (Valence and order of a $(p, q)$ tensor). 
    
    The \textit{valence} of a $(p, q)$ tensor is the tuple $(p, q)$. The \textit{order} of a $(p, q)$ tensor is $p + q$.
\end{defn}

\begin{theorem}
\label{ch::motivated_intro::thm::four_fundamental_isos}

    (Four fundamental natural linear isomorphisms for $(p, q)$ tensors). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$. Then there exist natural linear isomorphisms
    
    \begin{empheq}[box = \fbox]{align*}
        \LLLL(V_1 \times ... \times V_k \rightarrow W) &\cong \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W)
        \\
        \LLLL(V \rightarrow W) &\cong W \otimes V^*
        \\
        (V \otimes W)^* &\cong V^* \otimes W^*
    \end{empheq}
    
    Importantly, application of the first and third line, together with the fact that $Y \cong Y^{**}$ naturally for any vector space $Y$, yields
    
    \begin{align*}
        T_{p,q}(V) \cong (T_{p,q}(V))^{**} = (V^{\otimes p} \otimes (V^*)^{\otimes q})^{**} \cong ((V^*)^{\otimes p} \otimes V^{\otimes q})^* = \LLLL((V^*)^{\otimes p} \otimes V^{\otimes q} \rightarrow K)
        \cong
        \LLLL((V^*)^{\times p} \times V^{\times q} \rightarrow K).
    \end{align*}
    
    so we have the natural linear isomorphism
    
    \begin{align*}
        \boxed
        {
            T_{p,q}(V) = V^{\otimes p} \otimes (V^*)^{\otimes q}
            \cong
            \LLLL((V^*)^{\times p} \times V^{\times q} \rightarrow K)
        }
    \end{align*}
\end{theorem}

\begin{proof}
    The first line in the first box is Theorem \ref{ch::motivated_intro::thm::multilin_fns_iso_lin_fns}, and the second line in the first box is Theorem \ref{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}. We need to prove the third line in the first box; we need to prove that \textit{taking the dual distributes over the tensor product}.
    
    We do so by defining an isomorphism $\FF$ in the ``reverse'' direction. We define this isomorphism ${\FF:V^* \otimes W^* \rightarrow (V \otimes W)^*}$ on elementary tensors and extend linearly: $\FF(\phi \otimes \psi) := f_{\phi \otimes \psi} \in (V \otimes W)^*$, where $f_{\phi \otimes \psi}:V \otimes W \rightarrow K$ is defined by $f_{\phi \otimes \psi}(\vv \otimes \ww) = \phi(\vv) \psi(\ww)$.

    It's simple to check that $\FF$ is linear. To see that $\FF$ is bijective, since it is a map between vector spaces of the same dimension, it suffices to show that it is one-to-one, i.e., that it has a minimal kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}). So, assume $f_{\phi \otimes \psi} = 0$. We need to show $\phi \otimes \psi = \mathbf{0}$.
    
    We have $f_{\phi \otimes \psi}(\vv, \ww) = \phi(\vv) \psi(\ww) = \mathbf{0}$ for all $\vv \in V$ and $\ww \in W$. Note that in general, a map into a one-dimensional vector space is not the zero function iff it has a minimal kernel. Therefore, it cannot be be the case that both $\psi \neq 0$ and $\phi \neq 0$; if this were true, then both $\phi$ and $\psi$ would have minimal kernels, and $\phi(\vv) \psi(\ww)$ would be nonzero for all $(\vv, \ww) \neq (\mathbf{0}, \mathbf{0})$, which is a contradiction. So, at least one of $\phi$ and $\psi$ is the zero function. This means that we either have $\phi \otimes \psi = \mathbf{0} \otimes \psi = \mathbf{0}$ or $\phi \otimes \psi = \phi \otimes \mathbf{0} = \mathbf{0}$. We see $\phi \otimes \psi = \mathbf{0}$ in all cases, as desired.
\end{proof}

\begin{remark}
    (The four-fold nature of $(p, q)$ tensors).
    
    We have defined a $(p, q)$ tensor to be an element of a tensor product space; a $(p, q)$ tensor is a ``multilinear element''. Due to the important natural linear isomorphisms of the previous theorem we can think of $(p, q)$ tensors in the four ways depicted by this diagram:
    
    \begin{center}
        \begin{tikzcd}
         \substack{\text{multilinear element}
         \\ \text{(element of a tensor product space)}} \arrow[<->]{dd} &  & \text{multilinear function} \arrow[<->]{ll} \\
                                               &  &                                       \\
        \substack{\text{linear element} 
        \\ \text{(element of a vector space)}} \arrow[<->]{rr}      &  &  \text{linear function} \arrow[<->]{uu}     
        \end{tikzcd}
    \end{center}
    
    It's instructive to apply these interpretations to vectors and to dual vectors. Vectors are $1$-linear elements by definition, and they are less obviously linear functions because they are naturally identifiable with elements of $V^{**}$. Dual vectors are linear functions by definition, and they are less obviously $1$-linear elements because they form a vector space.
\end{remark}

\begin{remark}
\label{ch::motivated_intro::rmk::many_defs_tensor}
    (There are many definitions of ``tensor'').
    
    There are many ways to define the notion of a ``tensor''. Here are three common ways to define what a tensor is that differ from our definition.
    
    \begin{itemize}
        \item (A physicist's definition of a tensor). Physicists and engineers most commonly define tensors to be ``multidimensional matrices'' that follow the ``the tensor transformation law'' (which is really a change of basis formula; we will derive this in Theorem \ref{ch::intro_to_tensors::thm::ricci}). This definition of tensor is clearly unmotivated, as it describes how tensors behave before explaining what they really are.
        \item (The more ``concrete'' but less insightful mathematical definition of a tensor). Mathematicians often define a $(p, q)$ tensor to be a multilinear function $(V^*)^{\times p} \times V^{\times q} \rightarrow K$. This definition is equivalent to the one we have used (we see why in Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos}), but it is less preferable because it obscures the concept of a ``multilinear element'' that tensor product spaces so nicely capture.
        \item (Another physicist's definition of a tensor). Physicists also occasionally define an ``order-$n$ tensor'' on $V$ to be\footnote{See p. 7 and p. 19 of Chapter 2 in \cite{BonetWood} for a treatment of tensors in this way.} a linear function that sends an order-$(n - 1)$ tensor to a vector in $V$, where an order-$1$ tensor is defined to be a vector in $V$. Thus, an order-$2$ tensor is an element of $\LLLL(V \rightarrow V) \cong V \otimes V^*$, and an order-$3$ tensor is an element of $\LLLL(\LLLL(V \rightarrow V) \rightarrow V) \cong \LLLL(V \otimes V^* \rightarrow V) \cong V \otimes V^* \otimes V^*$. We can use induction to prove that an $n$-order tensor of this definition is identified with an element of $V \otimes (V^{*})^{n - 1} = T_{1, n - 1}(V)$.
    \end{itemize}
\end{remark}

\newpage

\section{Coordinates of $(p, q)$ tensors}

\label{ch::intro_to_tensors::coords_of_pq_tensors}

\begin{theorem}
\label{ch::intro_to_tensors::thm::coords_vector_dual_vector}
    (Coordinates of vectors and dual vectors).

    Let $V$ be a finite-dimensional vector space, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the basis for $V^*$ induced by $E$.
    
    We have

    \begin{empheq}[box = \fbox]{align*}
        ([\vv]_E)^i &= \phi^{\ee_i}(\vv) = \Phi_\vv(\phi^{\ee_i}) = ([\Phi_\vv]_{E^{**}})^i \\
        ([\phi]_{E^*})_i &= \phi(\ee_i)
    \end{empheq}

    Recall from Theorem \ref{ch::motivated_intro::thm::V_iso_double_dual} that $\vv$ is naturally identified with $\Phi_\vv \in V^{**}$ defined by $\Phi_\vv(\phi) = \phi(\vv)$.
    
    When trying to remember this theorem, it may help to involve the contraction map $C:V \times V^* \rightarrow K$ defined by $C(\vv, \phi) = \phi(\vv)$, since we have the following:
    
    \begin{align*}
        ([\vv]_E)^i &= C(\vv, \phi^{\ee_i}) \\
        ([\phi]_{E^*})_i &= C(\ee_i, \phi).
    \end{align*}
    
    In other words, we take the $i$th coordinate of a vector by contracting that vector with the relevant dual basis vector, and we take the $i$th coordinate of a dual vector by contracting that dual vector with the relevant ``regular'' basis vector.
\end{theorem}

\begin{proof}
    To prove the first equation in the first line, $([\vv]_E)^i = \phi^{\ee_i}(\vv)$, we decompose $\vv$ in the basis $E$:
    
    \begin{align*}
        \phi^{\ee_i}(\vv) = \phi^{\ee_i}\Big( \sum_{j = 1}^n ([\vv]_E)^j \ee_j \Big) = \sum_{j = 1}^n \Big( ([\vv]_E)^j \phi^{\ee_i}(\ee_j) \Big) = \sum_{j = 1}^n ([\vv]_E)^j \delta^i{}_j = ([\vv]_E)^i.
    \end{align*}
    
    To prove the second line, we decompose $\phi$ in the basis $E^*$:
    
    \begin{align*}
        \phi(\ee_i) = \Big( \sum_{j = 1}^n ([\phi]_{E^*})_j \phi^{\ee_j} \Big)(\ee_i) = \sum_{j = 1}^n \Big( ([\phi]_{E^*})_j \phi^{\ee_j}(\ee_i)\Big) = \sum_{j = 1}^n ([\phi]_{E^*})_j \delta^j{}_i = ([\phi]_{E^*})_i.
    \end{align*}
    
    Now we prove the second equality of the first line. Recall from Theorem \ref{ch::motivated_intro::thm::V_iso_double_dual} that $\Phi_\vv$ is defined as $\Phi_\vv(\phi) := \phi(\vv)$. Therefore $\phi^{\ee_i}(\vv) = \Phi_\vv(\phi^{\ee_i})$. By applying the second line, we then have $\Phi_\vv(\phi^{\ee_i}) = ([\Phi_\vv]_{E^{**}})^i$.
\end{proof}

\subsection*{Change of basis for $(p, q)$ tensors}

\begin{theorem}
    (Change of basis for vectors and dual vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$, and let $E^*$ and $F^*$ be the corresponding induced dual bases for $V^*$. Then
    
    \begin{empheq}[box = \fbox]{align*}
        &[\vv]_F = [\EE]_F [\vv]_E \\
        &[\EE^*]_{F^*} = [\EE]_F^{-\top} \\
        &[\phi]_{F^*} = [\EE]_F^{-\top} [\phi]_{E^*}
    \end{empheq}
    
    where $\vv \in V$ and $\phi \in V^*$.
\end{theorem}

\begin{proof} \mbox{} \\ \indent
    We prove the first, second, and third equation in the box one-by-one.

    \begin{enumerate}
        \item This is just Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors}.
        
        \item The $^i{}_j$ entry of $[\EE]_F$ is $([\ee_j]_F)^i$. Applying Theorem \ref{ch::intro_to_tensors::thm::coords_vector_dual_vector}, we have $([\ee_j]_F)^i = \psi^{\ff_i}(\ee_j)$. Expanding $\psi^{\ff_i}$ relative to $E^*$ basis, we have $\psi^{\ff_i}(\ee_j) = \Big(\sum_{k = 1}^n ([\psi^{\ff_i}]_{E^*})_k \phi^{\ee_k} \Big)(\ee_j) = \sum_{k = 1}^n ([\psi^{\ff_i}]_{E^*})_k \phi^{\ee_k}(\ee_j) = \sum_{k = 1}^n ([\psi^{\ff_i}]_{E^*})_k \delta^k{}_j = ([\psi^{\ff_i}]_{E^*})_j = (\text{$^j{}_i$ entry of $[\FF^*]_{E^*}$})$. So, we have $([\EE]_F)^i{}_j = ([\FF]_{E^*})^j{}_i$, i.e. $[\EE]_F = [\FF]_E^\top$. Recalling Theorem \ref{ch::lin_alg::thm::I_EF}, we have $[\FF]_E^\top = [\EE]_F^{-\top}$. Thus $[\EE^*]_{F^*} = [\EE]_F^{-\top}$, as desired.

        \item (1) implies $[\phi]_{F^*} = [\EE^*]_{F^*} [\phi]_{E^*}$. Applying (2) to this equation yields $[\phi]_{F^*} = [\EE]_F^{-\top} [\phi]_{E^*}$, as desired.
    \end{enumerate}
\end{proof}

\begin{remark}
\label{ch::intro_to_tensors::rmk::covar_contarvar_real_meaning}

    (The meaning of ``covariance'' and ``contravariance'').

    The first two equations of the previous theorem can be restated as
    
    \begin{align*}
        [\vv]_F &= [\FF]_E^{-1} [\vv]_E \\
        [\phi]_{F^*}^\top &= [\phi]_{E^*}^\top [\FF]_E.
    \end{align*}
    
    (We have simply copied the first equation from the previous theorem. The second equation has been obtained by applying the matrix transpose to its counterpart from the previous theorem).
    
    Paying close attention to the second above equation, we see that when we treat the coordinates of dual vectors taken relative to the $E^*$ basis as row vectors (i.e. as transposed column vectors), then these row vectors transform over to the $F^*$ basis with use of $[\FF]_E$. On the other hand, the first equation states that the coordinates of vectors relative to $E$ (when treated as column vectors, as usual) transform over to the $F$ basis with use of $[\FF]_E^{-1}$. Thus, it is reasonable to say that dual vectors ``co-vary'' \textit{with} $[\FF]_E$ when changing basis from $E$ to $E^*$, and vectors ``contra-vary'' \textit{against} $[\FF]_E$ when changing basis from $F$ to $F^*$.
\end{remark}

\begin{theorem}
    (Change of basis for vectors and dual vectors in terms of basis vectors and basis dual vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ and $F^* = \{\psi^{\ff_1}, ..., \psi^{\ff_n} \}$ be the corresponding induced dual bases for $V^*$. We have
    
    \begin{empheq}[box = \fbox]{align*}
        \ee_i &= \sum_{j = 1}^n ([\ee_i]_F)^j \ff_j = \sum_{j = 1}^n ([\EE]_F)^j{}_i \ff_j \\
        \phi^{\ee_i} &= \sum_{j = 1}^n ([\phi^{\ee_i}]_{F^*})_j \psi^{\ff_j} = \sum_{j = 1}^n ([\EE]_F^{-\top})^j{}_i \psi^{\ff_j}
    \end{empheq}
\end{theorem}

\begin{proof}
    The first line in the boxed equation follows directly from the definition of $[\cdot]_F$. (The first line is also Theorem \ref{ch::lin_alg::thm::change_of_basis_with_basis_vectors})). The second line in the boxed equation follows by applying the first line to the bases $E^*$ and $F^*$ for $V^*$. Specifically, the second equation in the second line follows because $\psi^{\ff_i} = \sum_{j = 1}^n ([\EE^*]_{F^*})^j_i \phi^{\ee_j}$, where we have $[\EE^*]_{F^*} = [\FF]_E^{-\top}$ due to the previous theorem.
\end{proof}

\begin{theorem}
\label{ch::intro_to_tensors::thm::ricci}

    (Change of basis for a $(p, q)$ tensor). 
    
    Let $V$ be a finite-dimensional vector space with bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ and $F^* = \{\psi^{\ff_1}, ..., \psi^{\ff_n}\}$ be the corresponding induced dual bases for $V^*$.
    
    We now derive how to change the coordinates of a $(p, q)$ tensor. Consider $\TT \in T_{p,q}(V)$ expressed relative to the bases $E$ and $E^*$,

    \begin{align*}
       \TT = \sum_{\substack{i_1, ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}}
       T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \ee_{i_p} \otimes \phi^{\ee_{j_1}} \otimes ... \otimes \phi^{\ee_{j_q}}
    \end{align*}.
    
    To express $\TT$ relative to the bases $F$ and $F^*$, apply the previous theorem to each basis vector in $\TT$:
    
    \begin{align*}
        &\ee_{i_1} \otimes ... \otimes \ee_{i_p} \otimes \phi^{\ee_{j_1}} \otimes ... \otimes \phi^{\ee_{j_q}} \\
        &= \Big(\sum_{j_1 = 1}^n ([\EE]_F)^{j_1}{}_{i_1} \ff_{j_1} \Big) \otimes ... \otimes \Big(\sum_{j_p = 1}^n ([\EE]_F)^{j_p}{}_{i_p} \ff_{j_p} \Big)
        \otimes
        \Big( \sum_{i_1 = 1}^n ([\EE]_F^{-1})^{j_1}{}_{i_1} \psi^{\ff_{i_1}} \Big) \otimes
        ... \otimes \Big( \sum_{i_q = 1}^n ([\EE]_F^{-1})^{j_q}{}_{i_q} \psi^{\ff_{i_q}} \Big) \\
        &= \sum_{j_1 = 1}^n ... \sum_{j_p}^n \sum_{i_1 = 1}^n ... \sum_{i_q = 1}^n \Big( ([\EE]_F)^{j_1}{}_{i_1} ... ([\EE]_F)^{j_p}{}_{i_p}
        ([\EE]_F^{-1})^{j_1}{}_{i_1} ... ([\EE]_F^{-1})^{j_q}{}_{i_q} 
        \ff_{j_1} \otimes ... \otimes \ff_{j_p} \otimes \psi^{\ff_{i_1}} \otimes ... \otimes \psi^{\ff_{i_q}} \Big).
    \end{align*}
    
    After substituting this expression back into the basis sum for $\TT$, we see that an arbitrary $(p, q)$ tensor with an $\Big( {}^{i_1 ... i_p}{}_{j_1 ... j_q} \Big)$ component of $T^{i_1 ... i_p}{}_{j_1 ... j_q}$ relative to $F$ and $F^*$ has a $\Big( {}^{i_1 ... i_p}{}_{j_1 ... j_q} \Big)$ component relative to $E$ and $E^*$ of 
    
    \begin{align*}
        &\sum_{k_1 = 1}^n ... \sum_{k_p}^n \sum_{\ell_1 = 1}^n ... \sum_{\ell_q = 1}^n \Big( ([\EE]_F)^{k_1}{}_{\ell_1} ... ([\EE]_F)^{k_p}{}_{\ell_p}
        ([\EE]_F^{-1})^{k_1}{}_{\ell_1} ... ([\EE]_F^{-1})^{k_q}{}_{\ell_q} 
        T^{k_1 ... k_p}{}_{\ell_1 ... \ell_q}\Big).
    \end{align*}
    
    (It is possible to ``simplify'' this expression by using the fact that $([\FF]_E)^i{}_j ([\FF]_E)^{-1})^i{}_j = \delta^i{}_j$. Let's not do that, because that would require introducing the $\max$ function to account for whether $p \geq q$ or $q < p$).
    
    This change of basis formula is sometimes called the \textit{Ricci transformation law}, or the \textit{tensor transformation law}.
    
    At this stage, it would be remiss not to mention what is called \textit{Einstein summation notation}. In Einstein summation notation, we assume that there is an ``implied summation'' over any index that appears in both a lower and upper index. We can use Einstein notation to write the $\Big( {}^{i_1 ... i_p}{}_{j_1 ... j_q} \Big)$ component of $\TT$ relative to $F$ and $F^*$ as
    
    \begin{align*}
        ([\EE]_F)^{k_1}{}_{\ell_1} ... ([\EE]_F)^{k_p}{}_{\ell_p}
        ([\EE]_F^{-1})^{k_1}{}_{\ell_1} ... ([\EE]_F^{-1})^{k_q}{}_{\ell_q} 
        T^{k_1 ... k_p}{}_{\ell_1 ... \ell_q} \quad \text{(Einstein notation)}.
    \end{align*}
\end{theorem}

\begin{remark}
    (Change of basis with tensors in physics).
    
    Physicists use the definitions $v^i := ([\vv]_E)^i$, $\phi_i := [\phi]_{E^*}^i$, $v^i{} := [\vv]_F^i$, $\phi_i := [\phi]_{F^*}^i$, $(a^i{}_j) := [\EE]_F$, $(b^i{}_j) := (a^i{}_j)^{-1}$, along with the Einstein summation convention, to state the change of basis laws weve seen more succinctly.

    In this notation, the change of basis for vectors and dual vectors are 

    \begin{align*}
        v^i{} &= a^i_j v^j \quad \text{(Einstein notation)} \\
        \phi_i{} &= b^j_i \phi^j \quad \text{(Einstein notation)}.
    \end{align*}

    Using $T^{i_1 ... i_p}{}_{j_1 ... j_q}{}'$ to denote the coordinates of $\TT$ relative to $F$ and $F^*$, the tensor transformation law is

    \begin{align*}
        T^{i_1 ... i_p}{}_{j_1 ... j_q}{}' = a^{k_1}{}_{\ell_1} ... a^{k_p}{}_{\ell_p} b^{\ell_1}{}_{k_1} ... b^{\ell_1}{}_{k_1} T^{k_1 ... k_p}{}_{\ell_1 ... \ell_q} \quad \text{(Einstein notation)}.
    \end{align*}

\end{remark}

\begin{remark}
    (Tensors as ``multidimensional matrices'' that ``transform like tensors''). 
    
    As was mentioned in Remark \ref{ch::motivated_intro::rmk::many_defs_tensor}, physicists often define tensors to be ``multidimensional matrices'' that follow the change of basis formula of the previous theorem.
\end{remark}

\newpage

\subsection*{Tensor contraction}

\begin{deriv}
\label{ch::intro_to_tensors::deriv::compos_linear_map_with_contract}
    (Composition of linear functions with contraction). 
    
    Let $V, W$ and $Z$ be vector spaces over a field $K$. Notice that the map $\circ$ which composes linear a function $V \rightarrow W$ with a linear function $W \rightarrow Z$ is itself a bilinear map $\LLLL(V \rightarrow W) \times \LLLL(W \rightarrow Z) \overset{\circ}{\rightarrow} \LLLL(V, Z)$. (Check this as an exercise!). Also recall from Section \ref{ch::motivated_intro::sec::motivated_intro} that every element of $\LLLL(V \rightarrow W)$ and $\LLLL(W \rightarrow Z)$ is a linear combination of rank-1 compositions of linear functions, i.e., of ``elementary compositions''. Thus, we can understand the composition map $\circ$ more deeply by looking at how it acts on such elementary compositions. 
    
    Lastly, recall the convention of Section \ref{ch::motivated_intro::sec::motivated_intro} which, for $\ww \in W$, uses the same symbol $\ww$ to denote the linear map $\ww \in \LLLL(K \rightarrow W)$ defined by $\ww(c) = c\ww$. Then, under the composition map, ${(\zz \circ \phi, \ww \circ \phi) \in \LLLL(V \rightarrow W) \times \LLLL(W \rightarrow Z)}$ is sent to
    
    \begin{align*}
        (\ww \circ \phi, \zz \circ \psi) \overset{\circ}{\longrightarrow} (\zz \circ \psi) \circ (\ww \circ \phi) = \zz \circ (\psi \circ \ww) \circ \phi.
    \end{align*}
    
    Now, notice that $\phi \circ \ww$ is the linear map $K \rightarrow K$ sending $c \mapsto c\psi(\ww)$. If we extend the above notation (that uses $\ww$ and $\zz$ to denote linear maps) to elements of $K$, and denote the linear map $K \rightarrow K$ sending $c \mapsto c\psi(\ww)$ by $\psi(\ww)$, then we have 
    \begin{align*}
        (\ww \circ \phi, \zz \circ \psi) \overset{\circ}{\longrightarrow} \zz \circ \psi(\ww) \circ \phi = \psi(\ww) \circ \zz \circ \phi = \zz \circ \phi \circ \psi(\ww)
    \end{align*}
    
    (In the last three equalities, we were able to commute $\psi(\ww)$ because it is a linear map $K \rightarrow K$).
    
    In general, the action of $\psi \in W^*$ on $\ww \in W$ is said to be the result of evaluating the \textit{natural pairing map on $W$ and $W^*$}, or, equivalently, the result of \textit{contracting $W$ against $W^*$}. Therefore, we see that the composition of linear maps, when we restrict the linear maps to be elementary compositions, involves \textit{contraction}. These notions are formalized in the next definition.
\end{deriv}

\begin{defn}
\label{ch::intro_to_tensors::defn::tensor_contraction}
    (Tensor contraction).
    
    Let $V$ be a vector space, and consider also its dual space $V^*$. There is a natural bilinear form $C$ on $V$ and $V^*$, often called the \textit{natural pairing (of $V$ and $V^*$)}, that is defined by $C(\vv, \phi) = \phi(\vv)$.
    
    In a slight generalization of the natural pairing map, we define the \textit{$(k, \ell)$ contraction} on elementary $(p, q)$ tensors, and extend with multilinearity. The $(k, \ell)$ contraction of an elementary tensor is defined as follows:

    \begin{align*}
        \vv_1 \otimes ... \otimes &\vv_p \otimes \phi^1 \otimes ... \otimes \phi^q \\
        \overset{\text{$(k, \ell)$ contraction}}&{\longmapsto} \\
        C(\vv_k, \phi^\ell) (\vv_1 \otimes ... \otimes \cancel{\vv_k} &\otimes ... \vv_p \otimes \phi^1 \otimes ... \otimes \cancel{\phi^\ell} \otimes ... \otimes \phi^q) \\
        &= \\
        \phi^\ell(\vv_k) (\vv_1 \otimes ... \otimes \cancel{\vv_k} &\otimes ... \vv_p \otimes \phi^1 \otimes ... \otimes \cancel{\phi^\ell} \otimes ... \otimes \phi^q).
    \end{align*}
\end{defn}

\begin{remark}
    (Contraction with upper and lower indices).
    
    Vectors can only ever get contracted against dual vectors, and dual vectors can only ever get contracted against vectors. Vectors cannot get contracted against vectors, and dual vectors cannot get contracted against dual vectors.
    
    Since the convention we laid out in \ref{ch::motivated_intro::defn::covariance_contravariance} requires that lower indices (e.g. those which appear in $\vv_k$) be used on vectors and that upper indices (e.g. those which appear in $\phi^\ell$) be used on vectors, then it follows that lower indices can only be contracted against upper indices, and that upper indices can only be contracted against lower indices.
\end{remark}

\begin{remark}
    (Composition of linear functions with tensor contraction, revisited). 
    
    The map $\circ$ which composes linear functions is itself a bilinear map ${\LLLL(V \rightarrow W) \times \LLLL(W \rightarrow Z) \overset{\circ}{\rightarrow} \LLLL(V, Z)}$. Due to Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos}, we have the natural isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$, so $\circ$ can be identified with a linear map $\widetilde{\circ}:(W \otimes V^*) \otimes (Z^* \otimes W) \rightarrow Z \otimes V^*$. Following a similar argument as was presented in Derivation \ref{ch::intro_to_tensors::deriv::compos_linear_map_with_contract}, we see that $\widetilde{\circ}$ acts on elementary tensors by $(\ww \otimes \phi) \otimes (\zz \circ \psi) \overset{\widetilde{\circ}}{\mapsto} C(\ww, \psi) (\zz \otimes \phi) = \psi(\ww) (\zz \otimes \phi)$.
\end{remark}

\begin{deriv}
    (Coordinates of a contracted tensor).

    Let $V$ be an $n$-dimensional vector space, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$ and let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$. Consider a $(p, q)$ tensor $\TT \in T_{p,q}(V)$ with coordinates $T^{i_1 ... i_p}{}_{j_1 ... j_q}$ relative to $E$ and $E^*$,
     
    \begin{align*}
        \TT = \sum_{\substack{i_1 ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \epsilon^{j_q}.
    \end{align*}

    We will derive the coordinates of the $(k, \ell)$ contraction of $\TT$ relative to $E$ and $E^*$. Applying the $(k, \ell)$ to the above expression for $\TT$, we have
    
    \begin{align*}
        &\sum_{\substack{i_1 ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} \epsilon^{j_\ell}(\ee_{i_k}) \spc T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \cancel{\ee_{i_k}} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \cancel{\epsilon^{j_\ell}} \otimes... \otimes \epsilon^{j_q} 
        \\
        = &\sum_{\substack{i_1 ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} \epsilon^{j_\ell}(\ee_{i_k}) \spc T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \cancel{\ee_{i_k}} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \cancel{\epsilon^{j_\ell}} \otimes... \otimes \epsilon^{j_q} 
        \\
        &= \sum_{i_k, j_\ell \in \{1, ..., n\}} \epsilon^{j_\ell}(\ee_{i_k}) \sum_{\substack{i_1 ..., \cancel{i_k}, ..., i_p \in \{1, ..., n\} \\ j_1, ..., \cancel{j_\ell}, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \cancel{\ee_{i_k}} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \cancel{\epsilon^{j_\ell}} \otimes... \otimes \epsilon^{j_q} 
    \end{align*}
        
    So, we can see that
    
    \begin{align*}
        \text{The $\Big( {}^{i_1 ... \cancel{i_k} ... i_p}{}_{j_1 ... \cancel{j_\ell} ... j_q} \Big)$ component of the $(k, \ell)$ contraction of $\TT$ is } \sum_{r, s} \epsilon^{j_s}(\ee_{i_r}) \sum_r T^{i_1 ... i_{k - 1} \spc r \spc i_{k + 1} ... i_p}{}_{j_1 ... j_{\ell - 1} \spc s \spc j_{\ell + 1} ... j_q}.
    \end{align*}

    Equivalently, after shifting the indices $i_{k + 1}, ..., i_p$ and $j_{\ell + 1}, ..., j_q$ down by one (this is valid because the indices $i_k$ and $j_\ell$ are no longer occupied), we obtain that

    \textbf{what can we do here???}
\end{deriv}

\begin{remark}
    (Coordinates of a contracted tensor taken relative to dual bases).

    \textbf{do we really use an $r$ in both indices?}

    Continuing on with the above, when $E^*$ is the dual basis to $E$, then we have $\epsilon^{j_\ell}(\ee_{i_k}) = \delta^{j_\ell}_{i_k}$ and thus
    
    \begin{align*}
        \text{The $\Big( {}^{i_1 ... i_{p - 1}} {}_{j_1 ... j_{q - 1}} \Big)$ component of the $(k, \ell)$ contraction of $\TT$ is } \sum_r T^{i_1 ... i_{k - 1} \spc r \spc i_k ... i_{p - 1}}{}_{j_1 ... j_{\ell - 1} \spc r \spc j_{\ell} ... j_{q - 1}} \\ \text{when $\TT$ is expressed relative to a basis and its induced dual basis}.
    \end{align*}
    
    In Einstein notation, this is stated as
    
    \begin{align*}
        \text{The $\Big( {}^{i_1 ... i_{p - 1}} {}_{j_1 ... j_{q - 1}} \Big)$ component of the $(k, \ell)$ contraction of $\TT$ is } T^{i_1 ... i_{k - 1} \spc r \spc i_k ... i_{p - 1}}{}_{j_1 ... j_{\ell - 1} \spc r \spc j_{\ell} ... j_{q - 1}} \\ \text{when $\TT$ is expressed relative to a basis and its induced dual basis}.
    \end{align*}
\end{remark}

\begin{remark}
    (Contraction and implicit sums).
    
    Some authors refer to the indices being summed over in implicit sums (e.g. $v_i w_i$) as ``being contracted''. This is misleading. While tensor contraction can be expressed in coordinates with implicit sums, its not very helpful to think of every implicit sums as the result of a tensor contraction.
\end{remark}

\begin{theorem}
    Taking any $(k, \ell)$ contraction is basis-independent.
\end{theorem}

\begin{proof}
    Recall that the definition of tensor contraction was phrased entirely in terms of tensor products of vectors and dual vectors; no bases were involved.
\end{proof}

\begin{theorem}
    (The trace is the $(1, 1)$ contraction of a $(1, 1)$ tensor).
    
    Let $V$ be a finite-dimensional vector space over a field $K$.
    
    The \textit{trace} of a square matrix $(a^i{}_j)$ with entries in $K$ is defined to be the sum of the matrix's diagonal entries: $\tr(a^i{}_j) := \sum_{i = 1}^n a^i_i$. We have that $\tr(a^i{}_j)$ is the $(1, 1)$ contraction of the $(1, 1)$ tensor corresponding to $(a^i{}_j)$.
    
    Thus, we see the trace is a special case of tensor contraction.
\end{theorem}

\begin{proof}
    Let $\ff:K^n \rightarrow K^n$ be the linear function satisfying $[\ff(\sE)]_\sE = (a^i{}_j)$, where $\sE = \{\see_1, ..., \see_n\}$ is the standard basis for $K^n$. Recall from Theorem \ref{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V} that if $V$ is a vector space, then there is a natural isomorphism $\LLLL(V \rightarrow V) \cong V \otimes V^*$. Using $V = K^n$, we see that $(a^i{}_j)$ can be identified with the $(1, 1)$ tensor $\sum_{ij} a^i{}_j \see^i \otimes \phi^{\see_j}$, where $\sE^* = \{\phi^{\see_1}, ..., \phi^{\see_n}\}$ is the basis for $(K^n)^*$ induced by the standard basis $\sE = \{\see_1, ..., \see_n\}$ for $K^n$. The $(1, 1)$ contraction of this $(1, 1)$ tensor is $\sum_{ij} a^i{}_j \phi^{\see_j}(\see^i) = \sum_{ij} a^i{}_j \delta^i{}_j = \sum_i a^i_i = \tr(a^i{}_j)$. 
\end{proof}
