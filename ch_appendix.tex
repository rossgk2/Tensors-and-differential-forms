\chapter{Appendix}

\section*{Modular arithmetic}

\begin{remark}
\label{ch::appendix::lemma::modular_arithmetic_lemma}
    (Modular arithmetic lemma).
    
    If $m > 0$ then $-m \bmod n$ = $(\text{least integer multiple of $n$ that's greater than $m$}) - m$.

    If $m > 0$ and $m \in (0, n]$ then $(\text{least integer multiple of $n$ that's greater than $m$}) = n$, so $-m \bmod n = n - m$.
\end{remark}

\begin{proof}
    The first statement follows since $(\text{greatest integer multiple of $n$ that's less than $-m$}) = -(\text{least integer multiple of $n$ that's greater than $m$})$.
\end{proof}

\section*{Function-input duality}

\begin{deriv}
\label{ch::appendix::deriv::function_input_duality}
    (Function-input duality).
    
    Let $X$ and $Y$ be sets, let $x \in X$, let $f:X \rightarrow Y$ be a function, and consider the expression $f(x)$.

    Typically, we think of $f$ as being applied to $x$ to produce the output $f(x)$. We can also think of $x$ as being applied to $f$, in some sense, to produce $f(x)$. In intuitive notation, we have $\text{``}x(f)\text{''} = f(x)$.

    To formalize the intuition, notice that for every $x \in X$ there is a function $F_x$ that itself takes a function $X \rightarrow Y$ as input and applies it to $x$; so, $F_x(f) = f(x)$. Since the function sending $x \mapsto F_x$ is a bijection, we can identify $x$ with $F_x$ and then think of $x$ as being applied to $f$ via the action of $F_x$ on $f$. The intuition thus formalizes as $\text{``}x(f)\text{''} = F_x(f) = f(x)$.

    We've successfully formalized the idea of applying inputs to functions, but there is still more insight to be had. If we define $X^* := \{\text{functions $X \rightarrow Y$}\}$, then we have $f \in X^*$ and $F_x \in (X^*)^* = X^{**}$. We've seen that $x \mapsto F_x$ is a bijection $X \rightarrow X^{**}$. Thus, $X$ and $X^{**}$ are ``the same'', in some sense, and we write $X \cong X^{**}$. On the other hand, there is no natural bijection between $X$ and $X^*$.

    Overall, the relationship between $X$, $X^*$, and $X^{**}$ is that elements of $X \cong X^{**}$ and $X^*$ act on each other. Since elements of $X^*$ cannot be naturally identified with elements $X$, yet can still act on elements of $X$, we call $X^*$ the \textit{dual set (to $X$)}. (We call $X^{**}$ the \textit{double dual set (to $X$)}).
\end{deriv}

\section*{Symmetric and orthogonal linear functions}

The only results from this appendix that are necessary to know are the conditions (3) and (4) of Definition \ref{ch::bilinear_forms_metric_tensors::defn::orthogonal_linear_fn} satisfied by an orthogonal linear function.

\begin{deriv}
\label{ch::appendix::defn::dual_transf_after_id}
    (The adjoint of a linear function with respect to a nondegenerate bilinear form).
    
    Let $V$ and $W$ be finite-dimensional vector spaces, let $B$ be a nondegenerate bilinear form on $V$ and $W$, and consider the musical isomorphisms $\flat_1:V \rightarrow W^{*}$ and $\flat_2:W \rightarrow V^{*}$ induced by $B$.
    
    There is an induced linear map $\ff^\dag:V \rightarrow W$, called the \textit{adjoint of $\ff$ (with respect to $B$)}, that is obtained by using the musical isomorphisms on the domain and codomain of the dual transformation $\ff^*:W^* \rightarrow V^*$. We define $\ff^\dag$ to be the unique map for which the following diagram commutes:
    
    \begin{center}
       % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRRkAjFVqMWbAOoA9AFTdeIDNjwEiI0mOr1mrRBwVK+awZvLi9Uw9O7iYUAObwioAGYAnCAFskZEBwIJC0QBjoAIxgGAAV+dSEQTywnAAscEF1JAxAAHVz3cIyeD28-RFCgpAAmanCo2PjzQ2S0jKz9Nnz3d2MSkC9ff2oqxABmOsjouLMNQwYYd3aJTsN8pycTAbKkCcDgxGquCi4gA
        \begin{tikzcd}
        V \arrow[d, "\flat_1"'] \arrow[r, "\ff^\dag"] & W \arrow[d, "\flat_2"] \\
        W^* \arrow[r, "\ff^*"']                & V^*        
        \end{tikzcd}
    \end{center}
    
    Thus, $\ff^\dag = \flat_2^{-1} \circ \ff^* \circ \flat_1$, or, equivalently, $\flat_2 \circ \ff^\dag = \ff^* \circ \flat_1$.
    
    Since $\flat_2 \circ \ff^\dag = \ff^* \circ \flat_1$, then $\ff^\dag(\vv_0)^{\flat_2} = \ff^*(\vv_0^{\flat_1})$. Using the definition of $\ff^*$ (recall Definition \ref{ch::motivated_intro::defn::dual_transf}), we have $\ff^\dag(\vv_0)^{\flat_2} = \vv_0^{\flat_1} \circ \ff$. This is the same\footnote{Here we've used $B(\cdot, \vv_2)$ to denote the function $\vv_1 \mapsto B(\vv_1, \vv_2)$.} as $B(\cdot, \ff^\dag(\vv_0)) = B(\vv_0, \cdot) \circ \ff$. After evaluating both sides of this most recent equation on $\vv_1 \in V$ and relabeling $\vv_0$ as $\vv_2$, we have:
    
    \begin{align*}
        B(\vv_1, \ff^\dag(\vv_2)) = B(\vv_2, \ff(\vv_1)) \text{ for all $\vv_1, \vv_2 \in V$}.
    \end{align*}
\end{deriv}

Replacing the bilinear form of the above derivation with an inner product, we see that when we have an inner product instead of an arbitrary nondegenerate bilinear form, then $\ff^\dag$ is uniquely characterized by the condition that $\langle \vv_1, \ff^\dag(\vv_2) \rangle = \langle \vv_2, \ff(\vv_1) \rangle$ for all $\vv_1, \vv_2 \in V$. We will use this condition as a means to characterize special classes of linear functions. Specifically, we consider linear functions $\ff$ for which $\ff = \ff^\dag$ and for which $\ff^\dag = \ff^{-1}$. Before investigating such linear functions, we quickly present a definition.

\begin{defn}
    (Transpose of a matrix).
    
    The \textit{transpose} of an $m \times n$ matrix $(a^i{}_j)$ is the $n \times m$ matrix $(a^j{}_i)$. The tranpose of a matrix $\AA$ is denoted $\AA^\top$.
\end{defn}

Now, we consider linear functions $\ff$ for which $\ff = \ff^\dag$ and for which $\ff^\dag = \ff^{-1}$.

\begin{defn} 
\label{ch::bilinear_forms_metric_tensors::defn::symmetric_linear_fn}
    (Symmetric linear function).

    Let $V$ be a vector space, consider a linear function $\ff:V \rightarrow V$. We define $\ff$ to be \textit{symmetric} iff the following equivalent conditions hold:
    
    \begin{enumerate}
        \item $\ff = \ff^\dag$.
        \item $\langle \ff(\vv_1), \vv_2 \rangle = \langle \vv_1, \ff(\vv_2) \rangle$ for all $\vv_1, \vv_2 \in V$.
        \item If $V$ is finite-dimensional and $\hU$ is an orthonormal basis of $V$, then the matrix of $\ff$ relative to $\hU$ is equal to its transpose: $[\ff(\hU)]_{\hU}^\top = [\ff(\hU)]_{\hU}$. (Matrices that are equal to their own transpose are called \textit{symmetric}).
    \end{enumerate}
\end{defn}
        
\begin{proof}
    \mbox{} We need to show that the conditions are indeed equivalent.
    \\ \indent ($1 \iff 2$).
    \\ \indent ($\implies$). Use $\ff = \ff^\dag$ with $\langle \vv_1, \ff^\dag(\vv_2) \rangle = \langle \vv_2, \ff(\vv_1) \rangle$. 
    \\ \indent ($\impliedby$). We have $\langle \ff(\vv_1), \vv_2 \rangle = \langle \vv_1, \ff(\vv_2) \rangle$ for all $\vv_1, \vv_2 \in V$ and $\langle \vv_1, \ff^\dag(\vv_2) \rangle = \langle \vv_2, \ff(\vv_1) \rangle$. Therefore $\langle \vv_1, \ff(\vv_2) \rangle = \langle \vv_1, \ff^\dag(\vv_2) \rangle$. Due to the cancelability of inner products (this follows from the positive-definiteness of inner products), we have $\ff(\vv_2) = \ff^\dag(\vv_2)$ for all $\vv_2 \in V$. So $\ff = \ff^\dag$. 

    ($2 \iff $ 3). 
    \\ \indent ($\implies$). If (2) is true, then the $j$th column of $[\ff(\hU)]_{\hU}$ is $[\ff(\huu_j)]_{\hU}$, so the $^i_j$ entry of $[\ff(\hU)]_{\hU}$ is $\langle \ff(\huu_j), \huu_i \rangle$. Similarly, the $^i_j$ entry of the matrix of $\ff^\dag$ relative to $\hU$ is $\langle 
    \ff^\dag(\huu_j), \huu_i \rangle$. Due to the condition on $\langle \cdot, \cdot \rangle$ induced by the identification $V \cong V^*$, we have $\langle \ff^\dag(\huu_j), \huu_i \rangle = \langle \huu_j, \ff(\huu_i) \rangle = \langle \ff^\dag(\huu_i), \huu_j \rangle$. But this is the $^j_i$ entry of $[\ff(\hU)]_{\hU}$, i.e., the $^i_j$ entry of $[\ff(\hU)]_{\hU}^{\top}$. Thus $[\ff(\hU)]_{\hU} = [\ff(\hU)]_{\hU}^{\top}$.
    \\ \indent ($\impliedby$). Let $\hU = \{\huu_1, ..., \huu_n\}$ be an orthonormal basis for $V$, and let the matrix $\AA$ of $\ff$ relative to $\hU$ satisfy $a_{ij} = a^j_i$. Since $a^i{}_j = \langle \ff(\huu_i), \huu_j \rangle$, then $\langle \ff(\huu_i), \huu_j \rangle = \langle \huu_i, \ff(\huu_j) \rangle$. Extend with multilinearity to obtain the conclusion.
\end{proof}

\begin{defn}
\label{ch::bilinear_forms_metric_tensors::defn::orthogonal_linear_fn}
    (Orthogonal linear function). 
    
    Let $V$ be a vector space over $K$ (where\footnote{This is a very technical condition, and not much attention should be paid to it. We require this so that $2 \neq 0$, which allows us to divide by $2$.} we have $K \neq \Z/2\Z$), consider a linear function $\ff:V \rightarrow V$. We define $\ff$ to be \textit{orthogonal} iff the following equivalent conditions hold:
    
    \begin{enumerate}
        \item $\ff^\dag = \ff^{-1}$.
        \item $\langle \ff(\vv_1), \vv_2 \rangle = \langle \vv_1, \ff^{-1}(\vv_2) \rangle$ for all $\vv_1, \vv_2 \in V$.
        \item $\ff$ preserves inner product: $\langle \vv_1, \vv_2 \rangle = \langle \ff(\vv_1), \ff(\vv_2) \rangle$ for all $\vv_1, \vv_2 \in V$.
        \item $\ff$ preserves length: $\langle \vv, \vv \rangle = \langle \ff(\vv), \ff(\vv) \rangle$ for all $\vv \in V$.
        \item $\ff$ preserves length and angle: $\langle \vv, \vv \rangle = \langle \ff(\vv), \ff(\vv) \rangle$ for all $\vv \in V$ and $\theta(\vv_1, \vv_2) = \theta(\ff(\vv_1), \ff(\vv_2))$ for all $\vv_1, \vv_2 \in V$.
        \item If $V$ is finite-dimensional and $\hU$ is an orthonormal basis of $V$, then the matrix $[\ff(\hU)]_{\hU}$ of $\ff$ relative to $\hU$ has orthonormal columns.
        \item If $V$ is finite-dimensional and $\hU$ is an orthonormal basis of $V$, then $[\ff(\hU)]_{\hU}^{-1} = [\ff(\hU)]_{\hU}^{\top}$.
    \end{enumerate}
\end{defn}
        
\begin{proof}
    We need to show that the conditions are indeed equivalent. To do so, we prove $(3) \iff (4) \iff (5)$ and then $(1) \iff (2) \iff (3) \iff (6) \iff (7)$. The proof of $(3) \iff (4) \iff (5)$ is essentially the same as the proof of Lemma \ref{ch::lin_alg::lemma::orthogonal_linear_fns_preserve_alg_dot_product}; just replace the dot product of that lemma with an inner product. So, it remains to show $(1) \iff (2) \iff (3) \iff (6) \iff (7)$:
    \mbox{} \\
        ($1 \iff 2$).
        \\ \indent ($\implies$). Use $\ff^\dag = \ff^{-1}$ with $\langle \vv_1, \ff^\dag(\vv_2) \rangle = \langle \vv_2, \ff(\vv_1) \rangle$.
        \\ \indent($\impliedby$). $\langle \vv_1, \ff^{-1}(\vv_2) \rangle = \langle \ff(\vv_1), \vv_2 \rangle$ by hypothesis, and $\langle \ff(\vv_1), \vv_2 \rangle = \langle \vv_1, \ff^\dag(\vv_2) \rangle$ by condition on $\langle \cdot, \cdot \rangle$ imposed by identifying $V \cong V^*$ for $\ff^\dag$. Thus $\langle \vv_1, \ff^{-1}(\vv_2) \rangle = \langle \vv_1, \ff^\dag(\vv_2) \rangle$.
        \\ ($2 \iff 3$). Setting $\vv_3 := \ff^{-1}(\vv_2)$ shows that ${(\langle \ff(\vv_1), \vv_2 \rangle = \langle \vv_1, \ff^{-1}(\vv_2) \rangle \text{ for all $\vv_1, \vv_2 \in V$})} \iff$ \\ ${(\langle \ff(\vv_1), \ff(\vv_3) \rangle = \langle \vv_1, \vv_3 \rangle \text{ for all $\vv_1, \vv_3 \in V$})}$.
        \\ ($3 \iff 6$). 
        \\ \indent $(\implies$). We have in particular that $\langle \ff(\huu_i), \ff(\huu_j) \rangle = \langle \huu_i, {\uu}_j \rangle$. Since $\hU$ is orthonormal, $\langle \huu_i, \huu_j \rangle = \delta^i{}_j$. Therefore $\langle \ff(\huu_i), \ff(\huu_j) \rangle = \delta^i{}_j$, so the columns $[\ff(\huu_i)]_{\hU}$ of the matrix of $\ff$ relative to $\hU$ are orthonormal.
        \\ \indent $(\impliedby)$. Since the columns $[\ff(\huu_i)]_{\hU}$ of the matrix of $\ff$ relative to $\hU$ are orthonormal, we have $\langle \ff(\huu_i), \ff(\huu_j) \rangle = \delta^i{}_j$. Extend with multilinearity to obtain the conclusion.
        \\ ($6 \iff 7$).
        \\ \indent $(\implies$). The $^i_j$ entry of $[\ff(\hU)]_{\hU} [\ff(\hU)]_{\hU}^\top$ is $(\text{$i$th row of $[\ff(\hU)]_{\hU}$}) \cdot (\text{$j$th column of $[\ff(\hU)]_{\hU}$})\top = (\text{$i$th row of $[\ff(\hU)]_{\hU}$}) \cdot (\text{$j$th row of $[\ff(\hU)]_{\hU}$}) = \langle \ff(\huu_i), \ff(\huu_j) \rangle = \delta^i{}_j$. Therefore $[\ff(\hU)]_{\hU} [\ff(\hU)]_{\hU}^\top = \II$. A similar argument shows $[\ff(\hU)]_{\hU}^\top [\ff(\hU)]_{\hU} = \II$.
        \\ \indent $(\impliedby)$. Reversing the logic of the forward direction, we know $\langle \ff(\huu_i), \ff(\huu_j) \rangle = \delta^i{}_j$. Therefore (3) is satisfied. Then we use $(3) \implies (6)$.
\end{proof}

\section*{Bubble sort}

\begin{defn}
\label{ch::appendix::defn::bubble_sort}
    (Bubble sort).
    
    Given a permutation $\sigma \in S_n$, the \textit{bubble sort algorithm} operates on the tuple $(\sigma(1), ..., \sigma(n))$, and produces the tuple $(1, ..., n)$ as output.
    
    The following is a description of bubble sort that does not include the popular optimizations:
    
    \begin{itemize}
        \item For each $i \in \{1, ..., n\}$:
        \begin{itemize}
            \item For each $j \in \{1, ..., n - 1\}$, if $\sigma(j) \geq \sigma(j + 1)$, then swap (i.e. apply a transposition to) $\sigma(j)$ and $\sigma(j + 1)$.
        \end{itemize}
    \end{itemize}
    
    This algorithm indeed sorts $(\sigma(1), ..., \sigma(n))$ because the $i$th greatest element of the tuple is in its final position after the $i$th pass of the outer loop. (The first-greatest element of the tuple, or maximum, is in its final position after the first pass of the loop, the second-greatest element of the tuple is in its final position after the second pass of the loop, and so on).
\end{defn}