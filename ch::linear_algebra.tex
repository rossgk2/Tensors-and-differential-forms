\chapter{Linear algebra}
\label{ch::lin_alg}

\begin{comment}
\textbf{This chapter uses some unconventional notation.} The unconventional notation will be introduced in a natural way, so there is no need to know any of it now. This is just a ``heads up'' for those who have experience with linear algebra. Here is the list of unconventional notation used in this chapter:

\begin{itemize}
    \item Unconventional combinations of symbols are used, such as:
    \begin{itemize}
        \item ``$\ff(E)$'' (this denotes a function acting on a list of vectors)
        \item ``$[\ff(E)]_F$'' (this is explicit notation for a matrix relative to bases) \item ``$\ff_{E,F}$'' (you'll have to find out!)
    \end{itemize}
    \item To notate the $i$th component of a vector $\vv$ relative to a basis $E$, we write $([\vv]_E)_i$ instead of $v_i$. This notation may seem unnecessarily verbose, but it has the advantage of making any involvement of bases apparent.
\end{itemize}


\vspace{.25cm}

\textbf{Notation for covariance and contravariance is not used in this chapter.} The use of both upper and lower indices to distinguish between ``covariant'' and ``contravariant'' will not be used in the following chapter of linear algebra review to prevent confusion. Only lower indices will be used. (If you don't know what ``covariant'' or ``contravariant'' means, that is 100\% expected. Covariance and contravariance are explained later).
\end{comment}

Linear algebra is the study of \textit{linear elements}- which are more commonly known as \textit{vectors}- and of functions that ``respect'' the algebraic structure of linear elements.

[TO-DO]

\newpage

\section{Vectors}

\begin{defn}
    (Intuitive definition of point).
    
    Intuitively, a \textit{point} is a location in space.
\end{defn}

\begin{defn}
    (Intuitive definition of vector).
    
    Intuitively, a \textit{vector} is a locationless directed line segment. 
    
    (By ``locationless'', we mean that two directed line segments are considered equal iff one of them can be moved- without changing the distance between its start and end point- so that it coincides with the other).
\end{defn}

It is tempting to try to define either \textit{point} in terms of \textit{vector} or \textit{vector} in terms of \textit{point}, but this is actually impossible.

\begin{remark}
    (Circularity in intuitive definitions of point and directed line segment).
        
    \begin{itemize}
        \item (Attempted definition of \textit{vector} in terms of \textit{point}). 
        
        Suppose that we try to define \textit{vector} in terms of \textit{point} as follows: a \textit{vector} is a difference of the form $\pp_2 - \pp_1$, where $\pp_1$ and $\pp_2$ are points. This definition doesn't work because any intuitively interpretable definition of ``difference of points'' is ``difference of the corresponding\footnote{At this point it is unclear if a correspondence between vectors and points even exists. If we assume there is such a correspondence, though, we see the argument is circular.} vectors''; which is clearly a circular definition because it involves \textit{vector}.
        
        \item (Attempted definition of \textit{point} in terms of \textit{vector}). 
        
        Suppose that we use the above definition of \textit{vector} and attempt to use it to give a definition of \textit{point} that is equivalent to the above definition of \textit{point}. The attempted definition is: a \textit{point} is a sum of the form $(((\qq + \vv_1) + \vv_2) + ...) + \vv_n$, where $\qq$ is any point, each $\vv_i$ is a vector, and where the sum of a point and a vector is the point obtained by starting on the point and ``traveling'' according to the vector. This definition is circular because it involves \textit{point}.
    \end{itemize}
    
    In addition to the logical circularities, also notice that these attempted definitions suggest that points and vectors are inextricably intertwined: the first attempted definition requires a correspondence between vectors and points, and the second attempted definition requires a notion of adding a point with a vector.
\end{remark}

The above result forces us to define \textit{point} and \textit{vector} independently of each other; we have to define \textit{point} in terms of some more primitive mathematical object, and \textit{vector} in terms of another more primitive mathematical object. (We will actually use the same primitive mathematical object for both \textit{point} and \textit{vector}). Notice that even after we have done this, the above detailed intertwinedness and logical circularities will always persist on the intuitive level, for better or for worse.

\newpage

\subsection*{$\R^n$}

We will use elements of $\R^n$ as the primitive mathematical objects that underlie the notion of \textit{point} and \textit{vector}.

\begin{defn}
    ($\R^n$).
    
    Let $n$ be a positive integer. Recall that if $S$ is any set, then $S^n$ denotes the set of length-$n$ tuples with entries from $S$:
    
    \begin{align*}
        S^n := \underbrace{S \times ... \times S}_{\text{$n$ times}} = \{(x_1, ..., x_n) \mid x_1, ..., x_n \in S\}.
    \end{align*}
    
    In particular, $\R^n$ is the set of length-$n$ tuples whose entries are real numbers: 
    
    \begin{align*}
        \R^n = \underbrace{\R \times ... \times \R}_{\text{$n$ times}} = \{(x_1, ..., x_n) \mid x_1, ..., x_n \in \R\}.
    \end{align*}
\end{defn}

\begin{defn}
    (Column notation for elements of $\R^n$).
    
    To save horizontal writing space, we will use the following \textit{column notation} to denote elements of $\R^n$, and write $\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$ rather than $(x_1, ..., x_n) \in \R^n$. Thus we have
    
    \begin{align*}
        \R^n = \underbrace{\R \times ... \times \R}_{\text{$n$ times}} = \Bigg\{ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \mid x_1, ..., x_n \in \R \Bigg\}.
    \end{align*}
    
    When we wish to refer to an element of $\R^n$ without explicitly specifying what its entries are, we use a bold letter, and say something like ``let $\xx \in \R^n$''.
\end{defn}

Now that we have defined $\R^n$ and introduced notation to represent elements of $\R^n$, we define an \textit{addition} operation $+:\R^n \times \R^n \rightarrow \R^n$ and a \textit{scalar multiplication} operation $\cdot: \R \times \R^n \rightarrow \R^n$, and investigate the algebraic properties of these operations.

\begin{defn}
    (Addition and scalar multiplication in $\R^n$).
    
    We define \textit{addition} $+:\R^n \times \R^n \rightarrow \R^n$ and \textit{scalar multiplication} $\cdot:\R \times \R^n \rightarrow \R^n$ operations as follows.
    
    As a preliminary, we note that for $\xx_1, \xx_2 \in \R^n$ we use the notation $\xx_1 + \xx_2 := +(\xx_1, \xx_2)$, and that for $c \in \R$ and $\xx \in \R^n$ we use the notation $c \xx := \cdot(c, \xx)$.
    
    We define
    
    \begin{align*}
        \underbrace
        {
            \begin{pmatrix}
                x_1 \\ \vdots \\ x_n
            \end{pmatrix}
        }_\xx
        +
        \underbrace
        {
            \begin{pmatrix}
                y_1 \\ \vdots \\ y_n
            \end{pmatrix}
        }_\yy
        &:=
        \underbrace
        {
            \begin{pmatrix}
                x_1 + y_1 \\ \vdots \\ x_n + y_n
            \end{pmatrix}
        }_{\xx + \yy}
        \\
        c
        \underbrace
        {
            \begin{pmatrix}
                x_1 \\ \vdots \\ x_n
            \end{pmatrix}
        }_\xx
        &:=
        \underbrace
        {
            \begin{pmatrix}
                c x_1 \\ \vdots \\ c x_n
            \end{pmatrix}
        }_{c \xx}.
    \end{align*}
\end{defn}

The above addition operation $+$ and scalar multiplication operation $\cdot$ are algebraically sensible because they obey properties one would expect of an ``addition operation'' and a ``multiplication operation''. The following theorem shows this.

\newpage

\begin{theorem}
\label{ch::lin_alg::thm::prop_operations_Rn}
    (Properties of $+$ and $\cdot$ in $\R^n$).
        \begin{enumerate}
            \item (Properties of $+$).
            \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in \R^n$ such that for all $\xx \in \R^n$, $\mathbf{0} + \xx = \xx$. Specifically, we have
        
                \begin{align*}
                    \mathbf{0} = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}.
                \end{align*}
                
                \item[1.2.] (Closure under additive inverses). For all $\xx \in \R^n$ there exists $-\xx \in \R^n$ such that $\xx + (-\xx) = \mathbf{0}$. Specifically, we have
        
                \begin{align*}
                    -\begin{pmatrix}
                        x_1 \\ \vdots \\ x_n
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        -x_1 \\ \vdots \\ -x_n
                    \end{pmatrix}.
                \end{align*}
        
                \item[1.3.] (Associativity of $+$). For all $\xx_1, \xx_2, \xx_3 \in \R^n$ we have $(\xx_1 + \xx_2) + \xx_3 = \xx_1 + (\xx_2 + \xx_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\xx_1, \xx_2 \in \R^n$ we have $\xx_1 + \xx_2 = \xx_2 + \xx_1$.
            \end{enumerate}
            \item (Properties of $\cdot$).
            \begin{enumerate}
                \item[2.1.] (Normalization for $\cdot$). For all $\xx \in \R^n$ we have $1 \xx = \xx$.
                \item[2.2.] (Left-associativity of $\cdot$). For all $\xx \in \R^n$ and $c_1, c_2 \in \R$ we have $c_2 (c_1 \xx) = c_2 c_1 \xx$.
            \end{enumerate}
            \item (Compatibility of $+$ and $\cdot$).
            \begin{enumerate}
                \item[3.1.] For all $\xx_1, \xx_2 \in \R^n$ and $c \in \R$ we have $c(\xx_1 + \xx_2) = c \xx_1 + c \xx_2$.
                \item[3.2.] For all $\xx \in \R^n$ and $c_1, c_2 \in \R$ we have $(c_1 + c_2)\xx = c_1 \xx + c_2 \xx$.
            \end{enumerate}
        \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
        \item The claims about the additive identity and additive inverses follow from the fact that $0$ is the additive identity in $\R$ and that the additive inverse of $x \in \R$ is denoted by $-x$. The associativity and commutativity of $+$ follow from the associativity and commutativity of addition of real numbers.
        \item The normalization property of $\cdot$ follows from the fact that $1$ is the multiplicative identity in $\R$. The left-associativity of $\cdot$ follows from the associativity of real numbers.
        \item Both compatibility properties follow from the facts that we have $c (x_1 + x_2) = c x_1 + x_2$ for all $c, x_1, x_2 \in \R$ and $(c_1 + c_2) x = c_1 x + c_2 x$ for all $c_1, c_2, x \in \R$. In other words, they follow from the fact that $\cdot$ distributes over $+$ in $\R$ and from the fact that $\cdot$ is commutative in $\R$.
    \end{enumerate} 
\end{proof}

\newpage

\subsection*{Points and vectors in $\R^n$}

Now we define \textit{point} and \textit{vector} in terms of elements of $\R^n$.

\begin{defn}
    (Points and vectors in $\R^n$).
    
    A \textit{point} in $\R^n$ is an element of $\R^n$. A \textit{vector} in $\R^n$ is also an element of $\R^n$.
    
    When we have $\xx = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$, we can interpret $\xx$ to be either a point or vector. The numbers $x_1, ..., x_n \in \R$ are called the \textit{entries}, or \textit{components}, of $\xx$.
    
    If we interpret $\xx$ to be a point, then each $x_i$ is the point's displacement on the $x_i$ axis. If we interpret $\xx$ to be a vector, then each $x_i$ is the displacement, on the $x_i$ axis, of the end of $\xx$ relative to the start of $\xx$, where- since vectors are locationless- we additionally allow the start of $\xx$ to be any point.
\end{defn}

\begin{remark}
    (Intertwinedness of points and vectors).
    
    The above definition easily implements the intertwinedness of points and vectors; since every point in $\R^n$ can also be interpreted as a vector in $\R^n$, the correspondence between points and vectors is given by the identity function.
\end{remark}

Even though there is always some circularity on an intuitive level between the concepts of \textit{point} and \textit{vector}; the setting of $\R^n$ grounds the following geometric intuitions.

\begin{remark}
\label{ch::lin_alg::rmk::geometric_intuition_points_vectors}
    (Geometric intuition about points and vectors).
        
    (Geometric intuition about $+$).
    \begin{itemize}
        \item (Point plus vector). Let $\pp \in \R^n$ be a point and $\vv \in \R^n$ be a vector. We interpret $\pp + \vv$ to be the point that is obtained by placing the start of $\vv$ on $\pp$ and then ``traveling'' according to $\vv$.
        \item (Vector plus vector). Let $\vv_1, \vv_2 \in \R^n$ be vectors. We interpret $\vv_1 + \vv_2$ to be the result of starting at the start point of $\vv_1$ (which of course can be any point), then traveling according to the vector $\vv_1$ to reach the end point of $\vv_1$, then applying ``locationlessness'' to place the start point of $\vv_2$ on the end point of $\vv_1$, and then traveling to the end point of $\vv_2$. That is, the directed line segment that reaches from the ``overall'' beginning to the ``overall'' end is $\vv_1 + \vv_2$.
        \begin{itemize}
            \item If we use locationlessness of to make the end point of $\vv_1$ coincide with the start point of $\vv_2$ before we start the above process, then $\vv_1 + \vv_2$ is represented by the directed line segment that connects the start point of $\vv_1$ to the end point of $\vv_2$. This interpretation is often called the ``tail-to-tip method'', since one computes $\vv_1 + \vv_2$ by forming the vector by drawing a straight line from the “tail” of $\vv_1$ to the “tip” of $\vv_2$.
        \end{itemize}
        \item ($\mathbf{0}$ as a point). When $\mathbf{0}$ is considered to be a point, we interpret it to be the ``central reference point'' of the coordinate system, i.e., the origin.
        \item ($\mathbf{0}$ as a vector). When $\mathbf{0}$ is interpreted to be a vector, we interpret it to be the locationless directed line segment that has a length of zero.
        \item (Additive inverses as points). When $\pp \in \R^n$ is a point, there is no satisfactory interpretation of $-\pp$.
        \item (Additive inverses as vectors). When $\vv \in \R^n$ is a vector, the fact $\vv + (-\vv) = \mathbf{0}$ forces us to interpret $-\vv$ to be the unique vector whose end is the start of $\vv$ if its start is made to coeinide with the end of $\vv$.
    \end{itemize}

    (Geometric intuition about $\cdot$).
    \begin{itemize}
        \item (Scalar times point). Let $c \in \R$. When $\pp \in \R^n$ is a point, there is no satisfactory interpretation of $c \pp$.
        \item (Scalar times vector). Let $c > 0$. When $\vv \in \R^n$ is a vector, we interpret $c \vv$ to be the vector that is obtained by stretching $\vv$ by a factor of $c$ (when $c > 1$) or that is obtained by shrinking $\vv$ by a factor of $c$ (when $c \in (0, 1)$). When $d < 0$, we interpret $d\vv$ to be $-(|d|\vv)$; that is, we interpret $d\vv$ to be the additive inverse of $|d|\vv$.
    \end{itemize}
\end{remark}

\begin{defn}
    (``Vector'' means ``vector'' or ``point'' depending on context).
    
    In practice, we say ``vector in $\R^n$'' to mean ``element of $\R^n$'', and determine whether $\vv \in \R^n$ is interpreted to be a point or a locationless directed line segment from context.
\end{defn}

\newpage

\subsection*{Vector-containing sets}

At this point, we know that we can make the intuitive notions of ``vector'' and ``point'' precise by grounding them in the concrete setting of $\R^n$. This works, of course, but thinking about of vectors in $\R^n$ has a good deal of unnecessary ``implementation detail''. When thinking about a single vector in $\R^n$, one is secretly thinking about the several numbers that are the vector's components!

Thankfully, the previous theorem (Theorem \ref{ch::lin_alg::thm::prop_operations_Rn}) provides an opportunity to escape dealing with components. The theorem states key properties of vectors in $\R^n$ (such as ``there exists $\mathbf{0} \in \R^n$ such that for all $\xx \in \R^n$ we have $\xx + \mathbf{0} = \xx$'', and ``for all $\xx_1, \xx_2 \in \R^n$ and $c \in \R$ we have $c(\xx_1 + \xx_2) = c\xx_1 + c\xx_2$'') in terms of the vectors themselves, and \textit{not} in terms of their components!

The properties of this helpful theorem are in general the properties that we expect of vectors. Thus, we can leave behind the baggage of $\R^n$ by effectively defining vectors to be objects that have the properties of the theorem. We do this now.

\begin{defn}
\label{ch::lin_alg::defn::vector_containing_set_R}
    (Vector-containing set over $\R$).
    
    Suppose that $T$ is a set for which there exist functions $+:T \times T \rightarrow T$ and $\cdot:\R \times T \rightarrow T$. We say that a set $S \subseteq T$ is a \textit{vector-containing set over $\R$} iff the operations $+$ and $\cdot$ satisfy the typical\footnote{When you try to remember these properties, there is no need to be incredibly specific. You don't need to list out ``existence of additive identity'', ``closure under additive inverses'', and so on \textit{every} time you remind yourself of what a vector-containing set over $\R$ is!} properties one would expect of ``vector addition'' and ``vector scaling''. That is, $S$ is a vector-containing set over $\R$ iff $+$ and $\cdot$ satisfy the properties\footnote{You may be confused as to why $T$, rather than $S$, is involved in all of the above conditions when we are defining what it means for $S$ to be a vector-containing set. This is because we want $S$ to have \textit{most} of the properties that $T$ does, but not all of them. Specifically: even though the above conditions require $\mathbf{0} \in T$ and $(\forall \vv \spc \vv \in T \iff -\vv \in T)$, they do not require $\mathbf{0} \in S$ or $(\forall \vv \spc \vv \in S \iff -\vv \in S)$.} listed in Theorem \ref{ch::lin_alg::thm::prop_operations_Rn}:
    
    \begin{enumerate}
        \item (Properties of $+$).
        \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in T$ such that for all $\vv \in T$ we have $\vv + \mathbf{0} = \vv$.
                \item[1.2.] (Closure under additive inverses). For all $\vv \in T$ there exists $-\vv \in T$ such that $\vv + (-\vv) = \mathbf{0}$.
                \item[1.3.] (Associativity of $+$). For all $\vv_1, \vv_2, \vv_3 \in T$ we have $(\vv_1 + \vv_2) + \vv_3 = \vv_1 + (\vv_2 + \vv_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\vv_1, \vv_2 \in T$ we have $\vv_1 + \vv_2 = \vv_2 + \vv_1$.
            \end{enumerate}
        \item (Properties of $\cdot$).
        \begin{enumerate}
            \item[2.2.] (Normalization for $\cdot$). For all $\vv \in T$ we have $1 \vv = \vv$.
            \item[2.1.] (Left-associativity of $\cdot$). For all $\vv \in T$ and $c_1, c_2 \in \R$ we have $c_2 (c_1 \vv) = c_2 c_1 \vv$.
        \end{enumerate}
        \item (Properties of $+$ and $\cdot$).
        \begin{enumerate}
            \item[3.1.] For all $\vv_1, \vv_2 \in T$ and $c \in \R$ we have $c(\vv_1 + \vv_2) = c \vv_1 + c \vv_2$.
            \item[3.2.] For all $\vv \in T$ and $c_1, c_2 \in \R$ we have $(c_1 + c_2)\vv = c_1 \vv + c_2 \vv$.
        \end{enumerate}
    \end{enumerate} 
    
    When $S$ is a vector-containing set over $\R$, we refer to elements of $S$ as \textit{vectors}. As before, elements of $\R$ are sometimes called \textit{scalars}.
\end{defn}

As we discussed before, this abstract characterization of a vector-containing set is useful because it decouples our notion of ``vector'' from the setting of $\R^n$. It allows us to see that many types of objects- which we might have overlooked before- can be interpreted to be vectors.

\begin{remark}
    (Examples of vector-containing sets over $\R$).
    
    Here are some examples of vector-containing sets over $\R$:
    
    \begin{itemize}
        \item $\R^n$, for any positive integer $n$, where $+$ and $\cdot$ are as usual (this one isn't intended to be surprising)
        \item $\Big\{\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} -3 \\ 0 \end{pmatrix}\Big\} \subseteq \R^2$, where $+$ and $\cdot$ are the same as the functions $+$ and $\cdot$ on $\R^2$
        \item $\{\vv_1, ..., \vv_k\} \subseteq S$, where $S$ is another vector-containing set, and $+$ and $\cdot$ are the same as the functions $+$ and $\cdot$ on $S$
        \item The set of polynomials with real coefficients of degree less than $n$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number
        \item The set of infinitely differentiable functions $\R \rightarrow \R$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number
        \item The set of infinite sequences of real numbers, where $+$ and $\cdot$ are defined in the ways you would expect
    \end{itemize}
\end{remark}

Now that we have defined what a ``vector-containing set over $\R$'' is, it is reasonable to introduce a little more abstraction. (Don't worry, though. This abstraction is much easier to grasp than the leap from ``vectors in $\R^n$'' to ``elements of a vector-containing set''). In place of ``vector-containing sets over $\R$'', we will now consider ``vector-containing sets over $K$'', where $K$ is the set-with-structure that contains the scalars.

In other words, in the same way that a vector is ``something that lives in a vector-containing set'', we will define a scalar to be ``something that lives in a `scalar space' ''.  Unfortunately, the terminology ``scalar space'' is nonstandard;  ``scalar spaces'' are actually called \textit{fields}.

\begin{defn}
    (Field). 
    
    Suppose that $K$ is a set for which there exist functions $+:K \times K \rightarrow K$ and $\cdot:K \times K \rightarrow K$. We say that the tuple $(K, +, \cdot)$ is a \textit{field} iff it satisfies\footnote{In the terminology of abstract algebra, a field can be defined to be (1) an ``integral domain that is closed under multiplicative inverses'' or (2) as a ``commutative division ring''.} the following:

    \begin{enumerate}
        \item $K$ is a ``commutative group under addition''. This means that conditions 1.1 through 1.5 must hold.
        \begin{enumerate}
            \item[1.1.] (Closure under $+$). For all $c_1, c_2 \in K$ we have $c_1 + c_2 \in K$.
            \item[1.2.] (Existence of additive identity). There exists $0 \in K$ such that for all $c \in K$ we have $0 + c = c$.
            \item[1.3.] (Associativity of $+$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 + c_2) + c_3 = c_1 + (c_2 + c_3)$.
            \item[1.4.] (Closure under additive inverses). For all $c \in K$ there exists $-c \in K$ such that $(-c) + c = 0$.
            \item[1.5.] (Commutativity of $+$). For all $c_1, c_2 \in K$ we have $c_1 + c_2 = c_2 + c_1$.
        \end{enumerate}
        \item $K$ is a ``commutative group under multiplication''. This means that conditions 2.1 through 2.5 must hold.
        \begin{enumerate}
            \item[2.1.] (Closure under $\cdot$). For all $c_1, c_2 \in K$ we have $c_1 \cdot c_2 \in K$.
            \item[2.2.] (Existence of multiplicative identity). There exists $1 \in K$ such that for all $c \in K$ we have $1 \cdot c = c$.
            \item[2.3.] (Associativity of $\cdot$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 \cdot c_2) \cdot c_3 = c_1 \cdot (c_2 \cdot c_3)$.
            \item[2.4.] (Closure under multiplicative inverses). For all $k \in K$ with $k \neq 0$, there exists $\frac{1}{c} \in K$ such that $\frac{1}{c} \cdot c = 1$.
            \item[2.5.] (Commutativity of $\cdot$). For all $c_1, c_2 \in K$ we have $c_1 \cdot c_2 = c_2 \cdot c_1$.
        \end{enumerate}
        \item ($\cdot$ distributes over $+$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 + c_2) \cdot c_3 = c_1 \cdot c_3 + c_2 \cdot c_3$.
    \end{enumerate} 

    Colloquially, we often say ``let $K$ be a field'' instead of ``let $(K, +, \cdot)$ be a field''.
    
    Recall that the whole point of defining a field is to formalize the notion of what a ``scalar'' is. For this reason, elements of a field are called \textit{scalars}.
\end{defn}

\begin{remark}
    (Examples of fields).
    
    \begin{itemize}
        \item $\R$ is a field.
        \item The complex numbers $\C = \{ a + b\sqrt{-1} \mid a, b \in \R\}$ are a field.
    \end{itemize}
\end{remark}

\begin{remark}
    (Don't memorize the definition of a field!).
    
    It's not necessary to memorize all the conditions for a field. Just remember that a field is ``a set that contains elements which one can add, subtract, multiply, and divide''. In this book, you can imagine an arbitrary field $K$ as being $\R$ almost all of the time\footnote{It's true that there is more to it when the field is a \textit{finite field}, but this is not a major concern in this book.}.
\end{remark}

Without further ado, we define ``vector-containing set over a field''.

\begin{defn}
\label{ch::lin_alg::defn::vector_space}
    (Vector-containing set over a field).
    
    Suppose that $K$ is a field, that $T$ is a set, and that there exist functions $+:T \times T \rightarrow T$ and $\cdot:K \times T \rightarrow T$. If $S \subseteq T$ is a set, we say that the tuple $(S, K, +, \cdot)$ is a \textit{vector-containing set}, or, more colloquially, that ``$S$ is a vector-containing set over $K$'', iff:
    
    \begin{enumerate}
        \item (Properties of $+$).
        \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in T$ such that for all $\vv \in T$ we have $\vv + \mathbf{0} = \vv$.
                \item[1.2.] (Closure under additive inverses). For all $\vv \in T$ there exists $-\vv \in T$ such that $\vv + (-\vv) = \mathbf{0}$.
                \item[1.3.] (Associativity of $+$). For all $\vv_1, \vv_2, \vv_3 \in T$ we have $(\vv_1 + \vv_2) + \vv_3 = \vv_1 + (\vv_2 + \vv_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\vv_1, \vv_2 \in T$ we have $\vv_1 + \vv_2 = \vv_2 + \vv_1$.
            \end{enumerate}
        \item (Properties of $\cdot$).
        \begin{enumerate}
            \item[2.2.] (Normalization for $\cdot$). For all $\vv \in T$ we have $1 \vv = \vv$.
            \item[2.1.] (Left-associativity of $\cdot$). For all $\vv \in T$ and $c_1, c_2 \in K$ we have $c_2 (c_1 \vv) = c_2 c_1 \vv$.
        \end{enumerate}
        \item (Properties of $+$ and $\cdot$).
        \begin{enumerate}
            \item[3.1.] For all $\vv_1, \vv_2 \in T$ and $c \in K$ we have $c(\vv_1 + \vv_2) = c \vv_1 + c \vv_2$.
            \item[3.2.] For all $\vv \in T$ and $c_1, c_2 \in K$ we have $(c_1 + c_2)\vv = c_1 \vv + c_2 \vv$.
        \end{enumerate}
    \end{enumerate}

    (If you are confused as to why $T$, rather than $S$, is involved in all of the above conditions, see the footnote above the conditions in Definition \ref{ch::lin_alg::defn::vector_containing_set_R}).
    
    Elements of vector-containing sets are called ``vectors''.
    
    In practice, we often don't explicitly mention a field, and say ``let $S$ be a vector-containing sets'' instead of ``let $S$ be a vector-containing set over a field $K$''.
\end{defn}

\subsection*{Vector spaces}

Now that our notion of vector-containing set is complete, we can make use of vector-containing sets. 

Most often, we interpret vectors to be points. In the setting of the vector-containing set $\R^n$, it is easy to describe a point: you simply list the displacements $x_1, ..., x_n \in \R$ of the point from each of the mutually perpendicular coordinate axes (we discussed this in Remark \ref{ch::lin_alg::rmk::geometric_intuition_points_vectors}). We can't describe a vector from a general vector this way, though, because the vectors are not necessarily elements of $\R^n$! We can do something similar, though. Just as $\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$ is a weighted sum of the vectors $\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, ..., \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} \in \R^n$, we can consider weighted sums of vectors from vector-containing sets. The following definition gives a name for vectors that are the result of such weighted sums.

\begin{defn}
    (Linear combination).
    
    Let $S = \{\vv_1, ..., \vv_k\}$ be a finite vector-containing set over a field $K$. We define a \textit{linear combination of the vectors in $S$}, or, more colloquially, a \textit{linear combination of $\vv_1, ..., \vv_k$}, to be a vector of the form
        
    \begin{align*}
        c_1 \vv_1 + ... + c_k \vv_k,
    \end{align*}
    
    where $c_1, ..., c_k$ are some scalars in $K$. So, a linear combination of $\vv_1, ..., \vv_k$ is a ``weighted sum'' involving $\vv_1, ..., \vv_k$.
\end{defn}

[motivation for considering vector containing sets that are spanned]

\begin{defn}
    (Span).
    
    Let $S = \{\vv_1, ..., \vv_k\}$ be a finite vector-containing set over a field $K$. We define the \textit{span} of $S$, or, more colloquially, the \textit{span of $\vv_1, ..., \vv_k$}, to be the set of all linear combinations of $\vv_1, ..., \vv_k$:
    
     \begin{align*}
        \spann(S) = \spann(\{\vv_1, ..., \vv_k\}) := \Big\{ c_1 \vv_1 + ... + c_k \vv_k \mid c_1, ..., c_k \in K \Big\}.
    \end{align*}
    
    For geometric intuition, notice that if $\vv_1, ..., \vv_k \in \R^n$, then $\spann(\{\vv_1, ..., \vv_k\})$ is the $k$-dimensional plane spanned by $\vv_1, ..., \vv_k$ that is embedded in $\R^n$. For example, if $\vv, \ww \in \R^3$, then ${\spann(\{\vv, \ww\}) = \{ c \vv + d \ww \mid c, d \in \R \}}$ is the $2$-dimensional plane spanned by $\vv$ and $\ww$ in $\R^3$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::vector_space}
    (Vector space over a field).
    
    Suppose that $K$ is a field, that $V$ is a set, and that there exist functions $+:V \times V \rightarrow V$ and $\cdot:K \times V \rightarrow V$. We say that the tuple $(V, K, +, \cdot)$ is a \textit{vector space}, or, more colloquially, that ``$V$ is a vector space over $K$'', iff 

    \begin{enumerate}
        \item $(V, K, +, \cdot)$ is a vector-containing set.
        \item $V$ is spanned by some vector-containing set, i.e., there is a vector-containing set $S$ for which $V = \spann(S)$. 
    \end{enumerate}
    
    Elements of vector spaces are called ``vectors''.
    
    In practice, we often don't explicitly mention a field, and say ``let $V$ be a vector space'' instead of ``let $V$ be a vector space over a field $K$''.
\end{defn}

\begin{remark}
    Vector-containing sets and subsets of vector spaces are one and the same: vector-containing sets are subsets of vector spaces, and subsets of vector spaces are vector-containing sets. From this point on, we will favor ``subset of a vector space'' over ``vector-containing set'', because the terminology ``vector-containing set'' is nonstandard.
\end{remark}

Most of the examples of vector-containing sets over $\R$ we gave before are actually vector spaces.

\begin{remark}
    (Examples of vector spaces).
    
    Here are some examples of vector spaces:
    
    \begin{itemize}
        \item $\R^n$ 
        \item $K^n$.
        \item The set of polynomials with real coefficients of degree less than $n$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number.
        \item The set of polynomials with rational coefficients of degree less than $n$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number.
        \item The set of infinitely differentiable functions $\R \rightarrow \R$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number.
        \item The set of infinite sequences of real numbers, where $+$ and $\cdot$ are defined in the ways you would expect.
    \end{itemize}
    
    Here are some examples of vector-containing sets that are \textit{not} vector spaces:
    
    \begin{itemize}
        \item $\Big\{\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} -3 \\ 0 \end{pmatrix}\Big\} \subseteq \R^2$.
        \item $\{\vv_1, ..., \vv_k\} \subseteq S$.
        \item $\{ z \in \R \mid z \geq ax + by \} \subseteq \R^3$, where $a, b \in \R$.
        \item $\{ x_n \in \R \mid x_n \geq c_1 x_1 + ... + c_k x_k \} \subseteq \R^n$, where $c_1, ..., c_k \in \R$, $k \leq n - 1$.
    \end{itemize}
\end{remark}

\begin{remark}
    ($\emptyset$ is not a vector space).
    
    The empty set $\emptyset$ is not a vector space over any field because it is not equal to the span of any set. (A set is either empty or not. The span of the empty set is $\{0\}$, and the span of a nonempty set is nonempty. Thus no set has span equal to the empty set).
\end{remark}

\begin{remark}
    (Why is the generality of ``vector space'' useful?)
    
    In this book, we will often consider an arbitrary vector space and use it as a ``base'' from which to construct more vector spaces, while secretly imagining the ``base'' vector space as being $\R^n$. Dealing in the abstraction of vector spaces makes it easier to see precisely how vector spaces produced from $\R^n$ are related to $\R^n$ by removing any doubt as to whether coordinates, which are easy to run into when one deals with $\R^n$ explicitly, are fundamentally at play.
\end{remark}

Now that we have defined what a vector space is in full generality, we present an alternative characterization of vector spaces that is easier in practice to check than the above definition. 

\begin{deriv}
    (Vector spaces are closed under addition and scalar multiplication).
    
    Suppose $V$ is a vector space over a field $K$. Working heuristically, we can discover that the spanning condition satisfied by vector spaces is equivalent to another condition:
    
    \begin{align*}
        &\text{There is a vector-containing set $S$ for which $V = \spann(S)$.} \\
        &\quad \quad \quad \quad \quad \quad \iff \\
        &\text{For all $\vv_1, \vv_2 \in V$ we have $\vv_1 + \vv_2 \in V$ (``$V$ is closed under vector addition'')} \\
        &\quad \quad \quad \quad \quad \quad \spc \spc \text{and} \\
        &\text{for all $c \in K$ and $\vv \in V$ we have $c \vv \in V$ (``$V$ is closed under vector scaling'')}.
    \end{align*}
    
    The idea for showing that the hypothesis implies the first part of the conclusion (the part before the ``and'') is this: if we assume that $V$ is spanned by some vector-containing set $S$, then we can compute $\vv_1 + \vv_2$ by writing $\vv_1$ and $\vv_2$ as ``weighted sums'' of elements from $S$, and the result, being a (longer) weighted sum of elements from $S$, is thus in $V$. Similar reasoning shows the second part of the conclusion. The reverse implication is simple: there is always a vector-containing set spanning $V$, since every vector-containing set spans itself.
\end{deriv}

When determining whether a set $V$ is a vector space or not, it is almost always easier to check the above closure conditions than to check if there is a set that spans $V$. Thus, the following characterization of vector spaces is almost always what one should use when testing if a set is a vector space.

\begin{theorem}
\label{ch::lin_alg::thm::vector_space_alt_characterization}
    (Alternate characterization of vector spaces).
    
    Suppose that $K$ is a field, that $V$ is a set, and that there exist functions $+:V \times V \rightarrow V$ and $\cdot:K \times V \rightarrow V$. The tuple $(V, K, +, \cdot)$ is a vector space iff
    
    \begin{enumerate}
        \item $(V, K, +, \cdot)$ is a vector-containing set.
        \item ``$V$ is closed under vector addition and vector scaling''.
        \begin{enumerate}
            \item[2.1.] (Closure under $+$). For all $\vv_1, \vv_2 \in V$, $\vv_1 + \vv_2 \in V$.
            \item[2.2.] (Closure under $\cdot$). For all $c \in \R$ and $\vv \in V$, $c \vv \in V$.
        \end{enumerate}
    \end{enumerate}
\end{theorem}

We know that if $V$ is a vector space, then $V$ is a vector-containing set, and thus that there is a set $T \supseteq V$ that contains zero and the additive inverse of every vector in $V$. We now see that the fact that vector spaces are spanned by vector-containing sets implies these important vectors are members of all vector spaces.

\begin{deriv}
    (Vector spaces have zero and additive inverses).

    Let $V$ be a vector space. Then for any $\vv \in V$,
    
    \begin{enumerate}
        \item The zero vector is $\mathbf{0} = 0\vv$.
        \item The additive inverse of $\vv$ is $-\vv = (-1)\vv$.
    \end{enumerate}

    Because $V$ is closed under vector scaling, both of these vectors are in $V$.
\end{deriv}

\begin{proof}
    We've already explained that if the given formulas for the zero vector and additive inverses are true, then $V$ contains the zero vector and all additive inverses. We now show that the formulas are true.

    \begin{enumerate}
        \item Let $\vv, \ww \in V$. We want to show that $0\vv + \ww = \ww$. We have $0\vv + \ww = 0\vv + \vv + (\ww - \vv) = (0 + 1)\vv + \ww - \vv = \vv + \ww - \vv = \ww$, as desired.
        \item For all $\vv \in V$ we have $\vv + (-1)\vv = 1\vv + (-1)\vv = (1 - 1)\vv = 0\vv = \mathbf{0}$.
    \end{enumerate}
\end{proof}

\subsection*{Vector subspaces}

\begin{defn}
\label{ch::lin_alg::defn::vector_subspace}
    (Vector subspace). 
    
    If $V$ and $W$ are vector spaces over $K$ and $W \subseteq V$, then $W$ is a \textit{vector subspace} of $V$.
\end{defn}

\begin{remark}
    (Examples of vector subspaces).
    
    \begin{itemize}
        \item $\R^2$ is a vector subspace of $\R^3$.
        \item $\R^n$ is a vector subspace of $\R^m$ whenever $n < m$.
        \item Any line in the plane is a vector subspace of $\R^2$.
        \item A line in three-dimensional space is not a vector subspace of $\R^2$ but is a vector subspace of $\R^3$.
        \item Two-dimensional planes of the form $\spann(\vv, \ww)$, for nonzero $\vv, \ww \in \R^3$, are not vector subspaces of $\R^2$ but are vector subspaces of $\R^3$.
        \item Two-dimensional planes of the form $\spann(\vv, \ww)$, for nonzero $\vv, \ww \in \R^4$, are not vector subspaces of $\R^3$ but are vector subspaces of $\R^4$.
        \item ``$k$-dimensional planes'' of the form $\spann(\{\vv_1, ..., \vv_k\})$, for nonzero $\vv_1, ..., \vv_k \in \R^n$, are not vector subspaces of $\R^k$ for $k < n$, but are vector subspaces of $\R^n$.
    \end{itemize}
\end{remark}

\newpage

\subsection*{Linear independence}

In this subsection, we introduce the concept of \textit{linear independence}, which will serve as formal footing that can be used to justify future geometric intuitions.

\begin{defn}
    (Linear independence).
    
    Let $V$ be a vector space. If $S = \{\vv_1, ..., \vv_k\}$ is a finite subset of $V$, then we say the vectors in $S$ are \textit{linearly independent} from each other iff for all $\vv \in S$ we have $\vv \notin \spann(S - \{\vv\})$.
    
    Intuitively, the vectors in $S$ are linearly independent if every vector inside $S$ is required to produce $\spann(S)$. Indeed, we prove in Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets} that removing a vector from a linearly independent set produces a set whose span is a proper subset of what it was before.
    
    Vectors are said to be \textit{linearly dependent} on each other iff they are not linearly independent from each other.
\end{defn}

\begin{remark}
    ($\emptyset$ is linearly independent). The empty set $\emptyset$ is linearly independent by false hypothesis.
\end{remark}

\begin{theorem}
    Any subset of a vector space that contains $\mathbf{0}$ is linearly dependent.
\end{theorem}

\begin{proof}
    If a subset $S$ of a vector space contains $\mathbf{0}$, then the condition $(\forall \vv \in S \spc \vv \notin \spann(S - \{\vv\}))$ is violated by $\vv = \mathbf{0}$, since $\mathbf{0}$ is in the span of every set.
\end{proof}

\begin{theorem}
    (Condition for linear independence).
    
    Let $V$ be a vector space over a field $K$. A subset $\{\vv_1, ..., \vv_k\} \subseteq V$ is linearly dependent iff there are scalars $c_1, ..., c_k \in K$ not all $0$ such that
    
    \begin{align*}
        c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}.
    \end{align*}
\end{theorem}

\begin{proof}
    Set $S := \{\vv_1, ..., \vv_k\}$.
    
   ($\implies$). Assume $S$ is linearly dependent. Then $S$ is not linearly independent, so there is some $i$ for which ${\vv_i \in \spann(S - \{\vv_i\})} = \spann(\{\vv_1, ..., \cancel{\vv_i}, ..., \vv_k\})$. This makes $\vv_i$ a linear combination of $\vv_1, ..., \cancel{\vv_i}, ..., \vv_k$, ${\vv_i = c_1 \vv_1 + ... + \cancel{c_i \vv_i} + ... + c_k \vv_k}$, where $c_j \in K$ for $j \in [1, k]$. Subtracting $\vv_i$ from both sides of the previous equation, we have ${(c_1 \vv_1 + ... + c_{i - 1} \vv_{i - 1}) - \vv_i + (c_{i + 1} \vv_{i + 1} + ... + c_k \vv_k) = \mathbf{0}}$, i.e., we have ${d_1 \vv_1 + ... + d_k \vv_k = \mathbf{0}}$, where $d_1 = c_1, ..., d_{i - 1} = c_{i - 1}, d_i = -1, d_{i + 1} = c_{i + 1}, ..., d_k = c_k$. Thus, as desired, there are $d_1, ..., d_k \in K$ not all zero such that $d_1 \vv_1 + ... + d_k \vv_k = \mathbf{0}$.
   
   ($\impliedby$). Assume there are $c_1, ..., c_k \in K$ not all zero such that $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$. Then $c_i \vv_i = c_1 \vv_1 + ... + \cancel{c_i \vv_i} + ... + c_k \vv_k$. Since $c_i \neq 0$, we can divide by $c_i$ to obtain $\vv_i = -\frac{c_1}{c_i} \vv_1 - ... - \cancel{\frac{c_i}{c_i}\vv_i} - ... - \frac{c_k}{c_i}\vv_k$. Thus $\vv_i \in \spann(\{\vv_1, ..., \cancel{\vv_i}, ..., \vv_k\}) = \spann(S - \{\vv_i\})$; $\vv_i$ violates the linear independence condition, so $S$ is linearly dependent.
\end{proof}

\begin{lemma}
    \label{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}
    (Adding and removing vectors from linearly independent and dependent sets).

    \begin{enumerate}
        \item Appending a vector to a linearly independent set produces...
        \begin{enumerate}
            \item[1.1.] another linearly independent set, if the vector isn't in the span of the original set.
            \item[1.2.] a linearly dependent set, if the vector is in the span of the original set.
        \end{enumerate}
        \item Removing a vector from a linearly independent set produces another linearly independent set.
        \item Appending a vector to a linearly dependent set produces another linearly dependent set.
        \item Removing a vector from a linearly dependent set does not change the span of that set.
    \end{enumerate}
\end{lemma}

\begin{proof}
   Left as exercise.
\end{proof}


\newpage

\subsection*{Basis and dimension}

In this subsection, we define the concepts of \textit{basis} and \textit{dimension}. A \textit{basis} for a vector space will be defined to be a spanning set of that vector space with as few vectors in it as possible; a basis is a ``minimal spanning set''. (We will come to see that this minimality is equivalent to linear independence). The \textit{dimension} of a \textit{finite-dimensional} vector space will be defined to be the number of vectors in any basis for it.

\begin{defn}
    (Finite- and infinite- dimensionality).
    
    A vector space is said to be \textit{finite-dimensional} iff it is spanned by a finite set of vectors and is said to be \textit{infinite-dimensional} iff this is not the case.
\end{defn}

\begin{defn}
    (Basis for a finite-dimensional vector space). 
    
    Let $V$ be a finite-dimensional vector space. We say that a subset $E \subseteq V$ is a \textit{basis} for $V$ iff
    
    \begin{enumerate}
        \item $E$ spans $V$.
        \item $E$ is minimal: if $F$ is another set of vectors spanning $V$, then $|F| \geq |E|$.
    \end{enumerate}
\end{defn}

\begin{defn}
    (Dimension of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space. The \textit{dimension} $\dim(V)$ of $V$ is the number of basis vectors in a basis for $V$.
\end{defn}

\begin{theorem}
    Every finite-dimensional vector space has a basis.
\end{theorem}

\begin{proof}
    Let $V$ be a finite-dimensional vector space. Let $C = \{S \mid \spann(S) = V \text{ and $S$ is finite} \}$. By the well-ordering principle, we can select $E$ to be the element of minimal size from $C$. This $E$ is a minimal spanning set for $V$ and thus a basis for $V$.
\end{proof}

\begin{remark}
\label{ch::lin_alg::rmk::zero_dim_vector_space}
    (The $0$-dimensional vector space). 
    
    The empty set $\emptyset$ spans $\{\mathbf{0}\}$ because [...]. It is also a minimal spanning set because there are no sets ``smaller'' than $\emptyset$. Therefore $\emptyset$ is a basis for $\{\mathbf{0}\}$, and so $\{\mathbf{0}\}$ is a zero-dimensional vector space. Since $\emptyset$ is the only set with cardinality zero, it is the only basis with cardinality zero; thus, $\{\mathbf{0}\}$ is the only $0$-dimensional vector space.
\end{remark}

The following two lemmas enable us to obtain a more useful characterization of bases.

\begin{lemma}
\label{ch::lin_alg::lemma::n_lin_indep_vectors_span_n_dim_space}
    ($n$ linearly independent vectors taken from the same $n$-dimensional vector space span that vector space).

    Let $V$ be a finite-dimensional vector space. If a finite set $S \subseteq V$ of vectors is linearly independent and $|S| = \dim(V)$, then $S$ spans $V$.
\end{lemma}

\begin{proof}
     % From p. 30 of Alan MacDonald's Geometric and Linear Algebra

   Suppose $S = \{\vv_1, ..., \vv_n\}$ is linearly independent and that $n = \dim(V)$. We immediately see that $V = \{\mathbf{0}\}$ and that $\spann(S) = \spann(\emptyset) = \{\mathbf{0}\} = V$ if $n = \dim(V) = 0$, so assume $n > 0$.
   
   Let $K$ be the field over which $V$ is taken to be a vector space. Since $V$ is $n$-dimensional, we can take a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$. Since $\spann(E) = V$, then there are $c_1, ..., c_n \in K$ such that ${\vv_1 = c_1 \ee_1 + ... + c_n \ee_n}$. Since $S$ is linearly independent, then $\vv_1 \neq \mathbf{0}$, and some $c_i$ must be nonzero. Since we can divide by $c_i$, we can solve for $\ee_1$ in terms of $c_1, ..., c_n, \vv_1, \ee_2, ..., \ee_n$, and conclude $\ee_1 \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$. 
   
   Now let $\vv \in V$. Again because $\spann(E) = V$, there are $d_1, ..., d_n \in K$ such that $\vv = d_1 \ee_1 + ... + d_n \ee_n$. Since $\ee_1 \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$, we can express $\ee_1$ as a linear combination of $\vv_1, \ee_2, ..., \ee_n$; doing this, we see $\vv$ is a linear combination of $\vv_1, \ee_2, ..., \ee_n$. Thus $\vv \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$. Since $\vv$ is arbitrary, then ${V \subseteq \spann(\{\vv_1, \ee_1, ..., \ee_n\})}$, which implies $V = \spann(\{\vv_1, \ee_1, ..., \ee_n\})$.

    Repeating this process inductively, we can conclude that $V = \spann(\{\vv_1, \vv_2, \ee_3, ..., \ee_n\})$, and ultimately that $V = \spann(\{\vv_1, ..., \vv_n\})$. 
\end{proof}

\begin{lemma}
\label{ch::lin_alg::lemma::dimension}
    (Linearly independent spanning set lemma). 
    
    For any $n$-dimensional vector space $V$ with $n \neq 0$, we have the following equivalent facts:
    
    \begin{enumerate}
        \item A set of more than $n$ vectors from $V$ is linearly dependent.
        \item A set of less than $n$ vectors from $V$ does not span $V$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    First we show the logical equivalence.
    
    ($(1) \implies (2)$). Suppose for contradiction that a set $S$ of vectors with $|S| < n$ spans $V$. Remove vectors from $S$ until a linearly independent set $T$ is obtained. By item 4 of Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}, removing vectors from linearly dependent sets does not change their span, so $T$ spans $V$. Thus, $T$ is a linearly independent spanning set with $|T| < n$. Since $n > |T|$, it follows from $(1)$ that $T$ is linearly dependent; contradiction.
    
    ($(2) \implies (1)$). Use a similar argument that invokes item 1.1 of Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}.

    Now we show (1) is true. Consider a set $S = \{\vv_1, ..., \vv_n, \vv_{n + 1}, ..., \vv_k\} \subseteq V$ of more than $n$ vectors from $V$. By the previous lemma, we know $\{\vv_1, ..., \vv_n\}$ spans $V$. Thus for each $i \in \{n + 1, ..., k\}$, we have ${\vv_i \in \spann(\{\vv_1, ..., \vv_n\}) \subseteq \spann(S - \{\vv_i\})}$. Since there is a $\vv \in S$ such that $\vv \in \spann(S - \{\vv\})$, then $S$ is linearly dependent.   
\end{proof}

\begin{theorem}
    (Bases for finite-dimensional vector spaces are linearly independent spanning sets).
    
    Let $V$ be a finite-dimensional vector space. A set $E$ of vectors is a basis for $V$ iff
    
    \begin{enumerate}
        \item $E$ spans $V$.
        \item $E$ is linearly independent.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    We show $(\text{$E$ is a minimal spanning set of $V$}) \iff (\text{$E$ is linearly independent})$.
    
    (Case $E \neq \emptyset$).
    
    \indent ($\implies$). Suppose that $E$ is a minimal spanning set of $V$. We need to show that $E$ is linearly independent, so suppose for contradiction that $E$ is linearly dependent. If $E$ is linearly dependent, then by item 4 from Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets} we can remove a vector from $E$ without changing $\spann(E) = V$, so, $E$ is not minimal; contradiction.
    
    \indent ($\impliedby$). Suppose that $E$ spans $V$ and that $E$ is linearly independent. Since $E$ spans $V$, item 2 from the previous lemma
    implies $|E| \geq \dim(V)$; since $E$ is linearly independent, item 1 from previous lemma implies $|E| \leq \dim(V)$. Therefore $|E| = \dim(V)$. Now we can see the assertion ``if $F$ is a spanning set of $V$ then $|F| \geq |E|$'' is true. If $F$ is a spanning set of $V$, then item $2$ of the previous lemma implies $|F| \geq \dim(V) = |E|$.
    
    (Case $E = \emptyset$). Since there is only one $E$ satisfying $E = \emptyset$, the if-and-only-if we need to show simplifies to an \textit{and} statement: $(\text{$\emptyset$ is a minimal spanning set of $V$}) \text{ and } (\text{$\emptyset$ is linearly independent})$. We discussed how the first statement is true in Remark \ref{ch::lin_alg::rmk::zero_dim_vector_space}. The second statement, which is equivalent to $(\forall \vv \in \emptyset \spc \vv \notin \spann(\emptyset - \{\vv\}))$, is true by false hypothesis, since\footnote{$(\forall \vv \in \emptyset \spc \vv \notin \spann(\emptyset - \{\vv\}))$ is equivalent to $(\forall \vv \spc \vv \in \emptyset \implies \vv \notin \spann(\emptyset - \{\vv\}))$. Because $\vv \in \emptyset$ is false for every $\vv \in \emptyset$, the overall implication $\vv \in \emptyset \implies \vv \notin \spann(\emptyset - \{\vv\})$ is true for every $\vv \in \emptyset$.} there is no $\vv$ for which ``$\vv \in \emptyset$'' is a true statement.
    
    
\end{proof}

Because the minimality condition in the definition we gave for ``basis'' boils down to a comparison of cardinalities of spanning sets, it is only applicable to finite-dimensional vector spaces. The above theorem suggests a more general definition that applies to all vector spaces, however.

\begin{defn}
    (Basis).
    
    Let $V$ be a vector space. We say that a set $E$ of vectors is a \textit{basis} for $V$ iff
   
   \begin{enumerate}
        \item $E$ spans $V$.
        \item $E$ is linearly independent.
    \end{enumerate}
\end{defn}

\begin{remark}
    (No need for the well-ordering principle!).

    With this new definition of ``basis'', it is now possible to prove that every finite-dimensional vector space has a basis without using the well-ordering principle, and thus without the axiom of choice\footnote{The axiom of choice is equivalent to the well-ordering principle.}. This is because linear independence is in some sense a ``local'' property (to test if a set $S$ is linearly independent, one only needs to know $S$), while the minimal spanning condition of before is a ``global'' property (to test if a set $S$ is a minimal spanning set, one needs to compare against all other spanning sets). Here's an outline of the proof: given a spanning set $S$ of a vector space, a linearly independent spanning set (a basis) can be obtained by removing vectors from $S$.
\end{remark}

\begin{defn}
    (Standard basis for $K^n$).
    
    Let $K$ be a field, and consider $K^n$ as a vector space over $K$. We define the \textit{standard basis} of $K^n$ to be the basis $\sE = \{\see_1, ..., \see_n\}$, where the $j$th entry of $\see_i$ is $1$ when $j = i$ and $0$ otherwise.
\end{defn}

\begin{proof}
   We should check that $\sE$ is indeed a basis. Checking that $\sE$ spans $K^n$ is easy and is left as an exercise.
   
   For linear independence, consider the equation $c_1 \see_1 + ... + c_n \see_n = \mathbf{0}$, where $c_1, ..., c_n \in K$. The equation can be rewritten as $\begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} = \mathbf{0}$, which is true only when $c_i = 0$ for all $i$. In all we have that $c_1 \see_1 + ... + c_n \see_n = \mathbf{0}$ only when $c_1, ..., c_n = 0$, so $\see_1, ..., \see_n$ are linearly independent.
\end{proof}

\begin{remark}
    (Why not define dimensionality in terms of bases?). 
    
    It is tempting to define a finite-dimensional vector spaces to be those that have finite bases. This definition would be equivalent to the one we've put in place as far as finite-dimensional vector spaces are concerned, but it becomes problematic for infinite-dimensional vector spaces. If we take the Axiom of Choice to be false, then not all vector spaces spanned by an infinite number of vectors have a basis. Thus, if we defined ``infinite-dimensional'' to mean ``has an infinite basis'', then, assuming the Axiom of Choice is false, not all vector spaces spanned by an infinite number of vectors would be classified as ``infinite-dimensional''! For this reason, it is best to define an infinite-dimensional vector space to be one spanned by an infinite number of vectors rather than one that has as an infinite basis.
\end{remark}

\begin{remark}
    (Vector spaces that \textit{don't} have bases?).
    
    The statement ``every vector space, including infinite-dimensional vector spaces, has a basis'' is equivalent to the Axiom of Choice.
\end{remark}

\newpage

\section{Linear functions}

At the beginning of the chapter, it was said that the two fundamental ideas underlying linear algebra are those of ``objects that behave like vectors'' and of ``functions that preserve the decomposition of their input vectors''. We learned that ``objects that behave like vectors'' are elements of vector spaces. Now, we will investigate ``functions that preserve the decomposition of their input vectors'', which are more formally referred to as \textit{linear functions}.

\begin{defn}
\label{ch::lin_alg::defn::linear_function_intuitive}
    (Linear function). 
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is said to be \textit{linear} iff we have $\ff(c_1 \vv_1 + c_2 \vv_2) = c_1 \ff(\vv_1) + c_2 \ff(\vv_2)$ for all $c_1, c_2 \in K$ and $\vv_1, \vv_2 \in V$.
    
    What does this definition really mean, though? It may help understanding to introduce the notation $\ww_1 := \ff(\vv_1)$ and $\ww_2 := \ff(\vv_2)$. With this notation, we see that $\ff$ is linear iff we have the following for every $c_1, c_2 \in K$ and $\vv_1, \vv_2 \in V$:
    
    \begin{align*}
        \vv_1 \overset{\ff}{\mapsto} \ww_1 &\text{ and } \vv_2 \overset{\ff}{\mapsto} \ww_2 \\
        &\implies \\
        c_1 \vv_1 + c_2 \vv_2 &\overset{\ff}{\mapsto} c_1 \ww_1 + c_2 \ww_2
    \end{align*}
    
    So, roughly speaking, $\ff$ is linear iff elements in $V$ ``interact'' in the same way as do their corresponding elements in the image $\ff(V)$.
    
    Linear functions are also referred to as \textit{linear transformations}, \textit{linear operators}, or \textit{linear maps}. We will stick with the terminology ``linear function''.
\end{defn}

\begin{remark}
    (``Linear'' in ``linear function'' means ``vector'').
    
    Initially, one might be confused as to what is actually ``linear'' about a linear function, and ask something like, ``What do linear functions have to do with lines?'' The answer is that the ``linear'' in ``linear function'' is meant to connote ``linear element''. (Recall that elements of vector spaces are also sometimes called ``linear elements''). Linear functions are called such because they are the functions that play nicely with linear elements. A better name for ``linear function'' would be ``vector-respecting function''.
\end{remark}

We now quickly present an slightly alternative characterization of linear functions. This is the characterization that is most often used for the definition of a linear function in other texts, and is often easier to check than the above definition in practice.

\begin{theorem}
    (The most common characterization of linear functions).
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is linear iff
    
    \begin{enumerate}
        \item $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ for all $\vv, \ww \in V$.
        \item $\ff(c\vv) = c\ff(\vv)$ for all $c \in K$ and $\vv \in V$.
    \end{enumerate}
\end{theorem}

\begin{proof}
   Left as exercise.
\end{proof}

This characterization of linear functions quickly generalizes to the following fact.

\begin{theorem}
    (Generalization of the most common characterization of linear functions).
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is linear iff
    
    \begin{align*}
        \ff \Big(\sum_{i = 1}^n c_i \vv_i \Big) = \sum_{i = 1}^n c_i \ff(\vv_i) \text{ for all } c_1, ..., c_n \in K \text{ and } \vv_1, ..., \vv_n \in V.
    \end{align*}
\end{theorem}

\begin{proof}
    Left as exercise.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::linear_functions_have_zero_in_image}
    (Every linear function includes $\mathbf{0}$ in its image).
    
    Every linear function includes $\mathbf{0}$ in its image because $\ff(0 \cdot \vv) = 0 \cdot \ff(\vv) = \mathbf{0}$ for linear function $\ff$ and any vector $\vv$.
    
    Somewhat counterintuitively, this means that only lines that pass through the origin $\mathbf{0}$ can be images of linear functions.
\end{theorem}

\begin{theorem}
\label{ch::lin_alg::thm::basis_sent_to_any_ordered_list}
    (Any basis can get sent to any list of vectors by some linear function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces and let $E = \{\ee_1, ..., \ee_n\}$ be a basis of $V$. For all ${\ww_1, ..., \ww_n \in W}$, there exists a linear function sending $\ee_i \mapsto \ww_i$.
\end{theorem}

\begin{proof}
    For any linear function $\ff$, we have $\ff(\vv) = \sum_{i = 1}^n ([\vv]_E)_i \ff(\ee_i)$. We want to construct a linear function $\gg$ with $\gg(\ee_i) = \ww_i$. Thinking about replacing the $\ff(\ee_i)$ in the previous sum with $\ww_i$ gives us the idea to define $\gg(\vv) := \sum_{i = 1}^n ([\vv]_E)_i \ww_i$. It is straightforward to check that we indeed have $\gg(\ee_i) = \ww_i$ and that $\gg$ is linear.
\end{proof}

Now that we are familiar with theoretical characterizations of linear functions, we will investigate linear functions from a geometric perspective.

\begin{remark}
    (Examples of linear functions $\R^2 \rightarrow \R^2$).
    
    The following are examples of linear functions from $\R^2$ to $\R^2$:
    
    \begin{itemize}
        \item rotations about the origin
        \item reflection across a line through the origin
        \item projection onto a line through the origin
    \end{itemize}
    
    In order to convince yourself that the above functions are linear, check that each above function $\ff$ satisfies $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ and $\ff(c \vv) = c \vv$.
\end{remark}

The following theorem gives further geometric intuition for how linear functions behave.

\begin{theorem}
    Linear functions $\R^n \rightarrow \R^n$ fix the origin and keep parallel lines parallel.
\end{theorem}

\begin{proof}
   Let $\ff$ be a linear function $\R^n \rightarrow \R^n$. Theorem \ref{ch::lin_alg::thm::linear_functions_have_zero_in_image} showed that $\ff(\mathbf{0}) = \mathbf{0}$, so it remains to show that $\ff$ sends parallel lines to parallel lines. 
   
   Consider two parallel lines in $\R^n$ described by $\eell_1(t) = \vv_0 + t\vv$ and $\eell_2(t) = \ww_0 + t\vv$. We have $\ff(\eell_1(t)) = \ff(\vv_0) + t \ff(\vv)$ and $\ff(\eell_2(t)) = \ff(\ww_0) + t \ff(\vv)$. These transformed lines are parallel because they have the same direction vector, $\ff(\vv)$.
\end{proof}

Now that we are somewhat familiar with linear functions, we present a more abstract result, which formalizes the way in which we can think of linear functions as themselves being vectors.

\begin{theorem}
\label{ch::lin_alg::thm::vector_space_linear_functions}
    (Vector space of linear functions).

    Let $V$ and $W$ be vector spaces over a field $K$. The set $\LLLL(V \rightarrow W)$ of linear functions $V \rightarrow W$, taken together with $+:V \rightarrow V \rightarrow V$ being function addition and $\cdot:V \rightarrow K \rightarrow V$ being function scalar multiplication, is a vector space over $K$.
\end{theorem}

\begin{proof}
    According to Theorem \ref{ch::lin_alg::thm::vector_space_alt_characterization}, we need to show that $\LLLL(V \rightarrow W)$ is a vector-containing set and that $\LLLL(V \rightarrow W)$ is closed under vector addition and vector scaling. We leave showing that $\LLLL(V \rightarrow W)$ is a vector-containing set as an exercise, and show the other two conditions.

    \begin{itemize}
        \item (Closure under vector addition). Assume $\ff, \gg \in \LLLL(V \rightarrow W)$, and consider $\ff + \gg$. We need to show $(\ff + \gg) \in \LLLL(V \rightarrow W)$; that is, we need to show (1) that $(\ff + \gg)(\vv + \ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww)$ for all $\vv, \ww \in V$ and (2) that $(\ff + \gg)(c\vv) = c(\ff + \gg)(\vv)$ for all $\vv \in V$ and $c \in K$.
        \begin{itemize}
            \item[1.] We have $(\ff + \gg)(\vv + \ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww) = \ff(\vv) + \gg(\vv) + \ff(\ww) + \gg(\ww) = \ff(\vv) + \gg(\vv) + \ff(\ww) + \gg(\ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww)$.
            \item[2.] We have $(\ff + \gg)(c\vv) = \ff(c\vv) + \gg(c\vv) = c\ff(\vv) + c\gg(\vv) = c(\ff(\vv) + \gg(\vv)) = c(\ff + \gg)(\vv)$.
        \end{itemize}
        \item (Closure under vector scaling). Assume $\ff \in \LLLL(V \rightarrow W)$ and $c \in K$. We need to show $c\ff \in \LLLL(V \rightarrow W)$; that is, we need to show (1) that $(c\ff)(\vv + \ww) = (c\ff)(\vv) + (c\ff)(\ww)$ for all $\vv, \ww \in V$ and (2) that $(c\ff)(d\vv) = d(c\ff)(\vv)$ for all $d \in K$ and $\vv \in V$.
        \begin{enumerate}
            \item[1.] We have $(c\ff)(\vv + \ww) = c \ff(\vv + \ww) = c (\ff(\vv) + \ff(\ww)) = c \ff(\vv) + c \ff(\ww) = (c\ff)(\vv) + (c\ff)(\ww)$.
            \item[2.] We have $(c\ff)(d\vv) = c\ff(d\vv) = cd\ff(\vv) = c(d\ff(\vv)) = c(d\ff)(\vv)$.
        \end{enumerate}
    \end{itemize}
\end{proof}

\newpage

\subsection*{Kernel and image of linear functions}

\begin{defn}
    (Kernel, image of a linear function).
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. The \textit{kernel} of $\ff$ is the set of all vectors that get sent to $\mathbf{0}$ by $\ff$:
    
    \begin{align*}
        \ker(\ff) := \ff^{-1}(\{\mathbf{0}\}) = \{ \vv \in V \mid \ff(\vv) = \mathbf{0} \}.
    \end{align*}
    
    The \textit{image} of $\ff$ is the set of all vectors that are mapped to by $\ff$:
    
    \begin{align*}
        \im(\ff) := \ff(V) = \{ \ww \in W \mid \exists \vv \in V \spc \ww = \ff(\vv) \}.
    \end{align*}
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::rank}
    (Rank of a linear function). 
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. The \textit{rank} of $\ff$ is defined to be $\dim(\ff(V))$, the dimension of the image of $\ff$.
\end{defn}

\begin{theorem}
    (Kernel and image are vector subspaces). 
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. The kernel of $\ff$ is a vector subspace of $V$ and the image of $\ff$ is a vector subspace of $W$.
\end{theorem}

\begin{proof}
   Left as exercise.
\end{proof}

\begin{defn}
    (Trivial kernel).
    
    Since $\{\mathbf{0}\}$ is the smallest (in the sense of set-containment) kernel possible for a linear function, we say that the kernel of a linear function is \textit{trivial} iff it is equal to $\{\mathbf{0}\}$.
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::linear_fn_1-1_trivial_kernel}
    (One-to-one linear functions are the linear functions that have trivial kernels). 
    
    Let $V$ and $W$ be vector spaces. A linear function $\ff:V \rightarrow W$ is one-to-one iff $\ff^{-1}(\{\mathbf{0}\}) = \{\mathbf{0}\}$. That is, a linear function is one-to-one iff it has a trivial kernel.
\end{theorem}

\begin{proof}
    We use the contrapositive and prove that $\ff$ has a nontrivial kernel iff it is not one-to-one.
    
    ($\implies$). $\ff$ has a nontrivial kernel $\iff$ there is a nonzero $\vv \in V$ for which $\ff(\vv) = \mathbf{0} \implies$ for any $\vv_1 \in V$ we have $\ff(\vv_1 + \vv) = \ff(\vv_1) + \ff(\vv) = \ff(\vv_1) + \mathbf{0} = \ff(\vv_1) \implies \ff$ is not one-to-one.
    
    ($\impliedby$). $\ff$ is not one-to-one $\iff$ for some $\vv_1, \vv_2 \in V$ with $\vv_1 \neq \vv_2$ we have $\ff(\vv_1) = \ff(\vv_2) \implies \ff(\vv_1) - \ff(\vv_2) = \ff(\vv_1 - \vv_2) = \mathbf{0}$. Since $\vv_1 \neq \vv_2$, we know $\vv_1 - \vv_2 \neq \mathbf{0}$, and so $\ff$ has a nontrivial kernel.
\end{proof}

\begin{remark}
    The idea in the above proof is that vectors in the preimage of every $\ww \in W$ ``differ by an element of the kernel''. You could prove the following fact to formalize this further: $\vv_1, \vv_2 \in \ff^{-1}(\ww)$ for some $\ww \in \ff(V)$ if and only if $\vv_2 = \vv_1 + \vv$, where $\vv_1 \in V$ and $\vv \in \ff^{-1}(\{\mathbf{0}\})$.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::lemma::only_inv_fns_preserve_lin_indep}

    (One-to-one linear functions are the linear functions that preserve linear independence). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces. A linear function $\ff:V \rightarrow W$ preserves the linear independence of vectors iff it is one-to-one. That is, 
    
    \begin{align*}
       (\vv_1, ..., \vv_k \text{ are linearly independent}) &\implies (\ff(\vv_1), ..., \ff(\vv_k) \text{ are linearly independent})\\
        \text{if} \text{ } &\text{and only if} \\
        \ff \text{ } &\text{is one-to-one}
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \indent ($\impliedby$). Suppose that $\ff$ is one-to-one and that $\vv_1, ..., \vv_k$ are linearly independent. Since $\ff$ is one-to-one, it has a trivial kernel, and thus for any $c_1, ..., c_k \in K$ we have that $\ff(c_1 \vv_1 + ... + c_k \vv_k) = \mathbf{0}$ implies $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$. Since $\vv_1, ..., \vv_k$ are linearly independent, $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$ implies that the $c_i$'s are all $0$. In all, we have that ``$\ff(c_1 \vv_1 + ... + c_k \vv_k) = \mathbf{0}$ implies the $c_i$'s are all $0$''. Since $\ff$ is linear, this statement becomes  ``$c_1 \ff(\vv_1) + ... + c_k \ff(\vv_k) = \mathbf{0}$ implies the $c_i$'s are all $0$''. Thus $\ff(\vv_1), ..., \ff(\vv_k)$ are linearly independent, as claimed.
    
    ($\implies$). Suppose that if $\vv_1, ..., \vv_k$ are linearly independent, then $\ff(\vv_1), ..., \ff(\vv_k)$ are linearly independent. We need to show $\ff$ is one-to-one; it suffices to show that $\ff$ has a trivial kernel. Let $\vv \in \ff^{-1}(\{\mathbf{0}\})$, so $\ff(\vv) = \mathbf{0}$. We want to show $\vv = \mathbf{0}$. 
    
    Since $V$ is finite-dimensional, there is a basis $E = \{\ee_1, ..., \ee_n\}$ for $V$. Expressing $\vv$ relative to $E$, we have $\ff(\vv) = \ff\Big(\sum_{i = 1}^k ([\vv]_E)_i \ee_i\Big) = \sum_{i = 1}^k ([\vv]_E)_i \ff(\ee_i) = \mathbf{0}$. The hypothesis implies that $\ff(\ee_1), ..., \ff(\ee_n)$ are linearly independent, so $([\vv]_E)_i = 0$ for all $i$ is the only solution to $\sum_{i = 1}^k ([\vv]_E)_i \ff(\ee_i) = \mathbf{0}$. Thus $([\vv]_E)_i = 0$ for all $i$, i.e., $\vv = \mathbf{0}$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::main_dim}
    (Main dimension theorem).
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. If $V$ is finite-dimensional, then $\ff^{-1}(\{\mathbf{0}\})$ and $\ff(V)$ are also finite-dimensional, and we have

    \begin{align*}
        \dim(\ff(V)) = \dim(V) - \dim(\ff^{-1}(\{\mathbf{0}\})).
    \end{align*}
    
    Also, if $\ff^{-1}(\{\mathbf{0}\})$ and $\ff(V)$ are finite-dimensional, then $V$ must be finite-dimensional, and the same relationship with dimensions holds.
    
    This result is commonly called the \textit{rank-nullity theorem}.
\end{theorem}

\begin{proof}
    We prove the first part of the theorem (before ``Also'').
    
    If $V$ is finite-dimensional, then $\ff^{-1}(\{\mathbf{0}\})$ is also finite-dimensional since $\ff^{-1}(\{\mathbf{0}\}) \subseteq V$, so we can choose a basis $\{\ee_1, ..., \ee_k\}$ for $\ff^{-1}(\{\mathbf{0}\})$. Using the uniqueness of dimension and Lemma \ref{ch::lin_alg::lemma::dimension}, one can show \textbf{add footnote} that it is possible to add vectors $\ee_{k + 1}, ..., \ee_n$ to this basis so that it becomes $\{\ee_1, ..., \ee_k, \ee_{k + 1}, ..., \ee_n\}$, a basis for $V$. Since $\dim(\ff^{-1}(\{\mathbf{0}\})) = k$ and $\dim(V) = n$, we want to show $\dim(\ff(V)) = \dim(V) - \dim(\ff^{-1}(\{\mathbf{0}\})) = n - k$; we want to show $\dim(\ff(V)) = n - k$.

    Since $\ff:V \rightarrow W$ is linear, we have ${\ff(\vv) = ([\vv]_E)_1 \ff(\ee_1) + ... + ([\vv]_E)_k \ff(\ee_k) + ([\vv]_E)_{k + 1} \ff(\ee_{k + 1}) + ... + ([\vv]_E)_n \ff(\ee_n)}$. Because $\ee_1, ..., \ee_k \in \ff^{-1}(\{\mathbf{0}\})$, this simplifies to ${\ff(\vv) = ([\vv]_E)_{k + 1} \ff(\ee_{k + 1}) + ... + ([\vv]_E)_n \ff(\ee_n)}$. 
    
    Therefore, any $\ww \in \ff(V)$ is in the span of $\{\ee_{k + 1}, ..., \ee_n\}$. We will show that $\{\ee_{k + 1}, ..., \ee_n\}$ is a basis for $\ff(V)$. Once know this, then, since there are $n - k$ of these vectors, we have shown $\dim(\ff(V)) = n - k$, which is what we want.
    
    It remains to show $\{\ee_{k + 1}, ..., \ee_n\}$ is a linearly independent set. Suppose for the sake of contradiction it's linearly dependent, i.e., that there are $c_{k + 1}, ..., c_n$ not all zero such that $c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = \mathbf{0}$. By the linearity of $\ff$, this is equivalent with $\ff(c_{k + 1} \ee_{k + 1} + ... + c_n \ee_n) = \mathbf{0}$ for some $c_i$'s not all zero. Thus $c_{k + 1} \ee_{k + 1} + ... + c_n \ee_n \in \ff^{-1}(\{\mathbf{0}\}) = \spann(\{\ee_1, ..., \ee_k\})$, which means $c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = d_1 \ee_1 + ... + d_k \ee_k$ for some $c_i$'s and $d_i$'s not all zero. Then $-(d_1 \ee_1 + ... + d_k \ee_k) + c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = 0$ for some $c_i$'s and $d_i$'s not all zero. But $\{\ee_1, ..., \ee_n\}$ is a basis for $V$, so this cannot happen. Thus $\ff(\ee_{k + 1}), ..., \ff(\ee_n)$ are linearly independent.
\end{proof}

\subsection*{Linear isomorphisms}

\begin{defn}
\label{ch::lin_alg::defn::linear_iso}
    (Linear isomorphism).
    
    Let $V$ and $W$ be vector spaces over a field $K$. Iff $\ff:V \rightarrow W$ is an invertible linear function, i.e. iff it is a bijection\footnote{Recall from Theorem \ref{ch::logic_pf_fns::thm::invertible_iff_bijection} that any (not necessarily linear) function is invertible iff it is a bijection.}, then it is called a \textit{linear isomorphism}, or an \textit{isomorphism (of vector spaces)}.
    
    Let's quickly explain this terminology. Recall that when we have a linear function $\ff:V \rightarrow W$, then elements in $V$ ``interact'' in the same way as do their corresponding elements in $\ff(V)$. When $\ff$ is also invertible, then $\ff(V)$ is \textit{all} of $W$, so, not only are all interactions in $V$ ``mirrored'' in $W$, but all interactions in $W$ are also mirrored in $V$! Thus, when $\ff$ is linear and invertible, $V$ and $W$ are in some sense the ``same'' vector space. For this reason, when $V$ and $W$ are isomorphic, we often say that an element $\vv \in V$ can be \textit{identified} with an element $\ww \in W$.
    
    We write $V \cong W$ to denote that the vector spaces $V$ and $W$ are isomorphic. Note that $\cong$ is an equivalence relation.
\end{defn}

The following theorem tells us that the inverse of a linear isomorphism is also a linear isomorphism.

\begin{theorem}
    (The inverse of a linear function is also a linear function).
    
    If $\ff:V \rightarrow W$ is an invertible linear function, then the inverse $\ff^{-1}$ is also a linear function.
\end{theorem}

\begin{proof}
     Left as exercise.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::linear_fn_1-1_iff_onto}
    
    (A linear function of finite-dimensional vector spaces of the same dimension is one-to-one iff it is onto).
    
    Let $V$ and $W$ be finite dimensional vector spaces with same dimension, $\dim(V) = \dim(W)$, and let ${\ff:V \rightarrow W}$ be a linear function. Then $\ff$ is one-to-one iff $\ff$ is onto. In other words, $\ff$ is a linear isomorphism iff it is either one-to-one or onto.
\end{theorem}

\begin{proof}
    We use the contrapositive to show that, assuming the hypotheses, $\ff$ is not one-to-one iff it is not onto.
    
    $\ff$ is not one-to-one if and only if it has a nontrivial kernel, i.e., iff $\dim(\ff^{-1}(\{\mathbf{0}\})) > 0$. By the main dimension theorem, this condition is equivalent to $\dim(V) > \dim(\ff(V))$. Since we assumed $\dim(V) = \dim(W)$, this most recent condition is the same as $\dim(W) > \dim(\ff(V))$. One can check that if $Y_1$ and $Y_2$ are subspaces of the same finite-dimensional vector space, then $\dim(Y_1) > \dim(Y_2)$ iff $Y_1 \supsetneq Y_2$. Using $Y_1 = W$ and $Y_2 = \ff(V)$, we obtain the equivalent condition $W \supsetneq \ff(V)$. This is equivalent to $\ff$ not being onto.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::same_dim_iff_isomorphic}
    (Finite-dimensional vector spaces are isomorphic iff they have the same dimension).
    
    Let $V$ and $W$ be finite-dimensional vector spaces. Then there exists a linear isomorphism $V \rightarrow W$ iff $\dim(V) = \dim(W)$.
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    ($\implies$). If $\ff:V \rightarrow W$ is a linear isomorphism, then it has a trivial kernel. The main dimension theorem then implies that $\dim(W) = \dim(V) - \dim(\ff^{-1}(\mathbf{0})) = \dim(V) - 0 = \dim(V)$.
    
    ($\impliedby$). It suffices to show that every $n$-dimensional vector space is isomorphic to $K^n$. So, let $V$ be an $n$-dimensional vector space, and let $E$ be a basis for $V$. In Definition \ref{ch::lin_alg::defn::coordinates_relative_to_basis} we will define the linear function $[\cdot]_E:V \rightarrow K^n$; Theorem \ref{ch::lin_alg::thm::[]E_invertible} will show that $[\cdot]_E$ is a linear isomorphism.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::iso_bases_to_bases}
    (Linear isomorphisms send bases to bases).

    Let $V$ and $W$ be finite-dimensional vector spaces. If $E$ is a basis of $V$ and $\ff:V \rightarrow W$ is a linear isomorphism, then $\ff(E)$ is a basis for $W$.
\end{theorem}

\begin{proof}
    Linear isomorphisms are one-to-one, so they preserve the linear independence of vectors. Thus $\ff(E)$ is a linearly independent set of $\dim(V)$ many vectors. Since $\ff$ is an isomorphism, then $\dim(V) = \dim(W)$, so $\ff(E)$ is a linearly independent set of $\dim(W)$ many vectors. Recall from Lemma \ref{ch::lin_alg::lemma::n_lin_indep_vectors_span_n_dim_space} that a linearly independent set of $n$ vectors from an $n$-dimensional vector space span that space. This makes $\ff(E)$ is a basis for $W$.
\end{proof}

\begin{defn}
\label{ch::lin_alg::defn::natural_iso}
    (Natural linear isomorphism).
    
    Roughly speaking, a linear isomorphism is said to be ``natural'' if it does not depend on a choice of basis. This definition of ``natural'' is not completely technically correct, but it will suffice for our purposes, because the converse (any linear isomorphism which depends on a choice of basis is unnatural) \textit{is} true. To read more about what ``natural'' really means, look up ``natural isomorphism category theory'' online.
\end{defn}

\newpage

\subsection*{Coordinatization of vectors}

\begin{defn}
\label{ch::lin_alg::defn::coordinates_relative_to_basis}
    (Coordinates of a finite-dimensional vector relative to a basis).
    
    Let $V$ be a finite-dimensional vector space over a field $K$, and let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$. Given a vector $\vv \in V$, we define $[\vv]_E$ to be the vector in $K^{\dim(V)}$ that stores the \textit{coordinates of $\vv$ relative to the basis $E$}. Formally, $[\vv]_E$ is the tuple of scalars 
    
    \begin{align*}
        [\vv]_E := \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix} \in K^n
    \end{align*}
    
    for which
    
    \begin{align*}
        \vv = ([\vv]_E)_1 \ee_1 + ... + ([\vv]_E)_n \ee_n.
    \end{align*}
    
    We are guaranteed that such scalars exist because $E$, being a basis for $V$, spans $V$.

    We often use the notation $[\cdot]_E$ to denote the function $V \rightarrow K^{\dim(V)}$ sending $\vv \mapsto [\vv]_E$.
\end{defn}

The above definition does not make it obvious that the function $[\cdot]_E$ is well-defined. We show this now.

\begin{theorem}
    ($[\cdot]_E$ is well-defined).

    Let $V$ be a finite-dimensional vector space. The function $[\cdot]_E$ is well-defined for all bases $E$ of $V$.
\end{theorem}

\begin{proof}
    Consider $\vv, \ww \in V$ that are equal, $\vv = \ww$. We need to show that $[\vv]_E = [\ww]_E$.
    
    Set $n := \dim(V)$. We have $\vv = \sum_{i = 1}^n ([\vv]_E)_i \ee_i$ and $\ww = \sum_{i = 1}^n ([\ww]_E)_i \ee_i$. Since $\vv = \ww$, we can subtract one equation from the other to obtain $\mathbf{0} = \sum_{i = 1}^n \Big(([\vv]_E)_i - ([\ww]_E)_i\Big) \ee_i$. Since $E$ is linearly independent, then we must have $([\vv]_E)_i - ([\ww]_E)_i = 0 \iff ([\vv]_E)_i = ([\ww]_E)_i$ for all $i$. Thus $[\vv]_E = [\ww]_E$.
\end{proof}

Of more practical importance than the above is the following fact.

\begin{theorem}
\label{ch::lin_alg::thm::[]E_invertible}
    ($[\cdot]_E$ is an invertible linear function).
    
    If $V$ is a finite-dimensional vector space with basis $E$, then $[\cdot]_E:V \rightarrow K^{\dim(V)}$ is an invertible linear function.
\end{theorem}

\begin{proof}
    For linearity, we show that $[\vv_1 + \vv_2]_E = [\vv_1]_E + [\vv_2]_E$ and that $[c\vv]_E = c[\vv]_E$.
    
    Let $n := \dim(V)$. We have
        
    \begin{align*}
        [\vv_1 + \vv_2]_E
        &= \Big[\sum_{i = 1}^n ([\vv_1]_E)_i \ee_i + \sum_{i = 1}^n ([\vv_2]_E)_i \ee_i\Big]_E \\
        &= \Big[\sum_{i = 1}^n \Big(([\vv_1]_E)_i + ([\vv_2]_E)_i \Big) \ee_i \Big]_E
        = 
        \begin{pmatrix} ([\vv_1]_E)_1 + ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_1]_E)_m + ([\vv_2]_E)_n \end{pmatrix}
        =
        \begin{pmatrix} ([\vv_1]_E)_1 \\ \vdots \\ ([\vv_1]_E)_n \end{pmatrix}
        +
        \begin{pmatrix} ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_2]_E)_n \end{pmatrix} \\
        &= [\vv_1]_E + [\vv_2]_E.
    \end{align*}
        
    Now we show $[c \vv]_E = c [\vv]_E$. We have

     \begin{align*}
        [c\vv]_E = \Big[ c \Big( \sum_{i = 1 }^n ([\vv]_E)_i \ee_i \Big) \Big] = \Big[\sum_i c([\vv]_E)_i \ee_i\Big]_E = \begin{pmatrix} c([\vv]_E)_1 \\ \vdots \\ c([\vv]_E)_n \end{pmatrix}
        = c \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix}
        = c[\vv]_E.
    \end{align*}
    
    $[\cdot]_E$ is invertible because it sends basis vectors to basis vectors, and therefore preserves linear independence.

    %Alternatively, one can see that $[\cdot]_E$ is linear by checking that it has a trivial kernel: if $[\cdot]_E(\vv) = \mathbf{0}$, then the coordinates of $\vv$ relative to $E$ are all zero, so $\vv = \mathbf{0}$.
\end{proof}

\newpage

\section{Coordinatization of linear functions with matrices}
\label{ch::lin_alg::section::coordinatization_of_linear functions}

\subsection*{Standard matrices}

In the previous section, we saw that when we have a finite-dimensional vector space $V$ with a basis $E$, we can represent any vector $\vv \in V$ by taking its coordinates $[\vv]_E$ relative to $E$. In this section, we discover that when we have finite-dimensional vector spaces $V$, $W$ with respective bases $E$, $F$, we can also coordinatize a linear function $\ff:V \rightarrow W$ by making use of the bases $E$ and $F$. The first step in doing so is to identify said $\ff$ with a linear function $K^{\dim(V)} \rightarrow K^{\dim(W)}$. The next definition shows us how to produce this linear function.

\begin{defn}
    \label{ch::lin_alg::defn::f_EF}
    (Induced linear function from $K^{\dim(V)}$ to $K^{\dim(W)}$).
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$, and let $E$ and $F$ be the respective bases of $V$ and $W$. Whenever we have a linear function $\ff:V \rightarrow W$, there is also an \textit{induced} linear function $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ for which this diagram commutes:
    
    \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRRkAjFVqMWbAGIA9YAB0FULAFsAFOwCUXbrxAZseAkRHlx9Zq0QgA6nr5HBp0mOqWpNuYuVr1tnW5xGCgAc3giUAAzACcIVSQyEBwIJDMJKzZkJQBjKAgcCgB9AFEAAgBeMpKQagY6ACMYBgAFfmMhEBisUIALHAcQWPikACZqFKQAZndJaxBshTyC4ulKsulB4YTEdMnEGYzPEAAVIuAS0mldOsbmtqcTG26+gZ5ouJ2k-fGj+ZOglwgA
        \begin{tikzcd}
            V \arrow[d, "{[\cdot]_E}"'] \arrow[r, "\ff"] & W \arrow[d, "{[\cdot]_F}"] \\
            K^{\dim(V)} \arrow[r, "{\ff_{E,F}}"']            & K^{\dim(W)}
        \end{tikzcd}
    \end{center}
        
    A diagram like the one above is said to ``commute'' iff the compositions of functions corresponding to different paths through the diagram are the same whenever the paths have the same start and end nodes. So, to say that the above diagram commutes is to say that $[\cdot]_F \circ \ff = \ff_{E,F} \circ [\cdot]_E$. That is,
    
    \begin{empheq}[box = \fbox]{align*}
        \ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}.
    \end{empheq}
    
    Concretely, the commutative diagram tells us that we can think of $\ff_{E,F}$ as accepting an input from $K^{\dim(V)}$ that is expressed relative to the basis $E$ for $V$ and producing an output in $K^{\dim(W)}$ that is expressed relative to the basis $F$ for $W$.
\end{defn}

As we continue our investigations into the coordinatization of a linear function $\ff:V \rightarrow W$, we can restrict ourselves to the case where $V = K^n$ and $W = K^m$, where $K$ is the field and $n$ and $m$ are positive integers. This is because any linear function $\ff$ \textit{not} of this form can immediately be identified\footnote{The map $\ff \mapsto \ff_{E,F}$ is a linear isomorphism.} with the linear function $\ff_{E,F}$ that \textit{is} of this form: just send $\ff \mapsto \ff_{E,F}$!

The following derivation shows us how to coordinatize a linear function $K^n \rightarrow K^m$.

\begin{deriv}
\label{ch::lin_alg::deriv::standard_matrix}
    (Standard matrix of a linear function $K^n \rightarrow K^m$). 
    
    Let $K$ be a field and consider a linear function $\ff:K^n \rightarrow K^m$. For any vector $\vv \in K^n$, we have 
    
    \begin{align*}
        \vv =
        \begin{pmatrix} ([\vv]_\sE)_1 \\ \vdots \\ 0 \end{pmatrix} + ... + \begin{pmatrix} 0 \\ \vdots \\ ([\vv]_\sE)_n \end{pmatrix},
    \end{align*}
    
    and so using the linearity of $\ff$, we find that
    
    \begin{align*}
        \ff(\vv) &= 
        \ff
        \Bigg(
            \begin{pmatrix} ([\vv]_\sE)_1 \\ \vdots \\ 0 \end{pmatrix} + ... + \begin{pmatrix} 0 \\ \vdots \\ ([\vv]_\sE)_n \end{pmatrix}
        \Bigg)
        =
        \ff \Bigg( 
        \begin{pmatrix} 
                ([\vv]_\sE)_1 \\ \vdots \\ 0 
        \end{pmatrix} \Bigg)
        +
        ...
        +
        \ff \Bigg(
        \begin{pmatrix} 
                0 \\ \vdots \\ ([\vv]_\sE)_n 
        \end{pmatrix} \Bigg) \\
        &=
        ([\vv]_\sE)_1
        \ff \Bigg(
        \begin{pmatrix} 
                1 \\ 0 \\ \vdots \\ 0 
        \end{pmatrix} \Bigg)
        +
        ...
        +
        ([\vv]_\sE)_n
        \ff \Bigg(
        \begin{pmatrix} 
                0 \\ 0 \\ \vdots \\ 1 
        \end{pmatrix} \Bigg).
    \end{align*}
    
    The above equalities can be written more compactly as
    
    \begin{align*}
        \ff(\vv) = \ff\Big( \sum_{i = 1}^n ([\vv]_\sE)_i \see_i \Big) = \sum_{i = 1}^n \ff(([\vv]_\sE)_i \see_i) = \sum_{i = 1}^n ([\vv]_\sE)_i \ff(\see_i).
    \end{align*}
    
    (Recall that the $j$th component of $\see_i$ is $1$ when $i = j$ and $0$ otherwise, and that $\sE = \{\see_1, ..., \see_n\}$ is the standard basis of $K^n$).
    
    Thinking more about the above, we see that the action of $\ff$ on an arbitrary $\vv \in K^n$ is determined by $\ff(\see_1), ..., \ff(\see_n)$. That is, if we know $\ff(\see_1), ..., \ff(\see_n)$, then we can figure out what $\ff$ is!
    
    Formally, we have discovered a function $\pp$ that takes as input the ordered list $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$, the vector $\vv \in K^n$, and produces $\ff(\vv) \in K^m$ as output:
    
    \begin{align*}
        \pp\Big( \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv \Big) := ([\vv]_E)_1 \ff(\see_1) + ... + ([\vv]_E)_n \ff(\see_n) = \ff(\vv).
    \end{align*}
    
    We turn our attention to the ordered list of column vectors that is an input to $\pp$:
    
    \begin{align*}
        \begin{pmatrix} 
            \ff(\see_1) & \hdots & \ff(\see_n)
        \end{pmatrix}.
    \end{align*}
    
    This ordered list of $n$ many column vectors from $K^m$ can be interpreted to be a grid of scalars with $m$ rows and $n$ columns. In general, an $m$ by $n$ grid of scalars is called a \textit{$m \times n$ matrix}. ($m \times n$ is read as ``$m$ by $n$'').
    
    Of course, the above matrix isn't just ``any old matrix'': this matrix represents\footnotemark the linear function $\ff$! For this reason, the above matrix is called the \textit{standard matrix of $\ff:K^n \rightarrow K^m$}.
    
    \footnotetext{\label{ch::lin_alg::footnote::standard_matrix_rep} When we say that the standard matrix of $\ff:K^n \rightarrow K^m$ ``represents'' $\ff$, we mean that the function $\FF$ that associates a linear function with its standard matrix is a bijection between the set of linear functions $K^n \rightarrow K^m$ and the set of $m \times n$ matrices. Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list} implies that $\FF$ is onto. Showing that $\FF$ is one-to-one is a simple exercise.}
\end{deriv}

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_vector_product}
    (Matrix-vector product).
    
    Let $K$ be a field and consider a linear function $\ff:K^n \rightarrow K^m$. The previous derivation showed that a linear function $\ff:K^n \rightarrow K^m$ is represented by its standard matrix, $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$. In this derivation, we formalize the notion of using $\ff$'s standard matrix to determine $\ff$ itself.
    
    From the previous derivation, we know that there is a function $\pp$ that returns $\ff(\vv)$ when given the standard matrix of $\ff$ and a vector $\vv$; specifically
    
    \begin{align*}
        \pp\Big( \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv \Big) := ([\vv]_E)_1 \ff(\see_1) + ... + ([\vv]_E)_n \ff(\see_n) = \ff(\vv).
    \end{align*}
    
    Notice that since the columns of the standard matrix of $\ff$ vary over $K^m$, and since $([\vv]_\sE)_1, ..., ([\vv]_\sE)_n$ vary\footnotemark over $K$, we can now restate the action of $\pp$ in terms of an arbitrary $m \times n$ matrix $\AA$ having $i$th column $\aa_i$ and an arbitrary column vector $\vv \in K^n$:
    
    \footnotetext{The $i$th column of $\ff(\sE)$ is $\ff(\ee_i)$. Each $\ff(\ee_i)$ can vary over $K^m$ because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}. The $([\vv]_E)_i$ vary over $K$ because $\vv$ varies over $K^n$.}
    
    \begin{align*}
        \pp\Bigg(
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA,
            \underbrace{
            \begin{pmatrix} 
                v_1 \\ \vdots \\ v_n 
            \end{pmatrix}}_\vv
            \Bigg)
            =
            v_1 \aa_1 + ... + v_n \aa_n.
    \end{align*}
    
    So, $\pp$ is really a function that accepts an $m \times n$ matrix and $n$-dimensional column vector as input and that produces an $m$-dimensional column vector as output. For this reason, we will call $\pp$ the \textit{matrix-vector product}.
    
    No one actually uses the letter $\pp$ when notating matrix-vector products. Instead, we simply establish the convention that writing a column vector to the right of a matrix indicates the evaluation of the corresponding matrix-vector product. That is, given an $m \times n$ matrix $\AA = \begin{pmatrix} \aa_1 \hdots & \aa_n \end{pmatrix}$ and a column vector $\vv \in K^m$, we define
    
    \begin{align*}
        \boxed
        {
            \AA \vv =
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA
            \underbrace{
            \begin{pmatrix} 
                v_1 \\ \vdots \\ v_n 
            \end{pmatrix}}_\vv
            :=
            v_1 \aa_1 + ... + v_n \aa_n
        }
    \end{align*}
    
    Expanding out the columns of $\AA$, here is what the above definition looks like when written out more explicitly:
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix} 
            a_{11} & \hdots & a_{1n} \\
            \vdots & \hdots & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}}_\AA
        \underbrace{
        \begin{pmatrix} 
            v_1 \\ \vdots \\ v_n 
        \end{pmatrix}}_\vv
        :=
        v_1 
        \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}
        + ... + v_n 
        \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}.
    \end{align*}
\end{deriv}

Now that we have the notation of the matrix-vector product, we can restate and expand upon the fact ``$\pp(\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv) = \ff(\vv)$''.

\begin{theorem}
    (Characterizing property of standard matrices, verbosely stated).
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$ of $\ff$ is the unique matrix satisfying
    
    \begin{align*}
        \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} \vv = \ff(\vv) \text{ for all $\vv \in K^n$}.
    \end{align*}
    
    (The left side of the above equation is a matrix-vector product).
\end{theorem}

\begin{proof}
   Derivation \ref{ch::lin_alg::deriv::standard_matrix} showed that $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$ satisfies the equation. What we have not yet shown is that the standard matrix is the \textit{only} matrix satisfying this equation. To prove this, suppose that some  matrix $\AA$ satisfies $\AA \vv = \ff(\vv)$ for all $\vv \in K^n$. We need to show $\AA$ is in fact equal to the standard matrix of $\ff$.
   
   Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list} guarantees that there is a linear function $\gg$ such that $\begin{pmatrix} \gg(\see_1) & \hdots & \gg(\see_n) \end{pmatrix} = \AA$. Thus $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} \vv = \begin{pmatrix} \gg(\see_1) & \hdots & \gg(\see_n) \end{pmatrix} \vv$ for all $\vv \in K^n$. That is, $\ff(\vv) = \gg(\vv)$ for all $\vv \in K^n$, so $\ff = \gg$, and $\AA = \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$, which is the standard matrix of $\ff$.
\end{proof}

To make notating standard matrices more compact, we make the following definition.

\begin{defn}
\label{ch::lin_alg::defn::linear_fn_acts_on_vectors}
    (Function acting on a list).
    
    Let $X$ and $Y$ by sets, let $f:X \rightarrow Y$ be a function, and let $L = \begin{pmatrix} x_1 & \hdots & x_n \end{pmatrix}$ be a finite list of elements of $X$. We define the notation
    
    \begin{align*}
        f(L) := \begin{pmatrix} f(x_1) & \hdots & f(x_n) \end{pmatrix}.
    \end{align*}
\end{defn}

The following theorem illustrates the succinctness of this new notation.

\begin{theorem}
    (Compact notation for standard matrix).
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix of $\ff$ is
    
    \begin{align*}
        \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} = \ff(\begin{pmatrix} \see_1 & ... & \see_n \end{pmatrix}) = \ff(\sE),
    \end{align*}
    
    where, in a slight abuse of notation, we use $\sE$ here to denote the \textit{list} $\begin{pmatrix} \see_1 & \hdots & \see_n \end{pmatrix}$ whose $i$th element is $\see_i$ rather than the \textit{set} $\{\see_1, ..., \see_n\}$ containing $\see_1, ..., \see_n$. (It is necessary to distinguish between lists and sets because sets have no ordering).
\end{theorem}

Now, we can restate the characterizing property of standard matrices concisely.

\begin{theorem}
    (Characterizing property of standard matrices).
    \label{ch::lin_alg::thm::characterizing_property_of_standard_matrix}
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix $\ff(\sE)$ of $\ff$ is the unique matrix satisfying the following characterizing property:
    
    \begin{align*}
        \boxed{
            \ff(\sE) [\vv]_E = \ff(\vv) \text{ for all $\vv \in K^n$}
        }
    \end{align*}
    
    (The left side of the above equation is a matrix-vector product).
\end{theorem}

[segway]

\begin{theorem}
    (A composition of linear functions is also linear).
    
    Let $V, W$, and $Y$ be vector spaces over the same field. If $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ are linear functions, then the composition $\gg \circ \ff$ is also a linear function.
\end{theorem}

\begin{proof}
   Left as an exercise.
\end{proof}

Since we know that a composition of linear functions is another linear function and that every linear function is represented by the matrix, we naturally ask: ``What is the matrix of a composition of linear functions''?

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}
    (Standard matrix of a composition of linear functions, matrix-matrix product). 
    
    Let $K$ be a field, and consider linear functions $\ff:K^n \rightarrow K^m$ and $\gg:K^m \rightarrow K^p$. Additionally, let $\sE = \{\ee_1, ..., \ee_n\}$ be the standard basis for $K^n$, $\sF = \{\see_1, ..., \see_m\}$ be the standard basis for $K^m$, and $\sG = \{\see_1, ..., \see_p\}$ be the standard basis for $K^p$.
    
    Since $\gg \circ \ff$ is a linear function $K^n \rightarrow K^p$, it has a standard matrix, $(\gg \circ \ff)(\sE)$:
    
    \begin{align*}
        (\gg \circ \ff)(\sE)
        &=
        \begin{pmatrix}
            (\gg \circ \ff)(\see_1) & \hdots & (\gg \circ \ff)(\see_n)
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            \gg(\ff(\see_1)) & \hdots & \gg(\ff(\see_n))
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            \gg(\sF) \ff(\see_1) & \hdots & \gg(\sF) \ff(\see_n)
        \end{pmatrix}.
    \end{align*}
    
    Recalling that $\ff(\see_i)$ is the $i$th column $(\ff(\sE))_i$ of $\ff(\sE)$, the standard matrix of $\ff$, we see that
    
    \begin{align*}
        (\gg \circ \ff)(\sE) = 
        \begin{pmatrix} 
            \gg(\sF) (\ff(\sE))_1 & \hdots & \gg(\sF) (\ff(\sE))_n
        \end{pmatrix}.
    \end{align*}
    
    Thus, the standard matrix $(\gg \circ \ff)(\sE)$ depends on the standard matrices $\ff(\sE)$ and $\gg(\sE)$. In other words, we have discovered that there is a function $\PP$ that takes the standard matrices $\ff(\sE)$ and $\gg(\sF)$ as input and returns the standard matrix $(\gg \circ \ff)(\sE)$ as output:
    
    \begin{align*}
        \PP(\gg(\sF), \ff(\sE)) := \begin{pmatrix} 
            \gg(\sF) (\ff(\sE))_1 & \hdots & \gg(\sF) (\ff(\sE))_n
        \end{pmatrix}.
    \end{align*}
    
    Since $\ff(\sE)$ varies over $K^{m \times n}$ and $\gg(\sF)$ varies over $K^{m \times p}$, we can restate the action of $\PP$ in terms of arbitrary matrices $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix} \in K^{m \times n}$ and $\BB \in K^{m \times p}$:
    
    \begin{align*}
        \PP \Bigg(\BB, \underbrace{\begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}}_\AA \Bigg) := 
        \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}.
    \end{align*}
    
    Just as was the case with the matrix-vector product $\pp$, no one actually uses the letter $\PP$ when notating matrix-matrix products. Instead, we establish the convention that writing two matrices of compatible sizes next to each other indicates the evaluation of the corresponding matrix-matrix product. That is, given an $m \times n$ matrix $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}$ and a $p \times m$ matrix $\BB$, we define

    \begin{align*}
        \boxed
        {
            \BB \AA 
            =
            \BB
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA
            := \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}
        }
    \end{align*}
    
    With the above definition, the standard matrix $(\gg \circ \ff)(\sE)$ of $\gg \circ \ff$ relative to $\sE$ is expressed as the matrix-matrix product $\gg(\sF) \spc \ff(\sE)$:
    
    \begin{align*}
        (\gg \circ \ff)(\sE) = \gg(\sF) \spc \ff(\sE).
    \end{align*}
\end{deriv}

\begin{remark}
    (Compatibility of matrices for matrix-matrix products). 
    
    We now expand on what was meant when we said the matrix-matrix product $\BB \AA$ is only defined when the sizes of $\AA$ and $\BB$ are ``compatible''.
    
    Consider that the composition $\gg \circ \ff$ of linear functions $\ff$ and $\gg$ is only defined when the output space of $\ff$ is the entire input space of $\gg$, i.e., when the dimension of $\ff$'s output is the same as the dimension of $\gg$'s input. Because of this, the matrix-matrix product $\BB \AA$ of an $m \times n$ matrix with an $r \times s$ matrix $\BB$ is only defined when $r = n$, i.e., when $\BB$ has as many columns as $\AA$ has rows.
\end{remark}

\begin{remark}
    Expanding out the columns of $\BB \AA$, here is what the matrix-matrix product $\BB \AA$ looks like when written out more explicitly:
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix} 
            b_{11} & \hdots & b_{1m} \\
            \vdots & \hdots & \vdots \\
            b_{p1} & \hdots & b_{pm}
        \end{pmatrix}}_\BB
        \underbrace
        {\begin{pmatrix} 
            a_{11} & \hdots & a_{1n} \\
            \vdots & \hdots & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}}_\AA
        &:=
        \begin{pmatrix}
            \underbrace
            {\begin{pmatrix} 
                b_{11} & \hdots & b_{1m} \\
                \vdots & \hdots & \vdots \\
                b_{p1} & \hdots & b_{pm}
            \end{pmatrix}}_\BB
            \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}
            &
            \hdots
            &
            \underbrace
            {\begin{pmatrix} 
                b_{11} & \hdots & b_{1m} \\
                \vdots & \hdots & \vdots \\
                b_{p1} & \hdots & b_{pm}
            \end{pmatrix}}_\BB
            \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            b_{11} a_{11} ... b_{1m} a_{m1} &
            \hdots
            &
            b_{11} a_{1n} + ... + b_{1m} a_{mn}
            \\
            \vdots & & \vdots \\
            \\
            b_{p1} a_{11} ... b_{pm} a_{m1} &
            \hdots & b_{p1} a_{1n} + ... + b_{pm} a_{mn}
        \end{pmatrix}.
    \end{align*}
    
    Don't try to make too much sense of this now. Theorem \ref{ch::lin_alg::thm::coordinates_of_matrix_matrix_product} makes thinking about entries of matrix-matrix products much more tractable.
\end{remark}

\newpage

\subsection*{Matrices}

The previous section showed us that matrices are important because they can be used to represent linear functions. We now provide explanations for how ideas about linear functions translate over to results ideas about matrices.

First, we restate the earlier hasty definition of ``matrix''; in this definition we also define some new notation for matrix entries and for the set of all $m \times n$ matrices over a field.

\begin{defn}
    (Matrix).
    
    Let $K$ be a field. An \textit{$m \times n$ matrix (with entries in $K$)} is a ``grid'' of elements of $K$ with $m$ rows and $n$ columns. For example, the following is a matrix with three rows and two columns that has entries in $\R$:
    
    \begin{align*}
        \begin{pmatrix}
            -1 & \frac{3}{4} \\
            \pi & 0 \\
            5 & -11
        \end{pmatrix}.
    \end{align*}
    
    The entry in the $i$th row and $j$th column of a matrix is called the \textit{$ij$ entry} of that matrix. (For example, the above matrix has a $21$ entry of $\pi$). Specifying matrices by describing their $ij$th entry is relatively common. We write ``$\AA = (a_{ij})$'' iff $\AA$ is the matrix with $ij$ entry $a_{ij}$.
    
    We also define $K^{m \times n}$ to be the set of $m \times n$ matrices with entries in $K$.
\end{defn}

\begin{defn}
    (Identity matrix).
    
    Let $K$ be a field, and consider the identity function on $K^n$, which is the function $\II_{K_n}:K^n \rightarrow K^n$ defined by $\II_{K^n}(\vv) = \vv$. The standard matrix of $\II_{K^n}$ relative to the standard basis $\sE$ is called the \textit{($n \times n$) identity matrix}. Notice that since the $i$th column of the identity matrix is $\II_{K_n}(\see_i) = \see_i$, it follows that the identity matrix has a diagonal of $1$'s, with $0$'s everywhere else; its $ij$ entry is $1$ if $i = j$ and $0$ if $i \neq j$. 
    
    The $3 \times 3$ identity matrix, for example, is
    
    \begin{align*}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{pmatrix}.
    \end{align*}
    
    We denote the identity matrix by $\II$ rather than by the more verbose notation $\II_{K^n}(\sE)$, and infer $n$ from context.
\end{defn}

\subsubsection{Matrix-vector products}

Now we return to the relationship between linear functions and matrices. This relationship hinges on the matrix-vector product, which we revisit. We first restate the definition of the matrix-vector product from Derivation \ref{ch::lin_alg::deriv::matrix_vector_product} for convenience.

\begin{defn}
    (Matrix-vector product).

    Let $K$ be a field. Given $\AA \in K^{m \times n}$ with $i$th column $\aa_i$ and $\vv \in K^n$ with $i$th entry $v_i$, we define the \textit{matrix-vector product} $\AA \vv$ to be the following:

    \begin{align*}
        \AA \vv =
        \underbrace
        {\begin{pmatrix} 
            \aa_1 & \hdots & \aa_n
        \end{pmatrix}}_\AA
        \underbrace{
        \begin{pmatrix} 
            v_1 \\ \vdots \\ v_n 
        \end{pmatrix}}_\vv
        :=
        v_1 \aa_1 + ... + v_n \aa_n.
    \end{align*}
\end{defn}

Now we present several new theorems.

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_vector_product}
    ($i$th entry of matrix-vector product).
    
        Let $\AA = (a_{ij})$ be an $m \times n$ matrix with entries in a field $K$ and let $\vv = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \in K^n$ be a column vector. By the previous definition, the matrix-vector product $\AA \vv$ is equal to the following:
    
    \begin{align*}
            \AA \vv = 
            \begin{pmatrix}
                a_{11} & \hdots & a_{1n} \\
                \vdots & & \vdots \\
                a_{i1} & \hdots & a_{in} \\
                \vdots & & \vdots \\
                a_{m1} & \hdots & a_{mn}
            \end{pmatrix}
            \begin{pmatrix} v_1 \\ \vdots \\ \vdots \\ \vdots \\ v_n \end{pmatrix}
            =
            v_1
            \begin{pmatrix} a_{11} \\ \vdots \\ a_{i1} \\ \vdots \\ a_{m1} \end{pmatrix}
            +
            ...
            +
            v_n
            \begin{pmatrix} a_{1n} \\ \vdots \\ a_{in} \\ \vdots \\ a_{mn} \end{pmatrix}
            =
            \begin{pmatrix} v_1 a_{11} + ... + v_n a_{1n} \\ \vdots \\ v_1 a_{i1} + ... + v_n a_{in} \\ \vdots \\ v_1 a_{m1} + ... + v_n a_{mn} \end{pmatrix}.
    \end{align*}

    Therefore, the $i$th entry $(\AA \vv)_i$ of $\AA \vv$ is $v_i a_{i1} + ... + v_n a_{in} $, which is
    
    \begin{align*}
        \boxed
        {
            (\AA \vv)_i = (\text{$i$th row of $\AA$}) \cdot \vv
        }
    \end{align*}
    
    Here $\cdot:K^n \times K^n \rightarrow K$ denotes the \textit{dot product} of vectors in $K^n$, defined by
    
    \begin{align*}
        \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}
        \cdot
        \begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix}
        =
        v_1 w_1 + ... + v_n w_n.
    \end{align*}
    
    Since the dot product must take two column vectors as input, what we technically mean by ``$i$th row of $\AA$'' in the boxed equation is ``column vector that contains entries of $i$th row of $\AA$''.
    
    The last section of this chapter discusses the dot product in depth.
\end{theorem}

\begin{theorem}
    (Linear functions from matrices).
    
    If $\AA$ is an $m \times n$ matrix with entries in a field $K$, then the function $\vv \mapsto \AA \vv$ is linear, and $\AA$ is the standard matrix of this function. We define $\ker(\AA) := \ker(\vv \mapsto \AA \vv)$ and $\im(\AA) := \im(\vv \mapsto \AA \vv)$.
\end{theorem}

\begin{proof}
    Define $\ff$ by $\ff(\vv) = \AA \vv$. We will show that $\ff$ is linear and that $\AA = \ff(\sE)$.

    Because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}, we can interpret the $i$th column $\aa_i$ of $\AA$ to be $\gg(\see_i)$ for some linear function $\gg$. Thus $\AA = \gg(\sE)$, and so $\ff(\vv) = \AA \vv = \gg(\sE) \vv = \gg(\sE) [\vv]_\sE$. The characterizing property of standard matrices says that $\gg(\sE) [\vv]_\sE = \gg(\vv)$, so we have $\ff(\vv) = \gg(\vv)$ for all $\vv \in K^n$, i.e. $\ff = \gg$. Since $\gg$ is linear, $\ff$ is also linear.
    
    The standard matrix $\ff(\sE)$ of $\ff$ has $i$th column $\ff(\see_i) = \AA \see_i$. Compute the matrix-vector product $\AA \see_i$ to verify that $\AA \see_i = \aa_i$, where $\aa_i$ is the $i$th column of $\AA$. This tells us that $\ff(\see_i) = \aa_i$, which means $\ff(\sE) = \AA$.
    
    %Since we have $\AA \vv = \ff(\vv)$ and $\ff(\sE) \vv = \ff(\vv)$, the uniqueness of standard matrices implies that $\AA = \ff(\sE)$. \textbf{Now we have to show that $\ff(\sE) = \gg(\sE)$, where $\gg(\vv) = \AA \vv$}.
\end{proof}

\begin{theorem}
    (Properties of the matrix-vector product).
    
    Practically speaking, the linearity of the function $\vv \mapsto \AA \vv$ translates into properties of the matrix-vector product: we have $\AA(\vv + \ww) = \AA \vv + \AA \ww$ and $\AA(c\vv) = c(\AA \vv)$ for any matrix $\AA$, column vectors $\vv$ and $\ww$, and scalar $c$. 
\end{theorem}

\subsubsection{Matrix-matrix products}

We also revisit the matrix-matrix product for convenience. Similarly to as was with matrix-vector products, we restate the definition of matrix-matrix product from Definition \ref{lin_alg::thm::matrix_matrix_product_relative_to_bases_standard} for convenience.

\begin{defn}
    (Matrix-matrix product).

    Let $K$ be a field. Given $\AA \in K^{m \times n}$ with $i$th column $\aa_i$ and $\BB \in K^{p \times m}$, we define the \textit{matrix-matrix product} $\BB \AA$ to be the following:

    \begin{align*}
        \BB \AA = \BB \underbrace{\begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}}_\AA := 
        \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}.
    \end{align*}
\end{defn}

Now, we present several new concepts regarding matrix-matrix products.

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_matrix_product}

    ($ij$ entry of matrix-matrix product). 
    
    Let $K$ be a field, let $\AA = (a_{ij}) \in K^{m \times n}$, and let $\BB = (b_{ij}) \in K^{m \times p}$. The $ij$ entry of the matrix-matrix product $\BB \AA$ can be computed by using the definition of the matrix-matrix product (Theorem \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}) together with the fact that the $i$th entry of the matrix-vector product $\AA \vv$ is $(\text{$i$th row of $\AA$}) \cdot \vv$, where $\cdot$ is the dot product. We have
    
    \begin{align*}
        \BB \AA
        = 
        \BB
        \begin{pmatrix}
            \aa_1 & \hdots & \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \BB \aa_1 & \hdots & \BB \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \bb_1 \cdot \aa_1 & \hdots & \bb_1 \cdot \aa_n \\
            \vdots & & \vdots \\
            \bb_m \cdot \aa_1 & \hdots & \bb_m \cdot \aa_n
        \end{pmatrix},
    \end{align*}
    
    where $\aa_i$ is the $i$th column of $\AA$ and $\bb_i$ is the $i$th row of $\BB$. So the $ij$ entry $(\BB \AA)_{ij}$ of $\BB \AA$ is $\bb_i \cdot \aa_j$, which is
    
    \begin{align*}
        \boxed
        {
            (\BB \AA)_{ij} = (\text{$i$th row of $\BB$}) \cdot (\text{$j$th column of $\AA$})
        }
    \end{align*}
    
    As was the case in Theorem \ref{ch::lin_alg::thm::coordinates_of_matrix_vector_product}, what we mean by ``$i$th row of $\BB$'' in the boxed equation is ``column vector that contains entries of $i$th row of $\AA$''.
\end{theorem}

\begin{remark}
    (Matrix-matrix products are associative).
    
    One would expect that $(\BB \AA) \vv = \BB (\AA \vv)$ for all column vectors $\vv$ when $\AA$ and $\BB$ are ``compatible'' matrices. This is indeed true because the corresponding linear functions $\ff$ and $\gg$ satisfy $(\gg \circ \ff)(\vv) = \gg(\ff(\vv))$.
\end{remark}

\begin{defn}
    (Inverse matrix).
    
    Let $\AA$ be an $m \times n$ matrix with entries in a field $K$. We say that $\AA$ is \textit{invertible} iff there exists a matrix $\AA^{-1}$ such that $\AA^{-1} \AA = \II = \AA \AA^{-1}$.
\end{defn}

\begin{theorem}
    (Inverse matrix).

    Let $\AA$ be an invertible $m \times n$ matrix with entries in a field $K$, and consider the linear function $\ff:K^m \rightarrow K^n$ defined by $\ff(\vv) = \AA \vv$. Because $\ff^{-1} \circ \ff = \II = \ff \circ \ff^{-1}$, it follows that $\AA = \ff(\sE)$ is invertible iff $\ff$ is, and that when $\AA^{-1}$ exists, it is equal to the standard matrix of $\ff^{-1}$. In other words, if $\ff:K^n \rightarrow K^n$ is an invertible linear function, then $\ff(\sE)^{-1} := \ff^{-1}(\sE)$.
    
    Since it is only possible for $\ff$ to be invertible when $n = m$, it follows that $\AA$ must be a $n \times n$ matrix, or \textit{square matrix}, in order to be invertible.
\end{theorem}

\begin{remark}
    (Matrix pedagogy). 
    
    Most linear algebra texts present the relationship between linear functions and matrices in the following way: first define matrices in the context of systems of linear equations (we have not seen how matrices are related to systems of linear equations), then define a linear function to be one for which $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ and $\ff(c\vv) = c\ff(\vv)$ for all vectors $\vv, \ww$ and scalars $c$, and then prove that each linear function has a standard matrix. This is bad pedagogy; there should be no need to conjecture and prove that a matrix-vector product corresponds to the action of a linear function, because this fact is apparent from natural investigations (such as Derivation \ref{ch::lin_alg::deriv::standard_matrix}). (Furthermore, while systems of linear equations are an important application of linear algebra, and while their study does enhance our knowledge about the kernels of linear functions, they should not be the starting point).
    
    Oftentimes, linear algebra texts present the formula for the $i$th entry of a matrix-vector product and the formula for the $ij$ entry of a matrix-matrix product as facts that should be memorized rather than understood. Be wary of this! You \textit{should not} memorize these formulas. If you can't quite remember them, try to derive them by starting with the fact that linear functions on finite-dimensional vector spaces are determined by what they do to bases, and by following the derivations given in this book!
\end{remark}

\subsubsection{Matrix transposes}

The topic of matrix tranposes does not become theoretically significant until Chapter \ref{ch::bilinear_forms_metric_tensors}. We present some surface-level details now, though, because they do appear occasionally before Chapter \ref{ch::bilinear_forms_metric_tensors}.

\begin{defn}
    (Transpose of a matrix).

    Let $\AA = (a_{ij})$ be an $m \times n$ matrix with entries in a field $K$. The \textit{transpose $\AA^\top$ of $\AA$} is the $n \times m$ matrix whose $ij$ entry is the $ji$ entry of $\AA$: $\AA^\top := (a_{ji})$. 
    
    We also use the notation $a_{ij}^\top$ to denote the $ij$ entry of $(a_{ij})^\top$.

    For any matrix $\AA$, the columns of $\AA^\top$ are the rows of $\AA$, and the rows of $\AA^\top$ are the columns of $\AA$.
\end{defn}

\begin{theorem}
    (Properties of the transpose).

    \begin{enumerate}
        \item $\II^\top = \II$.
        \item $(\AA^\top)^\top = \AA$ for all matrices $\AA$.
        \item $\AA \mapsto \AA^\top$ is linear for all matrices $\AA$.
        \item $(\BB \AA)^\top = \AA^\top \BB^\top$ for all matrices $\AA, \BB$ compatible with each other for matrix multiplication.
        \item $(\AA^{\top})^{-1} = (\AA^{-1})^\top$ for all invertible matrices $\AA$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\    
    \begin{enumerate}
        \item This is true because $\delta_{ij} = \delta_{ji}$.
        \item $((a_{ij})^\top)^\top = (a_{ij}^\top)^\top = (a_{ji})^\top = (a_{ji}^\top) = (a_{ij})$.
        \item We must show that $(\AA + \BB)^\top = \AA^\top + \BB^\top$ for all matrices $\AA$ and $\BB$ and that $(c\AA)^\top = c\AA^\top$ for all matrices $\AA$ and scalars $c$.

        We have $((a_{ij}) + (b_{ij}))^\top = (a_{ij} + b_{ij})^\top = (a_{ji} + b_{ji}) = (a_{ji}) + (b_{ji}) = (a_{ij})^\top + (b_{ij})^\top$, so the first property holds.

        We also have $(c(a_{ij}))^\top = (c a_{ij})^\top = (c a_{ji}) = c (a_{ji}) = c (a_{ij})^\top$, so the second property holds as well.
        
        \item $((b_{ij}) (a_{ij}))^\top = ( \sum_k b_{ik}a_{kj})^\top = (\sum_k b_{jk} a_{ki}) = (\sum_k a_{ik}^\top b_{kj}^\top) = (a_{ij}^\top) (b_{ij}^\top) = (a_{ij})^\top (b_{ij})^\top$.
        
        \item We need to show that $\AA^\top (\AA^{-1})^\top = \II$. By property (2), this equation is equivalent to $\AA^{-1} \AA = \II$, which is true.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Inverse transpose).

    Let $\AA$ be a matrix. Because the matrix inversion and transposition operations commute, it is unambiguous to define $\AA^{-\top} := (\AA^{\top})^{-1} = (\AA^{-1})^\top$.
\end{defn}

\newpage

\subsection*{Matrices relative to bases}

We started this section- ``Coordinatization of linear functions with matrices''- by noting that if $V$ and $W$ are $n$- and $m$-dimensional vector spaces, we can effectively study linear functions $V \rightarrow W$ by studying linear functions $K^n \rightarrow K^m$. Then, we showed that because a linear function $K^n \rightarrow K^m$ is determined by how it acts on a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$, any linear function $\ff:K^n \rightarrow K^m$ is represented by the matrix $\ff(\sE)$.

In Theorem \ref{ch::lin_alg::thm::linear_functions_matrices_isomorphism}, we saw that this result about representations of linear functions $K^n \rightarrow K^m$ ``ripples up'' to a result about representations of linear functions $V \rightarrow W$: that is, every linear function $V \rightarrow W$ is represented by a $\dim(V) \times \dim(W)$ matrix. That theorem does not tell us how to compute the matrix representation of an arbitrary linear function, however. We investigate this now.

\begin{lemma}
    \label{ch::lin_alg::lemma::basis_inverse}
    ($[\cdot]_E^{-1}(\sE) = E$).

    Let $V$ be a vector space and let $E$ be a basis of $V$. We have $[\cdot]_E^{-1}(\sE) = E$. 
\end{lemma}

\begin{proof}
    Since $[\ee_i]_E = \see_i$, then $[\cdot]_E^{-1}(\see_i) = \ee_i$, and so $[\cdot]_E^{-1}(\sE) = E$. 
\end{proof}

Notice that $E$ is not a matrix in the above lemma! It is a list of vectors that are not necessarily column vectors. We investigate the special case when $E$ is indeed a list of column vectors (i.e. a list of vectors from $K^n$) in Theorem \ref{ch::lin_alg::thm::basis_change_from_standard_vectors}.

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_relative_to_bases}
    (Matrix relative to bases).
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$ with bases $E$ and $F$, and consider a linear function $\ff:V \rightarrow W$. 
    
    The standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ represents $\ff_{E,F}$, and $\ff_{E,F}$ represents $\ff$, so\footnote{Again, when we say that $\hh$ \textit{represents} $\gg$, we mean that the map sending $\gg \mapsto \hh$ is a bijection.} we conclude by transitivity that $\ff_{E,F}(\sE)$ represents $\ff$. For this reason, we define the \textit{matrix of $\ff$ relative to $E$ and $F$} to be the standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F}$.
    
    Now, let's investigate $\ff_{E,F}(\sE)$ itself. We have $\ff_{E,F}(\sE) = ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1})(\sE) = \Big[\ff\Big([\cdot]_E^{-1}(\sE)\Big)\Big]_F$. By the previous lemma, we have $[\cdot]_E^{-1}(\sE) = E$, so
    
    \begin{align*}
        \boxed
        {
            \ff_{E,F}(\sE) = [\ff(E)]_F
        }
    \end{align*}
    
    Thus, the matrix of a linear function $\ff:V \rightarrow W$ relative to the bases $E$ and $F$ for $V$ and $W$ is $[\ff(E)]_F$. Explicitly, $[\ff(E)]_F$ looks like this:
    
    \begin{align*}
        [\ff(E)]_F =
        \begin{pmatrix}
            [\ff(\ee_1)]_F & \hdots & [\ff(\ee_n)]_F
        \end{pmatrix}
    \end{align*}
    
    The characterizing property of standard matrices for linear functions $K^n \rightarrow K^m$ gives us the following equivalent statements:
    
    \begin{align*}
        \ff_{E,F}(\vv) &= \ff_{E,F}(\sE) \spc \vv \text{ for all $\vv \in K^n$} \\
        ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1})(\vv) &= [\ff(E)]_F \spc \vv \text{ for all $\vv \in K^n$} \\
        ([\cdot]_F \circ \ff)(\vv_1) &= [\ff(E)]_F [\vv_1]_E \text{ for all $\vv_1 \in V$} \\
        [\ff(\vv_1)]_F &= [\ff(E)]_F [\vv_1]_E \text{ for all $\vv_1 \in V$}
    \end{align*}
    
    So, we have the following characterizing property for matrices relative to bases:
    
    \begin{align*}
        \boxed
        {
            [\ff(\vv)]_F = [\ff(E)]_F [\vv]_E \text{ for all $\vv \in V$}
        }
    \end{align*}
    
    This characterizing property tells a similar story as does the commutative diagram that describes $\ff_{E,F}$: it says that we can think of the function $\uu \mapsto [\ff(E)]_F \uu$ as accepting an input that is expressed relative to the basis $E$ for $V$ and as producing an output that is expressed relative to the basis $F$ for $W$.
\end{deriv}

\begin{remark}
\label{ch::lin_alg::rmk::standard_matrix_as_matrix_wrt_bases}
    (Standard matrix as special case of matrix relative to bases). 
    
    The standard matrix $\ff(\sE)$ of a linear function $\ff:K^n \rightarrow K^m$ is the matrix $\ff(\sE) = [\ff(\sE)]_\sE$ of $\ff:K^n \rightarrow K^m$ relative to the bases $\sE$ and $\sE$.
\end{remark}

\newpage

\subsection*{Vectors relative to bases}

We will now discover how when we have a finite-dimensional vector space with bases $E$ and $F$ we can relate $[\vv]_E$ to $[\vv]_F$ for any vector $\vv$.

\begin{theorem}
    \label{ch::lin_alg::thm::change_of_basis_for_vectors}
    
    (Change of basis for vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$. 
    
    The characterizing property of matrices relative to $E$ and $F$ says that for any linear function $\ff:V \rightarrow V$, we have $[\ff(\vv)]_F = [\ff(E)]_F [\vv]_E$. In particular, when $\ff$ is the identity on $V$, we obtain
    
    \begin{align*}
        \boxed
        {
            [\vv]_F = [\EE]_F [\vv]_E
        }
    \end{align*}
    
    where $\EE$ is the list of vectors whose $i$th entry is $\ee_i$, so that $[\EE]_F$ is the matrix whose $i$th column is $[\ee_i]_F$.
    
    It is a good sanity check that the identity on $V$ is involved in changing bases, since representing a vector with different bases does not change the vector itself.
\end{theorem}

\begin{remark}
    (Alternate proof of change of basis for vectors).
    
    Here is an alternate proof to the above. The matrix of $[\cdot]_F$ relative to $E$ and $\sE$ is $[[\cdot]_F(E)]_\sE = [\cdot]_F(E) = [\EE]_F$. Using the characterizing property of matrices relative to bases, we have $[\vv]_F = [[\cdot]_F(E)]_\sE [\vv]_E = [\EE]_F [\vv]_E$.
\end{remark}

The following is an important special case\footnote{Use the substitutions $E := \sE$ and $F := E$ in the change of basis theorem for vectors to obtain $[\vv]_E = [\sE]_E [\vv]_\sE = \EE^{-1} \vv$.} of the change of basis theorem for vectors.

\begin{theorem}
    (Change of basis from standard basis for vectors).
    \label{ch::lin_alg::thm::basis_change_from_standard_vectors}

    Let $V$ be a vector space and let $E$ be a basis of $V$. Then from the definition of $[\cdot]_E$ (recall Definition \ref{ch::lin_alg::defn::coordinates_relative_to_basis}) it immediately follows that

    \begin{align*}
        \boxed
        {
            \vv = \EE [\vv]_E \text{ for all $\vv \in V$}
        }
    \end{align*}

    When $V = K^n$, then the list $E$ becomes a matrix of column vectors $\EE$, and the standard matrices of $[\cdot]_E$ and $[\cdot]_E^{-1}$ exist. Using the result of Lemma \ref{ch::lin_alg::lemma::basis_inverse}, we see they are:

    \begin{align*}
        [\cdot]_E^{-1}(\sE) = \EE \text{ and } [\cdot]_E(\sE) = \EE^{-1}
    \end{align*}

    Importantly, we see that $\EE^{-1}$ exists. We can now use $\EE^{-1}$ to solve the equation $\vv = \EE[\vv]_E$ for $[\vv]_E$:

    \begin{align*}
        \boxed
        {
            [\vv]_E = \EE^{-1} \vv \text{ for all $\vv \in K^n$}
        }
    \end{align*}
\end{theorem}

\begin{theorem}
\label{ch::lin_alg::thm::I_EF}
    ($\II_{E,F}^{-1} = \II_{F,E}$ when $V = K^n$).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$. The identity function $\II:V \rightarrow V$ on $V$ satisfies $\II_{E,F}^{-1} = \II_{F,E}$. As a corollary, when $V = K^n$, then $E$ and $F$ are both lists of column vectors, so we have $[\EE]_F^{-1} = [\FF]_E$.
\end{theorem}

\begin{proof}
    Since $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$, then $\II_{E,F} = [\cdot]_F \circ [\cdot]_E^{-1}$ and $\II_{F,E} = [\cdot]_E \circ [\cdot]_F^{-1}$. We clearly have $\II_{E,F}^{-1} = \II_{F,E}$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::change_of_basis_with_basis_vectors}
    (Change of basis in terms of basis vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$. By the definition of $[\cdot]_E$, we have
    
    \begin{align*}
        \ff_i = \sum_{j = 1}^n ([\ff_i]_E)_j \ee_j = \sum_{j = 1}^n ([\FF]_E)_{ji} \ee_j,
    \end{align*}
    
    where $\FF$ is the matrix whose $i$th column is $\ff_i$.
    
    In the last equality, we have used that $[\ff_i]_E$ is the $i$th column of $[\FF]_E$.
\end{theorem}

\begin{remark}
    (On the order of proving change of basis theorems). 
    
    Most linear algebra texts first prove the previous theorem and use it to show a version of the first equation in the box of Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors}. This approach for proving Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors} was not used because it involves quite a bit of unmemorable matrix algebra. It's good to know that these theorems are equivalent, though.
\end{remark}

\newpage

\subsection*{Matrix similarity}

Not all linear algebra resources use the above notion of matrices relative to bases and instead investigate matrices of the form $\EE^{-1} \AA \EE$. The following theorem explains that such matrices arise as a special case of matrices relative to bases.

\begin{theorem}
    (Matrices of linear functions $K^n \rightarrow K^n$ relative to bases).
    
    Let $K$ be a field, consider the vector space $K^n$, let $E$ and $F$ be bases for $K^n$, and consider a linear function $\ff:K^n \rightarrow K^n$. The matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$ is
    
    \begin{align*}
        [\ff(E)]_F = \FF^{-1} \ff(\sE) \EE.
    \end{align*}

    An important special case of this theorem is that the matrix of $\ff$ relative to $E$ and $E$ is $\EE^{-1} \ff(\sE) \EE$.
\end{theorem}

\begin{proof}
   The matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$ is the standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$, which is $\ff_{E,F}(\sE) = [\cdot]_F(\sE) \spc \ff(\sE) \spc [\cdot]_E^{-1}(\sE)$. Since $F$ is a list of column vectors, then the corollary of Theorem \ref{ch::lin_alg::thm::I_EF} applies, and $[\cdot]_F(\sE) = [\sE]_F$ is equal to $[\FF]_\sE^{-1} = \FF^{-1}$. Also, because $E$ is a list of column vectors, we can apply Theorem \ref{ch::lin_alg::thm::basis_change_from_standard_vectors}, which says $[\cdot]_E^{-1}(\sE) = \EE$. So the most recent expression is equal to $\FF^{-1} \ff(\sE) \EE$, as claimed.
\end{proof}

\begin{remark}
    (Matrix similarity).

    Most linear algebra texts do not define the matrix of a linear function $V \rightarrow W$ relative to bases $E$ and $F$, and only define the ``matrix of a linear function $\ff:K^n \rightarrow K^n$ relative to the basis $E$ of $K^n$'' to be $\EE^{-1} \ff(\sE) \EE$.

    In such texts, one of two definitions of \textit{matrix similarity} is given:

    \begin{enumerate}
        \item Matrices $\AA$ and $\BB$ are \textit{similar} iff there exists a matrix $\EE$ such that $\BB = \EE^{-1} \AA \EE$.
        \item Matrices $\AA$ and $\BB$ are \textit{similar} iff there exists a matrix $\EE$ such that $\BB = \EE \AA \EE^{-1}$.
    \end{enumerate}
    
    The definitions are equivalent because matrices of the form $\EE^{-1} \AA \EE$ are also of the form $\EE \AA \EE^{-1}$.
    
    It is much better to use the first definition than the second one, though, since the placement of the matrix inverse in the second definition is different than it is in the formula $[\ff(E)]_E = \EE^{-1} \ff(\sE) \EE$. Using the second definition could lead one to think that ``$[\ff(E)]_E = \EE \ff(E) \EE^{-1}$'', which is not true!
\end{remark}

\newpage

\subsection*{Linear isomorphism between linear functions and matrices}

We've alluded to the correspondence between linear functions and matrices several times. Now that we know of matrices relative to bases, we can formalize our knowledge of this relationship.

\begin{theorem}
    \label{ch::lin_alg::thm::linear_functions_matrices_isomorphism}
    (Linear isomorphism between linear functions and matrices).
    
    Let $V, W$, and $Y$ be finite-dimensional vector spaces over a field $K$, with bases $E$, $F$, and $G$.

    The function $\FF:\LLLL(V \rightarrow Y) \rightarrow K^{\dim(Y) \times \dim(V)}$ defined by $\FF(\ff) := [\ff(E)]_G$ is a linear isomorphism, and also satisfies $\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$ for all linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$.

    The inverse $\FF^{-1}:K^{\dim(Y) \times \dim(V)} \rightarrow \LLLL(V \rightarrow Y)$ is defined by $\FF^{-1}(\AA) = [\cdot]_F^{-1} \circ (\vv \mapsto \AA \vv) \circ [\cdot]_E$, and satisfies $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ are linear functions with $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$.
\end{theorem}

\begin{proof}
    We first prove the statements regarding $\FF$, and then prove the statements regarding the claimed inverse to $\FF$.
    
    Notice that $\FF = \QQ \circ \PP$, where ${\PP:\LLLL(V \rightarrow Y) \rightarrow \LLLL(K^{\dim(V)} \rightarrow K^{\dim(Y)})}$ is defined by $\PP(\ff) := \ff_{E,G}$ and where ${\QQ:\LLLL(K^{\dim(V)} \rightarrow K^{\dim(Y)}) \rightarrow K^{\dim(Y) \times \dim(V)}}$ is defined by $\QQ(\gg) := \gg(\sE)$. (We have $(\QQ \circ \PP)(\ff) = \QQ(\PP(\ff)) = \QQ(\ff_{E,G}) = \ff_{E,G}(\sE) = [\ff(E)]_G$). We need to show that $\FF$ is linear, that it is invertible, and that it satisfies $\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$.

    \begin{itemize}    
        \item ($\FF$ is linear). It's easy to check that $\ff \mapsto \ff(E)$ is a linear function. Once we know this, then since $\ff \mapsto \ff(E)$ and $[\cdot]_G$ are both linear functions, we see $\ff \mapsto \FF(\ff) = [\ff(E)]_G$ is a composition of linear functions, and is thus linear.
        
        \item ($\FF$ is invertible). It suffices to show that $\PP$ and $\QQ$ are invertible.
        
        Since we have $\PP(\ff) = \ff_{E,G} = [\cdot]_G \circ \ff \circ [\cdot]_E^{-1}$, then $\PP^{-1}(\gg) = [\cdot]_G^{-1} \circ \gg \circ [\cdot]_E$. Thus $\PP$ is invertible.
        
        Since $\QQ(\gg)$ is the standard matrix of $\gg$, we claim that $\QQ^{-1}(\AA)$ is the linear function obtained from the matrix $\AA$: we claim $\QQ^{-1}(\AA) = (\vv \mapsto \AA \vv)$. We have $\QQ^{-1}(\QQ(\gg)) = \QQ^{-1}(\gg(\sE)) = (\vv \mapsto \gg(\sE) \vv) = \gg$ and $\QQ(\QQ^{-1}(\AA)) = \QQ(\vv \mapsto \AA \vv) = (\vv \mapsto \AA \vv)(\sE) = \AA$, so $\QQ^{-1}$ is indeed defined by $\QQ^{-1}(\AA) = (\vv \mapsto \AA \vv)$. Thus $\QQ$ is invertible too.
        
        \item ($\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$). We have $\FF(\gg \circ \ff) = (\QQ \circ \PP)(\gg \circ \ff) = \QQ(\PP(\gg \circ \ff)) = \QQ((\gg \circ \ff)_{E,G})$. Notice that $(\gg \circ \ff)_{E,G} = [\cdot]_G \circ \gg \circ \ff \circ [\cdot]_E^{-1} = ([\cdot]_G \circ \gg \circ [\cdot]_F^{-1}) \circ ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1}) = \gg_{F,G} \circ \ff_{E,F}$. Thus $\QQ((\gg \circ \ff)_{E,G}) = \QQ(\gg_{F,G} \circ \ff_{E,F})$. Since $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ and $\gg:K^{\dim(W)} \rightarrow K^{\dim(Y)}$, we can apply the result from the end of Derivation \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard} to conclude that the standard matrix of the composition is equal to the matrix-matrix product of the corresponding standard matrices; that is, $\QQ(\gg_{F,G} \circ \ff_{E,F}) = \gg_{F,G}(\sE) \ff_{E,F}(\sE) = [\gg(F)]_G [\ff(E)]_F$.
    \end{itemize}

    Now we prove the statements regarding the claimed inverse to $\FF$. We first derive the expression for $\FF^{-1}(\AA)$. Above, we saw that $\FF = \QQ \circ \PP$, where $\PP^{-1}(\gg) = [\cdot]_G^{-1} \circ \gg \circ [\cdot]_E$ and $\QQ^{-1}(\AA) = (\vv \mapsto \AA \vv)$. We therefore have $\FF^{-1}(\AA) = (\PP^{-1} \circ \QQ^{-1})(\AA) = \PP^{-1}(\QQ^{-1}(\AA)) = \PP^{-1}(\vv \mapsto \AA \vv) = [\cdot]_G^{-1} \circ (\vv \mapsto \AA \vv) \circ [\cdot]_E$, as desired.

    Lastly, we prove that $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$. We start with the proven fact $\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$. Applying $\FF^{-1}$ to both sides, we obtain ${\gg \circ \ff = [\cdot]_G^{-1} \circ (\vv \mapsto [\gg(F)]_G [\ff(E)]_F \vv) \circ [\cdot]_E}$. Consider the matrix-matrix product that appears in the expression on the right side, $\gg_{F,G}(\sE) \ff_{E,F}(\sE)$. Again applying the result from the end of Derivation \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}, we see that the matrix-matrix product of standard matrices $\gg_{F,G}(\sE) \ff_{E,F}(\sE)$ is equal to the standard matrix of the composition $\gg_{F,G} \circ \ff_{E,F}$; that is, \\ ${\gg_{F,G}(\sE) \ff_{E,F}(\sE) = (\gg_{F,G} \circ \ff_{E,F})(\sE)}$. Furthermore, we have
    ${\vv \mapsto (\gg_{F,G} \circ \ff_{E,F})(\sE) \vv = \gg_{F,G} \circ \ff_{E,F}}$. Thus $\gg \circ \ff = [\cdot]_G^{-1} \circ (\gg_{F,G} \circ \ff_{E,F}) \circ [\cdot]_E = [\cdot]_G^{-1} \circ ([\cdot]_G \circ \gg \circ [\cdot]_F^{-1}) \circ ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1}) \circ [\cdot]_E^{-1} = \gg \circ \ff$. In all, we've shown that $\FF^{-1}([\gg(F)]_G [\ff(E)]_F) = \gg \circ \ff$ for all linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$. Because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}, then for any matrices $\AA$ and $\BB$ there exist linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ such that $\AA = [\ff(E)]_F$ and $\BB = [\gg(F)]_G$. We can apply the equation $\FF^{-1}([\gg(F)]_G [\ff(E)]_F) = \gg \circ \ff$ to these $\ff$ and $\gg$ to obtain that $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$. This is the desired result.
\end{proof}

The following special case of the above theorem follows immediately.

\begin{theorem}
    \label{ch::lin_alg::thm::linear_functions_matrices_isomorphism_special_case}

    (Special case of linear isomorphism between linear functions and matrices).
    
    Let $K$ be a field, and let $\sE$ be a basis for $K^n$ and $\sF$ be a basis for $K^m$.
    
    The function $\FF:\LLLL(K^n \rightarrow K^p) \rightarrow K^{p \times n}$ defined by $\FF(\ff) := \ff(\sE)$ is a linear isomorphism, and also satisfies $\FF(\gg \circ \ff) = \gg(\sF) \ff(\sE)$ for all linear functions $\ff:K^n \rightarrow K^m$ and $\gg:K^m \rightarrow K^p$.

    The inverse $\FF^{-1}:K^{p \times n} \rightarrow \LLLL(K^n \rightarrow K^p)$ is defined by $\FF^{-1}(\AA) = (\vv \mapsto \AA \vv)$, and satisfies $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $\ff(\sE) = \AA$ and $\gg(\sE) = \BB$.
\end{theorem}

\newpage

\section*{Outline}

\begin{itemize}
    \item solving linear equations
    \item correspondence between linear matrix equations and systems of linear equations
    \item augmented matrices
    \item because adding equations, multiplying equations by a scalar, and swapping equations doesn't change the system, performing the analogous row operations on the corresponding matrix doesn't change the system to which the matrix corresponds
    \item RREF
    \item finding a basis for the kernel
    \begin{itemize}
        \item corollary: $\rref(\AA) = \II$ iff $\AA$ is invertible
    \end{itemize}
    \item finding a basis for the image
    \begin{itemize}
        \item The columns of $\AA$ corresponding to the pivot columns of $\rref(\AA)$ form a basis of the image.
        \begin{itemize}
            \item Proof. Let $\AA = (\aa_1 ... \aa_n)$. 
            
            (Case: $\rref(\AA)$ has no pivot columns). Then $\rref(\AA) = \mathbf{0}$. [?]

            (Case: $\rref(\AA)$ has pivot columns). We have $\rref(\AA) = (\see_1, ..., \see_k, \bb_{k + 1}, ..., \bb_n)$ for some integer $k > 0$. Since row-reduction is an invertible linear operation, there is an invertible matrix $\RR$ for which $\rref(\AA) = \RR \AA$. We thus have $\AA = \RR^{-1} \rref(\AA) = (\RR^{-1}\see_1, ..., \RR^{-1}\see_k, \RR^{-1}\bb_{k + 1}, ..., \RR^{-1}\bb_n)$. Recall from Lemma \ref{ch::lin_alg::lemma::only_inv_fns_preserve_lin_indep} that because $\RR^{-1}$ is invertible it preserves linear independence. Thus, since $\see_1, ..., \see_k$ are linearly independent, $\aa_1 = \RR^{-1} \see_1, ..., \aa_k = \RR^{-1} \see_k$ are linearly independent.

            [need to prove that $\see_1, ..., \see_k$ span $\im(\rref(\AA))$]
        \end{itemize}
    \end{itemize}
    \item test for linear independence
    \item row operations don't change the kernel and column operations don't change the image
\end{itemize}

misc.
\begin{itemize}
    \item rref is an invertible linear operation
    \item the standard basis is indeed a basis. true because $(c_1, ..., c_n)$ is the zero vector only when all $c_i$'s are zero.
\end{itemize}

\section{Systems of linear equations with matrices}

This section presents how systems of linear equations can be solved by using concepts from linear algebra.

Reading this section is entirely unnecessary for the remaining content in this book. This treatment of systems of linear equations has only been provided to serve as a superior alternative to the traditional approach of teaching linear algebra (which is: start with systems of linear equations, define the matrix-vector product, and then haphazardly discover the other linear algebra material we have covered). The approach we take emphasizes that linear algebra should not be introduced as a field of study that is discovered by starting with systems of linear equations\footnote{Linear algebra \textit{did} historically grow out of the study of systems of linear equations. Just remember that the historical approach is not always the most enlightening one!}, and that systems of linear equations should be viewed as an application of linear algebra.

\begin{defn}
    (Linear equation).

    Let $V$ and $W$ be vector spaces, let $\ff:V \rightarrow W$ be a function, and let $\ww \in W$. When $\ff$ is linear, the equation $(\ff(\vv) = \ww \text{ for all $\vv \in V$})$ is called a \textit{linear equation}.

    The set of solutions to the equation, $\{\text{solns}\} := \{\vv \in V \mid \ff(\vv) = \ww\}$, is a vector space whenever it is nonempty.
\end{defn}

Linear equations are ``easy'' to solve whenever the linear function $\ff$ involved is invertible. When $\ff$ isn't invertible, we convert the problem of the linear equation into an equivalent problem and solve that instead.

\begin{deriv}
    (Linear matrix equations).
    
    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces over a field $K$. Since $\ff:V \rightarrow W$ can be studied by choosing bases for $V$ and $W$ and investigating the induced linear function $K^n \rightarrow K^m$, we can study the linear equation

    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in V$, where $\ff:V \rightarrow W$ is linear and $\ww \in W$},
    \end{align*}
    
    by studying equations of the form 

    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in K^n$, where $\ff:K^n \rightarrow K^m$ is linear and $\ww \in K^m$}.
    \end{align*}

    Since for $\ff:K^n \rightarrow K^m$ we have $\ff(\vv) = \ff(\sE) \vv$ for all $\vv \in K^n$, and since the columns of $\ff(\sE)$ vary over $K^m$ (recall Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}), we can study equations of the above form by studying equtions of the form

    \begin{align*}
        \AA \vv = \ww \text{ for all $\vv \in K^n$, where $\AA \in K^{m \times n}$ and $\ww \in K^m$}.
    \end{align*}
    
    Equations of the above form are called \textit{(linear) matrix equations}\footnote{These equations are most commonly called \textit{matrix equations}}. 
\end{deriv}

\begin{deriv}
    (Systems of linear equations).

    Let $\AA = (a_{ij})$ be an $m \times n$ matrix with entries in a field $K$, let $\ww \in K^m$, and consider the linear matrix equation

    \begin{align*}
        \AA \vv = \ww \text{ for all $\vv \in V$}.
    \end{align*}

    If $\vv$ has $i$th component $v_i$ and $\ww$ has $i$th component $w_i$, then this is the same as

    \begin{align*}
        \begin{pmatrix}
            a_{11} & \hdots & a_{1n} \\
            \vdots & & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}
        \begin{pmatrix}
            v_1 \\ \vdots \\ v_n
        \end{pmatrix}
        &=
        \begin{pmatrix}
            w_1 \\ \vdots \\ w_m
        \end{pmatrix}
        \text{ for all $v_1, ..., v_n \in K$}
        \\
        \begin{pmatrix}
            v_1 a_{11} + ... + v_n a_{1n} \\
            \vdots \\
            v_1 a_{m1} + ... + v_n a_{mn}
        \end{pmatrix}
        &=
        \begin{pmatrix}
            w_1 \\ \vdots \\ w_m
        \end{pmatrix}
        \text{ for all $v_1, ..., v_n \in K$}
        ,
    \end{align*}

    which is equivalent to the system of equations

    \begin{align*}
        \begin{cases}
            v_1 a_{11} + ... + v_n a_{1n} = w_1 \\
            \vdots \\
            v_1 a_{m1} + ... + v_n a_{mn} = w_m
        \end{cases}
        \text{ for all $v_1, ..., v_n \in K$}.
    \end{align*}
\end{deriv}



Obviously, the linear equation $\ff(\vv) = \ww$ is ``easy'' to solve when $\ff$ is invertible, as $\{\text{solns}\} = \{\ff^{-1}(\ww)\}$ in that case. One might think that things become difficult when $\ff$ is not invertible. The following theorem says otherwise!

\begin{theorem}
    (Solutions to linear equations).

    Let $V$ and $W$ be vector spaces, let $\ff:V \rightarrow W$ be a linear function, and let $\ww \in W$. Consider the linear equation

    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in V$}.
    \end{align*}
    
    When the set of solutions $\{\text{solns}\}$ is nonempty, it can be described in terms of any particular solution:

    \begin{align*}
        \{\text{solns}\} = \{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} \text{ for any $\vv \in \{\text{solns}\}$}.
    \end{align*}

    This theorem is very useful when in the case when $\ff$ is not invertible, as it says that we can obtain the entire solution set by finding a \textit{single} solution to the equation, finding the kernel of $\ff$, and then combining these two pieces of information in a simple way.
\end{theorem}

\begin{proof}
    \mbox{} \\

    Since the theorem assumes solutions to the linear equation exist, we know there is some $\vv \in \{\text{solns}\}$.
    
    \indent (Case: $\ff$ is invertible). In this case, $\ker(\ff) = \{\mathbf{0}\}$ and $\vv = \ff^{-1}(\ww)$, so $\{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} = \{ \vv \} = \{\ff^{-1}(\ww)\}$. Since $\ff$ is invertible, the solution to the linear equation is unique, and $\{\text{solns}\} = \{\ff^{-1}(\ww)\}$. Thus $\{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} = \{\ff^{-1}(\ww)\} = \{\text{solns}\}$.

    (Case: $\ff$ is not invertible). We need to show $\{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff)\} \subseteq \{\text{solns}\}$ and $\{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff)\} \supseteq \{\text{solns}\}$.

    \indent \indent ($\subseteq$). Let $\uu$ be in the first set, so $\uu = \vv + \vv_0$ for some $\vv_0 \in \ker(\ff)$. We have $\ff(\uu) = \ff(\vv + \vv_0) = \ff(\vv) + \ff(\vv_0) = \ff(\vv) + \mathbf{0} = \ff(\vv) = \ww$. Overall, we have $\ff(\uu) = \ww$, so $\uu$ is in the second set.

    \indent \indent ($\supseteq$). Let $\uu$ be in the second set, so $\ff(\uu) = \ww$. We have $\ff(\uu - \vv) = \ff(\uu) - \ff(\vv) = \ww - \ww = \mathbf{0}$, so $\uu - \vv \in \ker(\ff)$. That is, $\uu - \vv = \vv_0$, where $\vv_0 \in \ker(\ff)$. We conclude that $\uu = \vv + \vv_0$, where $\vv_0 \in \ker(\ff)$, so $\uu$ is in the first set.
\end{proof}

\begin{theorem}
    (Number of solutions to linear equations).

    A linear equation has either zero, one, or infinitely many solutions.
\end{theorem}

\begin{proof}
    Let $V$ and $W$ be finite-dimensional vector spaces, let $\ff:V \rightarrow W$ be a linear function, and let $\ww \in W$. Consider the linear equation $\ff(\vv) = \ww$.

    (Case: there are no solutions).

    (Case: there are solutions). 
    
    \indent \indent (Case: $\ff$ is invertible). We saw in the previous proof that the solution set is $\{\ff^{-1}(\ww)\}$ in this case. 

    \indent \indent (Case: $\ff$ is not invertible). In general, the solution set is $\{\vv + \vv_0 \mid \vv_0 \in \ker(\ff)\}$. Since $\ff$ is not invertible, then $\ker(\ff)$ has dimension at least $1$, and so the solution set is a vector-space\footnote{Check for yourself that when $\ker(\ff)$ has dimension of at least one, the solution set is a vector space with dimension equal to that of $\ker(\ff)$.} of dimension at least $1$. Vector spaces of dimension at least $1$ contain infinitely many elements, so the solution set is infinite when $\ff$ is not invertible.
\end{proof}


\begin{deriv}
    (Equation of an $n$-dimensional plane).

    A \textit{$k$-dimensional plane in $K^n$} is a subset of $K^n$ of the form

    \begin{align*}
        \Bigg\{ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n \Big| \spc x_{n + 1} = c_1 x_1 + ... + c_k x_k + d \Bigg\}, \text{ where $c_1, ..., c_k, d \in K$}.
    \end{align*}

    Notice, for example, that a familiar line- or $1$-dimensional plane in $\R^2$- is a set of the form

    \begin{align*}
        \Bigg\{ \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \in \R^2 \Big| \spc x_2 = c x_1 + d \Bigg\} \text{, where $c, d \in \R$}.
    \end{align*}

    The type of equation satisfied by a point in an $n$-dimensional plane (an equation of the form $x_{n + 1} = a_1 x_1 + ... + a_n x_n + b$) is called a \textit{linear equation}\footnote{Although linear equations are related to linear functions (since the dot product is a bilinear function), the word ``linear`` in ``linear function'' has a different meaning than it does in ``linear equation''}. 
\end{deriv}

\begin{defn}
    (System of linear equations).
    
    A system of linear equations is simply a set of linear equations such as the following:
    
    \begin{align*}
        a_{1,1} x_1 + &... + a_{1,n} x_n = b_1, \\
        a_{2,1} x_1 + &... + a_{2,n} x_n = b_2, \\
        &\vdots \\
        a_{m,1} x_1 + &... + a_{m,n} x_n = b_m.
    \end{align*}
    
    Just as was the case with linear equations, when one considers a system of linear equations, one is typically interested in the set of points  $\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$ that satisfy all equations in the system.
\end{defn}

\begin{lemma}
    (Intersection of two $k$-dimensional planes).
    
    The intersection of two $k$-dimensional planes is either an $k$-dimensional plane, an $(k - 1)$-dimensional plane, or the empty set.
\end{lemma}

\begin{proof}
    
\end{proof}

\begin{theorem}
    (Number of solutions to systems of linear equations). 
    
    no solutions, infinitely many solutions, or exactly one solution.
\end{theorem}

\begin{proof}
    % There are three different proofs: 
    % \begin{enumerate}
    %     \item Reasoning about $n$-dimensional planes and induction \item Reasoning about $\ff(\xx) = \bb$ where $\ff:\R^n \rightarrow \R^m$ is the linear function representing the system and considering cases $n < m$, $n = m$, $n > m$
    %     \item Reasoning about $\AA \xx = \bb$, RREFing, and considering analogous cases to ones in (2)
    % \end{enumerate}
    
    Consider a system of $m$ linear equations in $n$ unknowns. Let the planes given by the equations be $P_{11}, ...,  P_{1m}$.
    
    (Case: $m$ is even). For each $i \in [1, \infty)$, set $P_{i + 1, 1} := P_{i, \sigma_i(1)} \cap P_{i, \sigma_i(2)}, P_{i + 1, 2} := P_{i, \sigma_i(3)} \cap P_{i, \sigma_i(4)}, ..., P_{i + 1, 2^{1 - (i + 1)} m} = P_{i \spc \sigma_i(2^{1 - i}m - 1)} \cap P_{i \spc \sigma_i(2^{1 - i}m)}$, where $\sigma_i:\{1, ..., n\}$ is defined as follows: if the planes $P_{i,1}, ..., P_{i,2^{1 - i}m}$ of the $i$th iteration can be reordered so that for some $j$ there is a plane ${P_{i + 1, j} = P_{i,\sigma_i(j)} \cap P_{i,\sigma_i(j + 1)}}$ of the $(i + 1)$st iteration that is the empty set, then $\sigma_i$ is that reordering; otherwise, $\sigma_i$ is the identity reordering that sends $j$ to $j$.

    We now prove inductively that the planes of the $i$th iteration are either all $n - (i + 1)$ dimensional or are all the empty set.

    (Base case).

    (Inductive case).

    \indent \indent (Case: one of the planes is the empty set). 
    
    \indent \indent (Case: none of the planes of the $i$th iteration is the empty set).

    \indent \indent \indent (Case: one of the intersections of the $(i + 1)$st iteration is the empty set). Done

    \indent \indent \indent (Case: none of the intersections of the $(i + 1)$st iteration is the empty set). By the inductive hypothesis, all of the planes are $n - (i - 1)$-dimensional. Apply the lemma
    
    (Case: $m$ is odd). 

        
    

\end{proof}

\begin{deriv}
    (Systems of linear equations correspond to vector equations).
    
    Consider the system of linear equations
    
    \begin{align*}
        a_{1,1} x_1 + &... + a_{1,n} x_n = b_1, \\
        a_{2,1} x_1 + &... + a_{2,n} x_n = b_2, \\
        &\vdots \\
        a_{m,1} x_1 + &... + a_{m,n} x_n = b_m.
    \end{align*}
    
    We can represent this system of equations in an alternate form to the above by reorganizing information. If we collect all the coefficents $a_{i,j}$ into a matrix $\AA$,
    
    \begin{align*}
        \AA
        =
        \begin{pmatrix}
            a_{1, 1} & \hdots & a_{1, n} \\
            a_{2, 1} & \hdots & a_{2, n} \\
            & \vdots & \\
            a_{m, 1} & \hdots & a_{m, n} \\
        \end{pmatrix},
    \end{align*}
    
    and also collect the $x_i$ and $b_i$ into a vectors $\xx$ and $\bb$,   
    
    \begin{align*}
        \xx
        =
        \begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix}, \quad
        \bb
        =
        \begin{pmatrix}
            b_1 \\ \vdots \\ b_n
        \end{pmatrix},
    \end{align*}
    
    then the above system of equations corresponds to the tuple $(\AA, \xx, \bb)$. We can go a step further, however.
    
    Notice that the system of linear equations that arises from the vector equation $\AA \xx = \bb$ is exactly the system of linear equations we started with. 
    
    The vector equation
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix}
            a_{1, 1} & \hdots & a_{1, n} \\
            a_{2, 1} & \hdots & a_{2, n} \\
            & \vdots & \\
            a_{m, 1} & \hdots & a_{m, n} \\
        \end{pmatrix}}_{\AA}
        \underbrace{
        \begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix}}_{\xx}
        =
        \underbrace
        {\begin{pmatrix}
            b_1 \\ \vdots \\ b_n
        \end{pmatrix}}_{\bb}
    \end{align*}
    
    is equivalent to
    
\begin{align*}
        \begin{pmatrix}
            \begin{pmatrix} a_{1, 1} & \hdots & a_{1, n} \end{pmatrix} \cdot \xx \\
            \begin{pmatrix} a_{2, 1} & \hdots & a_{2, n} \end{pmatrix} \cdot \xx \\
            \vdots \\
            \begin{pmatrix} a_{m, 1} & \hdots & a_{m, n} \end{pmatrix} \cdot \xx
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_1 \\ \vdots \\ b_n
        \end{pmatrix},
\end{align*}

which is equivalent to

\begin{align*}
    \begin{pmatrix}
            a_{1, 1} x_1 + ... + a_{1, n} x_n \\
            a_{2, 1} x_1 + ... + a_{2, n} x_n \\
            \vdots \\
            a_{m, 1} x_1 + ... + a_{m, n} x_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_1 \\ \vdots \\ b_n
        \end{pmatrix}.
\end{align*}

The set of equations obtained by equating the components of the vectors on either sides of the equality is the original system of equations we began with.

\end{deriv}

\begin{theorem}
    (Row operations are linear transformations).
\end{theorem}

\begin{remark}
    (Column operations).
\end{remark}

\begin{comment}
    \begin{lemma}
        (The rows of an invertible matrix form a basis for $K^n$).
        
        Suppose $\AA$ is an $n \times n$ matrix with entries in a field $K$. If $\AA$ is invertible, then it has a trivial kernel, and so by considering the equation $\AA \vv = \mathbf{0} \iff \sum_i v_i \aa_i = \mathbf{0}$ we see that $\AA$ has $n$ linearly independent columns, i.e., the columns of $\AA$ form a basis for $K^n$.
        
        If $\AA$ is invertible, then $\AA^\top$ is also invertible (with $(\AA^\top)^{-1} = (\AA^{-1})^\top$). And if $\AA^\top$ is invertible, then the columns of $\AA^\top$ (i.e. the rows of $\AA$ form a basis for $K^n$).
        
        In all, we see that if $\AA$ is invertible, then the rows of $\AA$ form a basis for $K^n$ and the columns of $\AA$ also form a basis for $K^n$.
    \end{lemma}
\end{comment}

\begin{lemma}
    (A linear function is invertible iff it is a composition of elementary linear functions).
    
    Equivalently, a matrix is invertible iff it is a product of elementary matrices.
\end{lemma}

\begin{proof}
   
\end{proof}

\begin{theorem}
    (A matrix is invertible iff its RREF is the identity matrix).
\end{theorem}

\begin{proof}
   
   \mbox{} \\ \indent
   ($\implies$). Consider an arbitrary invertible matrix $\AA$. Since $\AA$ is invertible, it is a product of elementary matrices, $\AA = \EE_i ... \EE_1$.

   
   
   
   ($\impliedby$). Suppose $\text{rref}(\AA) = \II$. Since RREF is obtained by performing a sequence of row operations, and since performing a row operation is equivalent to multiplying on the left by an elementary matrix, we have $\text{rref}(\AA) = \EE_i ... \EE_1$ for some elementary matrices $\EE_1, ..., \EE_i$. Thus $\EE_i ... \EE_1 \AA = \II$. Elementary matrices are invertible, so we see that
   
   [...]
   
   row operations correspond to invertible linear functions. since columns are LI iff invertible and (only) linear functions preserve LI, RREF must be identity iff invertible.
\end{proof}

\newpage

\section{Length and angle}
\label{ch::lin_alg::section::dot_product}

In this section, we develop the notions of the \textit{length of a vector} and the \textit{angle between two vectors} for vectors from a finite-dimensional vector space over $\R$. 

We approach this by first defining notions of length and angle in $\R^2$, generalizing these notions to $\R^n$, and then generalizing again to finite-dimensional vector spaces over $\R$.

It is easy to understand length in $\R^2$, and easy to generalize length in $\R^2$ to length in $\R^n$. All of the nuance in this section arises in the analysis of angle in $\R^2$. Once angle in $\R^2$ is understood, it too is easily generalized to $\R^n$.

\vspace{.5cm}

We briefly outline the analysis of angle in $\R^2$.

Immediately after defining the angle between vectors in $\R^2$, we stumble across a fundamental concept that underlies both length and angle- the \textit{dot product} of vectors in $\R^2$. The dot product is a subtle concept that requires careful explanation. Unfortunately, all texts I have seen have some sort of major error in their presentation of this important concept.

Most authors define the dot product using the ``algebraic characterization'' ${\vv \cdot \ww := \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i}$, and then use the law of cosines to show that this definition implies the ``geometric characterization'', ${\vv \cdot \ww = ||\vv|| \spc ||\ww|| \cos(\theta)}$, where $\theta$ is the angle between $\vv$ and $\ww$. This approach is incorrect because using the law of cosines gives no intuition\footnote{When used as a starting point, the law of cosines is unintuitive because the validity of the law of cosines is established via Euclidean geometry, which is less intuitive than geometry done via linear algebra (to me, at least).}, and because there is typically no motivation given for the algebraic characterization; it just ``pops out of nowhere''.

Some authors try to provide more motivation for the dot product by starting with the geometric characterization, $\vv \cdot \ww := ||\vv|| \spc ||\ww|| \cos(\theta)$, as the definition, explaining that this characterization is a natural idea\footnote{The motivation goes like this. If a box is pushed with a force $\FF \in \R^2$ along a floor through a displacement $\xx$, then $||\FF|| \cos(\theta)$ is the amount of force exerted in the direction of the displacement $\xx$, and thus $||\xx||(||\FF|| \cos(\theta)) = ||\FF|| \spc ||\xx|| \cos(\theta)$ is the product of the amount of force aligned with the displacement with the magnitude of the displacement.}, and then showing that the algebraic characterization follows. This is an improvement over starting abruptly with the algebraic characterization, to be sure, but unfortunately, I have always seen that authors use the law of cosines to show the algebraic characterization follows from the geometric characterization. As mentioned before, the law of cosines gives no intuition.

\textit{Both} of the typical approaches to $(\text{algebraic characterization}) \implies (\text{geometric characterization})$ and $(\text{geometric characterization}) \implies (\text{algebraic characterization})$ share a major failing not yet mentioned: they don't engage with a satisfactory definition of angle. Either angle is implicitly- because of the use of the law of cosines- taken to be as given in the cumbersome and imprecise definition from Euclidean geometry, or, when the algebraic characterization of the dot product is used as the definition, a rigorous but unmotivated definition\footnote{The rigorous but unmotivated definition is $\theta(\vv, \ww) := \arccos(\hvv \cdot \hww)$, where the notation $\hvv$ and $\hww$ is defined in Definition \ref{ch::lin_alg::defn::unit_vector_hat_notation}.} of angle is given, and the equivalence of this definition to the (cumbersome and imprecise) one relied upon by the law of cosines is not explained.

All of this mess is alleviated in this section. We start by investigating a rigorous definition of angle between vectors in $\R^2$. Our investigations of angle cause us to stumble upon the algebraic characterization of the dot product. The geometric characterization immediately follows, though it follows logically- not quite intuitively. To provide the missing intuition, we present proofs of $(\text{geometric characterization}) \implies (\text{algebraic characterization})$ and $(\text{algebraic characterization}) \implies (\text{geometric characterization})$ that do not rely on the law of cosines. Lastly, we are able to use our knowledge of the dot product to provide a useful formula for computing vector projections, and, most satisfyingly, to prove the law of cosines.

\vspace{.5cm}

The \textit{cross product} also comes with two common pedagogical problems. The last section of Chapter \ref{ch::exterior_pwrs} describes these problems and presents a satisfying explanation of the cross product. (The cross product is addressed in Chapter \ref{ch::exterior_pwrs} because understanding cross products requires understanding determinants).

\newpage

\subsection*{Length in $\R^n$}
\label{ch::lin_alg::subsection::mag_and_angle_Rn}

\begin{defn}
\label{ch::lin_alg::defn::length_in_R2}
    (Length of a vector in $\R^2$).

    Let $\vv \in \R^2$. If we interpret $([\vv]_\sE)_1$ and $([\vv]_\sE)_2$ as the lengths of the base and height, respectively, of a right triangle, then the length of the hypotenuse of this right triangle is $\sqrt{([\vv]_\sE)_1^2 + ([\vv]_\sE)_2^2}$. Since $\vv = ([\vv]_\sE)_1 \see_1 + ([\vv]_\sE)_2 \see_2$, it makes sense to think of the hypotenuse as being $\vv$, and thus, to define the \textit{length} (or \textit{magnitude}) $||\vv||$ of $\vv$ to be the length of the hypotenuse: $||\vv|| := \sqrt{([\vv]_\sE)_1^2 + ([\vv]_\sE)_2^2}$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::length_in_Rn}
    (Length of a vector in $\R^n$). 

    When $n > 2$ and we have a vector $\vv_n \in \R^n$, we consider $\vv_n$ in the form $\vv_n = \vv_{n - 1} + ([\vv]_\sE)_n \see_n$, where $\vv_{n - 1} \in \R^{n - 1}$, and define the \textit{length} (or \textit{magnitude}) $||\vv_n||$ of $\vv_n$ to be

    \begin{align*}
        ||\vv_n|| = \sqrt{||\vv_{n - 1}||^2 + ([\vv]_\sE)_n^2} \text{ when $n > 2$}.
    \end{align*}
\end{defn}

\begin{theorem}
    (Length of a vector in $\R^n$).

    For all $\vv \in \R^n$ and all $n$, the length of $\vv$ is

    \begin{align*}
        ||\vv|| = \sqrt{\sum_{i = 1}^n ([\vv]_\sE)_i^2}\spc.
    \end{align*}
\end{theorem}

\begin{proof}
    We prove the theorem by induction.

    (Base case). The definition of the length of a vector in $\R^2$ establishes the base case.

    (Inductive case). Let $\vv_n \in \R^n$. Assume as the inductive hypothesis that the claim is true for $n$; we need to show the claim is true for $n + 1$.

    By the definition the length of a vector in $\R^{n + 1}$ (Definition \ref{ch::lin_alg::thm::length_in_Rn}), for $\vv_{n + 1} \in \R^{n + 1}$ we have \\ ${||\vv_{n + 1}|| = \sqrt{||\vv_n||^2 + ([\vv]_\sE)_{n + 1}^2}}$. By the inductive hypothesis, $||\vv_n|| = \sqrt{\sum_{i = 1}^n ([\vv]_\sE)_i^2}^2$, so

    \begin{align*}
        ||\vv_{n + 1}|| = \sqrt{\sqrt{\sum_{i = 1}^n \Big(([\vv]_\sE)_i^2\Big)}^2 + ([\vv]_\sE)_{n + 1}^2} = \sqrt{\sum_{i = 1}^n \Big([\vv]_\sE)_i^2\Big) + ([\vv]_\sE)_{n + 1}^2} = \sqrt{\sum_{i = 1}^{n + 1} ([\vv]_\sE)_i^2}.
    \end{align*}

    This proves the claim for $n + 1$.
\end{proof}

\begin{defn}
\label{ch::lin_alg::defn::unit_vector_hat_notation}
    (Unit vector hat notation). 
    
    For $\vv \in \R^n$, we define the notation $\hat{\vv} := \frac{\vv}{||\vv||}$. We have $||\hat{\vv}|| = 1$ for all $\vv \in \R^n$.
\end{defn}

\subsection*{Angle in $\R^2$}

\begin{defn}
\label{ch::lin_alg::defn::angle_in_R2}
    (Angle between vectors in $\R^2$).
    
    Let vectors $\vv, \ww \in \R^2$ have the same length $r = ||\vv|| = ||\ww||$, suppose the initial points of $\vv$ and $\ww$ coincide, and consider the circle of radius $r$ that has the coinciding initial points of $\vv$ and $\ww$ as its center. Let $s(\xx, \vv, \ww)$ be the length traversed on the path $\xx:\R \rightarrow \R^2$ from $\vv$ to $\ww$:

    \begin{align*}
        s(\xx, \vv, \ww) :=  \int_{t_1}^{t_2} \Big|\Big|\frac{d\xx(t)}{dt}\Big|\Big| dt, 
        \text{where $\xx(t_1) = \vv$, $\xx(t_2) = \ww$}, t_1 \leq t_2.
    \end{align*}

    Now let $\cc:\R \rightarrow \R^2$ be the traditional parameterization of the circle of radius $r$ with center $\mathbf{0}$, and $\widetilde{\cc}$ be the reverse parameterization:

    \begin{align*}
        \cc(t) := r
        \begin{pmatrix}
            \cos(t) \\
            \sin(t)
        \end{pmatrix}
        \text{ and }
        \widetilde{\cc}(t) := \cc(-t).
    \end{align*}
    
    \textbf{find and replace ``unsigned''.}
    \textbf{$t \in [0, 2\pi)$ should probably be $t \in [0, \pi]$}

    We define the \textit{angle $\theta(\vv, \ww)$ between $\vv$ and $\ww$} to be the ratio of the shortest path between $\vv$ and $\ww$ on the circle to the radius of the circle:
    
    \begin{align*}
        \theta(\vv, \ww) := \frac{\min(s(\cc, \vv, \ww), s(\widetilde{\cc}, \vv, \ww))}{r}.
    \end{align*}
    
    When $\vv$ and $\ww$ don't have the same length, we define $\theta(\vv, \ww) := \theta(r_0\hvv, r_0\hww)$, where $r_0 > 0$ is any positive real number.
\end{defn}

\begin{remark}
    (Well-definedness of angle between two vectors).

    Since $\theta(\vv, \ww)$ depends on the radius $r$ of the circle whose center coincides with the initial points of $\vv$ and $\ww$, it isn't yet clear that $\theta:\R^2 \times \R^2 \rightarrow \R$ is well-defined.
\end{remark}

\begin{theorem}
    (Properties of $\theta$).
    
    \begin{enumerate}
        \item $\theta(\vv, \ww) \geq 0$ for all $\vv, \ww \in \R^2$.
        \item $\theta(\vv, \ww) = \theta(\ww, \vv)$ for all $\vv, \ww \in \R^2$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
        \item We have $s(\xx, \vv, \ww) \geq 0$ for all $\xx \in \{\cc, \widetilde{\cc}\}$ because the integrand in $s(\xx, \vv, \ww)$ is nonnegative for all $\xx \in \{\cc, \widetilde{\cc}\}$ and $t_1 \leq t_2$.

        Since $\theta(\vv, \ww) = \frac{s(\yy, \vv, \ww)}{r}$ for some $\yy \in \{\cc, \widetilde{\cc}\}$, then because $s(\yy, \vv, \ww) \geq 0$ and $r > 0$, we have $\theta(\vv, \ww) = \frac{s(\yy, \vv, \ww)}{r} \geq 0$. 
        \item We have $s(\xx, \vv, \ww) = s(\xx, \ww, \vv)$ for all $\vv, \ww \in \R^2$ and $\xx \in \{\cc, \widetilde{\cc}\}$ because the length of a path is equal to the length of the reverse path. 
        
        Since $\theta(\vv, \ww) = \frac{s(\yy, \vv, \ww)}{r}$ for some $\yy \in \{\cc, \widetilde{\cc}\}$, then $\theta(\yy, \vv, \ww) = \frac{s(\yy, \vv, \ww)}{r} = \frac{s(\yy, \ww, \vv)}{r} = \theta(\yy, \ww, \vv)}$ for all $\vv, \ww \in \R^2$.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Perpendicular vectors in $\R^2$). We say that $\vv, \ww \in \R^2$ are \textit{perpendicular}, or \textit{orthogonal}, iff $\theta(\vv, \ww) = \frac{\pi}{2}$.
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::unsigned_angle_Rn}
    (Angle formula in $\R^2$).
        
    \begin{align*}
        \theta(\vv, \ww) = \text{arccos}\Big( \sum_{i = 1}^2 ([\hvv]_\sE)_i  ([\hww]_\sE)_i \Big) \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}

    Note, since $\arccos:[-1, 1] \rightarrow [0, \pi]$, we have $\theta(\vv, \ww) \in [0, \pi]$ for all $\vv, \ww \in \R^2$.
\end{theorem}

\begin{proof}
    We prove the theorem in the case when $||\vv|| = ||\ww|| = r$; the general case follows from this immediately. For some $\xx \in \{\cc, \widetilde{\cc}\}$, we have

    \begin{align*}
        \theta(\vv, \ww) = \frac{s(\xx, \vv, \ww)}{r} = \frac{1}{r} \int_{t_1}^{t_2} \Big|\Big| \frac{d\xx(t)}{dt} \Big|\Big| dt, \text{ where $\xx(t_1) = \vv$, $\xx(t_2) = \ww$}, t_1 \leq t_2.
    \end{align*}
    
    We have $||\frac{d\xx(t)}{dt}|| = r$ for all $\xx \in \{\cc, \widetilde{\cc}\}$, so $\theta(\vv, \ww) = t_2 - t_1$. Since, $\xx(t_1) = r \hvv$ and $\xx(t_2) = r \hww$ for all $\xx \in \{\cc, \widetilde{\cc}\}$, then

    \begin{align*}
        {r \cos(\pm t_1) = (r[\hvv]_\sE)_1} &\text{ and } {r \cos(\pm t_2) = (r[\hww]_\sE)_1},
    \end{align*}

    where the $\pm$ signs are both $+$ when $\xx = \cc$ and are both $-$ when $\xx = \widetilde{\cc}$. Thus
   
   \begin{align*}
       \theta(\vv, \ww) = t_2 - t_1 &= \pm \text{arccos}\Big(([\hww]_\sE)_1\Big) \mp \text{arccos}\Big(([\hvv]_\sE)_1\Big),
   \end{align*}

   where the $+, -$ case occurs when $\xx = \cc$ and the $-, +$ case occurs when $\xx = \widetilde{\cc}$.
   
   The cosine angle difference identity $\cos(\alpha - \beta) = \cos(\alpha) \cos(\beta) + \sin(\alpha) \sin(\beta)$ implies\footnote{To obtain the second identity, write the first identity in terms of $\gamma, \delta$ by making the substitutions ${\gamma := \cos(\alpha) \iff \alpha = \arccos(\gamma)}$ and ${\delta = \cos(\beta) \iff \beta = \arccos(\delta)}$, and take $\arccos$ of both sides.} the identity \\ ${\text{arccos}(\gamma) - \text{arccos}(\delta) = \text{arccos}(\gamma \delta + \sqrt{1 - \gamma^2}\sqrt{1 - \delta^2})}$. In the $+, -$ case of the above, this identity gives
   
    \begin{align*}
       \theta(\vv, \ww) &= \text{arccos}\Bigg(([\hww]_\sE)_1 ([\hvv]_\sE)_1 + \sqrt{1 -([\hww]_\sE)_1^2} \sqrt{1 - ([\hvv]_\sE)_1^2} \Bigg) \\
       &= \text{arccos}\Bigg(([\hww]_\sE)_1 ([\hvv]_\sE)_1 + ([\hww]_\sE)_2 ([\hvv]_\sE)_2 \Bigg).
   \end{align*}
   
   In the $-, +$ case, applying the identity yields an analogous (and ultimately equivalent) result.
\end{proof}

\begin{theorem}
    (Angle between two vectors is well-defined).

    The formula for $\theta(\vv, \ww)$ given by the previous theorem does not depend on the radius $r$ of the circle whose center coincides with the initial points of $\vv$ and $\ww$, so we now know that $\theta(\vv, \ww)$ is well-defined.
\end{theorem}

\begin{defn}
    (The dot product of unit vectors).

    Let $\vv, \ww \in \R^2$. The expression that appears inside $\arccos$ in the formula for $\theta(\vv, \ww)$,

    \begin{align*}
        \sum_{i = 1}^2 ([\hvv]_\sE)_i  ([\hww]_\sE)_i,
    \end{align*}
    
    is denoted $\hvv \cdot \hww$ and is called the \textit{dot product of $\hvv$ and $\hww$}. 
\end{defn}

With this new definition, the formula for angle in $\R^2$ suddenly has a new, more compact form.

\begin{theorem}
    (Angle formula in $\R^2$).

    \begin{align*}
        \theta(\vv, \ww) = \arccos(\hvv \cdot \hww) \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}
\end{theorem}

The formula from the previous definition can also be used to define a dot product of non-unit vectors.

\begin{defn}
    (The dot product in $\R^2$). 

    The \textit{dot product $\vv \cdot \ww$ of $\vv, \ww \in \R^2$} is
    
    \begin{align*}
        \vv \cdot \ww := \sum_{i = 1}^2 ([\vv]_\sE)_i  ([\ww]_\sE)_i.
    \end{align*}    
\end{defn}

\begin{theorem}
    (Length in $\R^2$ in terms of dot product in $\R^2$).

    \begin{align*}
        ||\vv|| = \sqrt{\vv \cdot \vv} \text{ for all $\vv \in \R^2$}.
    \end{align*}
\end{theorem}

\begin{proof}
    We have $\vv \cdot \vv = ([\vv]_\sE)_1^2 + ([\vv]_\sE)_2^2$. Take the square root of of both sides to obtain the result.
\end{proof}

\begin{theorem}
    (The dot product in $\R^2$ is bilinear).

    The dot product in $\R^2$ is a bilinear function. That is, both $\vv \mapsto \vv \cdot \ww$ and $\ww \mapsto \vv \cdot \ww$ are linear functions.
\end{theorem}

\begin{proof}
    The dot product in $\R^2$ is symmetric, so it suffices to show that it is linear in either argument; it suffices to show that $\vv \mapsto \vv \cdot \ww$ is a linear function. That is, it suffices to show that for all $\vv_1, \vv_2, \ww \in \R^2$ we have $(\vv_1 + \vv_2) \cdot \ww = \vv_1 \cdot \ww + \vv_2 \cdot \ww$ and for all $\vv, \ww \in \R^2$ and $c \in \R$ we have $(c\vv) \cdot \ww = c(\vv \cdot \ww)$.
    
    Note that because $[\cdot]_\sE$ is linear we have $([\vv_1 + \vv_2]_\sE)_i = ([\vv_1]_\sE)_i + ([\vv_2]_\sE)_i$ and $([c\vv]_\sE)_i = c([\vv]_\sE)_i$ for all $i$. Using this fact, we see

    \begin{align*}
        (\vv_1 + \vv_2) \cdot \ww
        &= \sum_{i = 1}^2 ([\vv_1 + \vv_2]_\sE)_i ([\ww]_\sE)_i
        = \sum_{i = 1}^2 \Big(([\vv_1]_\sE)_i + ([\vv_2]_\sE)_i\Big) ([\ww]_\sE)_i \\
        &= \sum_{i = 1}^2 \Big( ([\vv_1]_\sE)_i ([\ww]_\sE)_i + ([\vv_2]_\sE)_i ([\ww]_\sE)_i \Big)
        = \sum_{i = 1}^2 ([\vv_1]_\sE)_i ([\ww]_\sE)_i + \sum_{i = 1}^2 ([\vv_2]_\sE)_i ([\ww]_\sE)_i \\
        &= \vv_1 \cdot \ww + \vv_2 \cdot \ww
    \end{align*}

    and

    \begin{align*}
        (c\vv) \cdot \ww
        = \sum_{i = 1}^2 ([c\vv]_\sE)_i ([\ww]_\sE)_i
        = \sum_{i = 1}^2 c([\vv]_\sE)_i ([\ww]_\sE)_i
        = c \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
        = c(\vv \cdot \ww),
    \end{align*}

    as claimed.    
\end{proof}

With this expanded definition of the dot product, it's possible to express the dot product of $\vv$ and $\ww$ in terms of the angle between $\vv$ and $\ww$.

\begin{theorem}
    (Dot product in $\R^2$ in terms of angle in $\R^2$).

    \begin{align*}
        \vv \cdot \ww = ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww)) \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}
\end{theorem}

\begin{proof}
    From the previous theorem, we have $\theta(\vv, \ww) = \arccos\Big( \frac{\vv}{||\vv||} \cdot \frac{\ww}{||\ww||} \Big)$. Since the dot product is bilinear, we have $\frac{\vv}{||\vv||} \cdot \frac{\ww}{||\ww||} = \frac{1}{||\vv||}\Big(\vv \cdot \frac{\ww}{||\ww||}\Big) = \frac{1}{||\vv||\spc||\ww||}(\vv \cdot \ww) = \frac{\vv \cdot \ww}{||\vv|| \spc ||\ww||}$. Thus $\theta(\vv, \ww) = \arccos\Big(\frac{\vv \cdot \ww}{||\vv|| \spc ||\ww||}\Big)$. Take $\cos$ of both sides and then multiply both sides by $||\vv|| \spc ||\ww||$ to obtain the result.
\end{proof}

\begin{remark}
    (The dot product of perpendicular vectors in $\R^2$ is zero).

    It follows immediately from the previous theorem that $\vv, \ww \in \R^2$ are perpendicular iff $\vv \cdot \ww = 0$.
\end{remark}

At this point, we've seen that the ``algebraic'' characterization of the dot product, $\vv \cdot \ww := \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i$, and the ``geometric'' characterization of the dot product, $\vv \cdot \ww = ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww))$, are true (with the algebraic characterization being true by definition), but don't yet understand their connection. We now discover how each characterization follows from the other.

\newpage

\subsection*{Prerequisites for understanding the dot product}

\subsubsection*{Vector projection}

\begin{lemma}
    (Orthogonal basis of $\R^2$).

    Let $\vv \in \R^2$. If $\vv_\perp$ is perpendicular to $\vv$, then $\{\vv, \vv_\perp\}$ is a basis of $\R^2$.
\end{lemma}

\begin{proof}
    Recall from Lemma \ref{ch::lin_alg::lemma::n_lin_indep_vectors_span_n_dim_space} that $n$ linearly independent vectors taken from the same $n$-dimensional vector space span that vector space. Since $\dim(\R^2) = 2$, it suffices to show that $\vv, \vv_\perp$ are linearly independent.

    Suppose for contradiction that $\vv, \vv_\perp$ are linearly dependent. Then there exist $c, d \in \R$ not both zero such that $c\vv + d\vv_\perp = \mathbf{0}$. Assume without loss of generality that $c \neq 0$. Then we can divide by $c$, and we have $\vv = -\frac{d}{c}\vv_\perp$. Taking the dot product of each side with $\vv$, we obtain $\vv \cdot \vv = -\frac{d}{c}\vv_\perp \cdot \vv$. Since $\vv$ and $\vv_\perp$ are perpendicular, we have $\vv_\perp \cdot \vv = 0$ and thus $\vv \cdot \vv = 0$, so $\vv$ is perpendicular to itself. Since $\vv$ was arbitrary, this shows that every vector in $\R^2$ is perpendicular to itself. This is clearly not true; contradiction.
\end{proof}

\begin{defn}
\label{ch::lin_alg::defn::vector_proj}
    (Vector projection in $\R^2$).
    
    Consider vectors $\vv, \ww, \ww_\perp \in \R^2$, where $\ww_\perp$ is perpendicular to $\ww$. Since $\{\hww, \hww_\perp\}$ is a basis of $\R^2$, there exist unique $v_{||}, v_\perp \in \R$ such that $\vv = v_{||} \hww + v_\perp \hww_\perp$.
    
    The \textit{vector projection} of $\vv$ is $\proj(\vv \rightarrow \ww) := v_{||} \hww$.

    The \textit{signed magnitude of the vector projection} of $\vv$ onto $\ww$ is $\sproj(\vv \rightarrow \ww) := v_{||}$. 
\end{defn}

\begin{remark}
    Note that, for $\vv, \ww \in \R^2$, we have $\proj(\vv \rightarrow \ww) = \proj(\vv \rightarrow \hww)$ because $\hat{\hww} = \hat{\ww}$.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::thm::vector_proj_Signed_magnitude}
    (Vector projection and its signed magnitude in $\R^2$).

    Let $\vv, \ww \in \R^2$. We have the following.
    
    \begin{enumerate}
        \item $\proj(\vv \rightarrow \ww) = \sproj(\vv \rightarrow \ww) \hww$.
        \item $\sproj(\vv \rightarrow \ww) = \pm ||\proj(\vv \rightarrow \ww)||$.
    \end{enumerate}

    This second fact is what should be emphasized when thinking about the signed magnitude of a vector projection.
\end{theorem}

\begin{proof}
    (1) follows from the definition of $\proj(\vv \rightarrow \ww)$. (2) is obtained by taking the magnitude of each side of (1).
\end{proof}

\subsubsection*{Orthogonal linear functions}

\begin{theorem}
\label{ch::lin_alg::lemma::orthogonal_linear_fns_preserve_alg_dot_product}
    (Preserved quantities and linear functions).
    
    If $\ff:\R^2 \rightarrow \R^2$ is a linear function, then the following statements are equivalent:

    \begin{enumerate}
        \item ($\ff$ preserves length). $||\vv|| = ||\ff(\vv)||$ for all $\vv \in \R^2$.
        \item ($\ff$ preserves dot product). $\vv \cdot \ww = \ff(\vv) \cdot \ff(\ww)$ for all $\vv, \ww \in \R^2$.
        \item ($\ff$ preserves length and angle). $||\vv|| = ||\ff(\vv)||$ for all $\vv \in \R^2$ and $\theta(\vv, \ww) = \theta(\ff(\vv), \ff(\ww))$ for all $\vv, \ww \in \R^2$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{itemize}
        \item ((1) $\implies$ (2)). Suppose $\ff$ preserves length. If we showed that $\vv \cdot \ww$ depends on $||\vv||$ and $||\ww||$ (on lengths), it would follow that $\ff$ preserves dot product. This is what we will do.

        We need to show that $\vv \cdot \ww$ depends on $||\vv||$ and $||\ww||$. Note that $||\vv||^2 = \vv \cdot \vv$ for $\vv \in \R^n$. So \\ $||\vv + \ww||^2 = (\vv + \ww) \cdot (\vv + \ww) = \vv \cdot \vv + 2 \vv \cdot \ww + \ww \cdot \ww = ||\vv||^2 + 2 \vv \cdot \ww + ||\ww||^2$. Solve for $\vv \cdot \ww$ to see that $\vv \cdot \ww$ does indeed depend on $||\vv||$ and $||\ww||$:
        
       \begin{align*}
            \vv \cdot \ww = \frac{1}{2}\Big(||\vv + \ww||^2 - (||\vv||^2 + ||\ww||^2) \Big).
        \end{align*}
        
        ((1) $\impliedby$ (2)). If $\ff$ preserves dot product, then $||\ff(\vv)|| = \sqrt{\ff(\vv) \cdot \ff(\vv)} = \sqrt{\vv \cdot \vv} = ||\vv||$, so $\ff$ preserves length.
        
        \item \mbox{} \\
        ((2) $\implies$ (3)). If $\ff$ preserves length, then, since angle is a function of quantities that are preserved by $\ff$ (dot product and length), $\theta(\vv, \ww) = \arccos\Big( \frac{\vv \cdot \ww}{||\vv|| \spc ||\ww||} \Big)$, it too is preserved by $\ff$. \\
        
        ((2) $\impliedby$ (3)). Suppose $\ff$ preserves angle: $\theta(\vv, \ww) = \theta(\ff(\vv), \ff(\ww))$ for all $\vv, \ww \in \R^2$. Then $\ff(\vv) \cdot \ff(\ww) = ||\ff(\vv)|| \spc ||\ff(\ww)|| \cos(\theta(\ff(\vv), \ff(\ww)))$
    \end{itemize}
\end{proof}

\begin{defn}
\label{ch::lin_alg::defn::orthogonal_linear_fn_Rn}
    (Orthogonal linear function).
    
    We say that a linear function $\ff:\R^n \rightarrow \R^n$ is \textit{orthogonal} iff it preserves dot product.
\end{defn}

\begin{remark}
    (Orthogonal linear function).
    
    ``Orthogonal linear function'' is a somewhat misleading name. It's true that certain facts about orthogonal linear functions involve orthogonal \textit{vectors}, but orthogonal linear functions aren't ``perpendicular'' to other orthogonal linear functions in some sense.
\end{remark}

\subsubsection*{Rotation}

\begin{defn}
    (Rotation in $\R^2$).

    We define the \textit{rotation $\RR_\theta$ by $\theta \in [0, 2\pi)$} to be the linear function $\RR_\theta:\R^2 \rightarrow \R^2$ that sends $\see_1$ to $(\cos(\theta), \sin(\theta))^\top$ and $\see_2$ to $(-\sin(\theta), \cos(\theta))$.
\end{defn}

Drawing a picture and using trigonometry confirms that this definition agrees with our intuition on rotation.

\begin{theorem}
    (Rotation in $\R^2$).

    We have

    \begin{align*}
        \RR_\theta(\vv) =
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
        \vv
        \text{ for all $\vv \in \R^2$}.
    \end{align*}
\end{theorem}

\begin{theorem}
    (A rotation by $\phi$ changes angle by $\phi$). We have $\cos(\theta(\vv, \RR_\phi(\ww))) = \cos(\theta(\vv, \ww) + \phi)$ for all $\phi \in [0, 2\pi)$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
    \end{align*}
\end{proof}

\begin{proof}
    Let $\cc$ and $\widetilde{\cc}$ be the parameterizations of the circle of radius $r$ with center $\mathbf{0}$ from Definition \ref{ch::lin_alg::defn::angle_in_R2}.
    
    We have

    \begin{align*}
        \theta(\vv, \RR_s(\ww)) =
        \frac{\min(s(\cc, \vv, \RR_\phi(\ww)), s(\widetilde{\cc}, \vv, \RR_\phi(\ww)))}{r}
        =
        \begin{cases}
            \theta(\vv, \ww) + \phi & \theta(\vv, \ww) + \phi \leq \pi \\
            2\pi - (\theta(\vv, \ww) + \phi) & \theta(\vv, \ww) + \phi > \pi
        \end{cases}.
    \end{align*}

    Take $\cos$ of both sides to obtain the result.
\end{proof}

\begin{theorem}
    Rotations in $\R^2$ are orthogonal linear functions.
\end{theorem}

\begin{proof}
    It suffices to show that rotations preserve length. So, for $\vv \in \R^2$, consider $||\vv||$ and $||\RR_\theta(\vv)||$. 
    
    We have $\RR_\phi(\vv) = \RR_\phi(||\vv|| \hvv) = ||\vv|| \RR_\phi(\hvv)$. Since ${\hvv = (\cos(\theta), \sin(\theta))^\top}$ for some $\theta \in [0, 2\pi)$, ...
\end{proof}

\begin{theorem}
    $\RR_\theta(\hvv) = \widehat{\RR_\theta(\vv)}$
\end{theorem}

\begin{proof}
    
\end{proof}

\begin{lemma}
    \label{ch::lin_alg::lemma::rotated_projection}
    (Rotated projection is projection of rotated vectors). 
    
    Let $\vv, \ww \in \R^2$. For any $\theta \in [0, 2\pi)$, we have $\RR_\theta(\proj(\vv \rightarrow \ww) = \proj(\RR_\theta(\vv) \rightarrow \RR_\theta(\ww))$.
\end{lemma}

\begin{proof}
    We have $\vv = v_{||} \hvv + v_\perp \hvv_\perp$, so $\RR_\theta(\vv) = v_{||} \ff(\hvv) + v_\perp \RR_\theta \hvv_\perp$. The claim follows if we show (1) that $\RR_\theta(\hww) = \widehat{\RR_\theta(\hww)}$ and (2) that $\RR_\theta(\hww_\perp) = \widehat{\RR_\theta(\ww)}_\perp$. (1) is true because rotations are length-preserving. (2) is true because there exists a rotation that sends $\ww$ to $\ww_\perp$, because rotations commute with each other, and because rotations are length-preserving.
\end{proof}

\subsection*{Geometric characterization $\implies$ algebraic characterization}

\begin{lemma}
    (Signed projection onto a vector in $\R^2$ is a linear function).
    
    Let $\vv_1, \vv_2 \in \R^2$. The map $\vv_1 \mapsto \sproj(\vv_1 \rightarrow \vv_2)$ is linear.
\end{lemma}

\begin{proof}
    Let $\vv_1, \vv_2, \ww \in \R^2$, and define $\ff:\R^2 \rightarrow \R$ by $\ff(\vv) = \proj(\vv \rightarrow \ww)$. We need to show that $\ff(\vv_1 + \vv_2) = \ff(\vv_1) + \ff(\vv_2)$ and that $\ff(c\vv) = c\ff(\vv)$ for all $c \in \R$. 
    
    We have
    
    \begin{align*}
        \ff(\vv_1 + \vv_2) &= \ff\Big(\Big((v_1)_{||}\hat{\ww} + (v_1)_{\perp}\hat{\ww}_\perp \Big) + \Big((v_2)_{||}\hat{\ww} + (v_2)_\perp \hat{\ww}_\perp \Big) \Big), \text{ where $(v_i)_{||} = \sproj(\vv_i \rightarrow \ww)$ for $i \in \{1, 2\}$} \\
        &= \ff \Big(\Big((v_1)_{||} + (v_2)_{||} \Big) \hat{\ww} + \Big((v_1)_\perp + (v_2)_\perp) \Big) \hat{\ww}_\perp \Big) \\
        &= \sproj\Big(\Big[ \Big((v_1)_{||} + (v_2)_{||} \Big) \hat{\ww} + \Big((v_1)_\perp + (v_2)_\perp \Big) \hat{\ww}_\perp \Big] \rightarrow \ww \Big) \\
        &= (v_1)_{||} + (v_2)_{||} 
        = \sproj(\vv_1 \rightarrow \ww) + \sproj(\vv_2 \rightarrow \ww) = \ff(\vv_1) + \ff(\vv_2).
    \end{align*}

    And we have
    
    \begin{align*}
        \ff(c \vv) &= 
        \ff \Big(c \Big(v_{||}\hat{\ww} + v_{\perp}\hat{\ww}_\perp \Big) \Big), \text{ where $v_{||} = \sproj(\vv \rightarrow \ww)$} \\
        &= \ff\Big(cv_{||}\hat{\ww} + cv_{\perp}\hat{\ww}_\perp \Big)
        = \sproj\Big(\Big(cv_{||}\hat{\ww} + cv_{\perp}\hat{\ww}_\perp \Big) \rightarrow \ww \Big) \\
        &= c v_{||} \hat{\ww} = c \sproj(\vv \rightarrow \ww) = c \ff(\vv).
    \end{align*}
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::geom_dot_product_bilinear}
    (The dot product in $\R^2$ is a bilinear function).
    
    The dot product in $\R^2$ is a bilinear function. That is, both $\vv \mapsto \vv \cdot \ww$ and $\ww \mapsto \vv \cdot \ww$ are linear functions.
\end{theorem}

\begin{proof}
    The dot product in $\R^2$ is symmetric, so it suffices to show that it is a linear function in either argument; it suffices to show that $\ff:\R^2 \rightarrow \R$ defined by $\ff(\vv) = \vv \cdot \ww$ is a linear function. Well, ${\ff(\vv) = ||\ww||\sproj(\vv \rightarrow \ww)}$, and we know that $\vv \mapsto \sproj(\vv \rightarrow \ww)$ is linear. Since $\ff$ is the result of scaling a linear function by $||\ww||$, it too is a linear function.
\end{proof}

Knowing that the dot product is bilinear allows us to derive an alternate ``algebraic'' (as opposed to ``geometric'') expression for the dot product of two vectors in $\R^2$. Before we derive this alternate expression, though, we will need to note the following basic fact.

\begin{lemma}
    Let $\sE = \{\see_1, \see_2 \}$ be the standard basis for $\R^2$. We have
    
    \begin{align*}
        \see_i \cdot \see_j = 
        \begin{cases}
            1 & i = j \\
            0 & i \neq j
        \end{cases}
        = \delta_{ij}
    \end{align*}

    for all $i, j \in \{1, 2\}$.
\end{lemma}

\begin{proof}
   The lemma is equivalent to the statement $\sproj(\see_i \rightarrow \see_j) = \delta_{ij}$, which is easily proved.
\end{proof}

\begin{deriv}
    (Algebraic dot product in $\R^2$). 
    
    We can now derive the algebraic characterization of the dot product by using its bilinearity (Theorem \ref{ch::lin_alg::thm::geom_dot_product_bilinear}) together with the previous lemma.
    
    We will make use of the following general fact about bilinear functions\footnote{If $V_1, V_2$, and $W$ are vector spaces over a field $K$, then $\ff:V_1 \times V_2 \rightarrow W$ is a \textit{bilinear} function iff $\vv \mapsto \ff(\vv, \ww)$ and $\ww \mapsto \ff(\vv, \ww)$ are both linear functions.}. If $V$ is a finite-dimensional vector space over a field $K$ with a basis $E = \{\ee_i\}_{i = 1}^n$, then a bilinear function $B:V \times V \rightarrow K$ satisfies
    
    \begin{align*}
        B(\vv, \ww) &= B\Big(\sum_{i = 1}^n ([\vv]_E)_i \ee_i, \sum_{i = 1}^n ([\vv]_E)_j \ee_j\Big) \\
        &= \sum_{i = 1}^n ([\vv]_E)_i B\Big(\ee_i, \sum_{i = 1}^n ([\vv]_E)_j \ee_j\Big) \\
        &= \sum_{i = 1}^n \sum_{j = 1}^n ([\vv]_E)_i ([\ww]_E)_i B(\ee_i, \ee_j).
    \end{align*}
    
    In the above, we've written $\vv$ and $\ww$ as sums of basis vectors and then used the linearity of $B$ in each argument.
    
    The dot product is a bilinear function $\cdot:\R^2 \times \R^2 \rightarrow \R$, so the above fact can be applied to the dot product. We know from the previous lemma that $B(\see_i, \see_j) = \delta_{ij}$, so we have
    
    \begin{align*}
        \vv \cdot \ww = \sum_{i = 1}^2 \sum_{j = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i \delta_{ij} = \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i.
    \end{align*}
    
    Therefore
    
    \begin{align*}
        \boxed
        {
            \vv \cdot \ww = \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
        }
    \end{align*}
\end{deriv}

\subsection*{Algebraic characterization $\implies$ geometric characterization}

\begin{deriv}
    (Algebraic characterization $\implies$ geometric characterization).
    
    If

    \begin{align*}
        \vv \cdot \ww := \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
    \end{align*}

    then
    
    \begin{align*}
        \vv \cdot \ww := \sproj(\vv \rightarrow \ww) \spc ||\ww|| \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}
\end{deriv}

\begin{proof}
    Let $\vv, \ww \in \R^2$ and consider $\vv \cdot \ww$. We can\footnote{Use $\theta = \arccos(||\vv||)$.} choose $\theta \in [0, 2\pi)$ so that $\RR_\theta(\hvv) = \see_1$, i.e., so that $\RR_\theta(\vv) = ||\vv||\see_1$. With this choice of $\theta$, we have
    
    \begin{align*}
        \vv \cdot \ww 
        = \RR_\theta(\vv) \cdot \RR_\theta(\ww) 
        = ||\vv|| \see_1 \cdot \RR_\theta(\ww)
        = 
        \begin{pmatrix} ||\vv|| \\ 0 \end{pmatrix}
        \cdot 
        \begin{pmatrix} ([\RR_\theta(\ww)]_\sE)_1 \\ ([\RR_\theta(\ww)]_\sE)_2 \end{pmatrix} 
        = 
        ||\vv|| ([\RR_\theta(\ww)]_\sE)_1.
    \end{align*}
    
    We have
    
    \begin{align*}
        ([\RR_\theta(\ww)]_\sE)_1
        &= \proj(\RR_\theta(\ww) \rightarrow \see_1) 
        = \proj(\RR_\theta(\ww) \rightarrow \RR_\theta(\hvv))
        = \proj(\RR_\theta(\ww) \rightarrow \widehat{\RR_\theta(\vv)}) \\
        &= \proj(\RR_\theta(\ww) \rightarrow \RR_\theta(\vv)) 
        = \proj(\ww \rightarrow \vv),
    \end{align*}
    
    where the last equality is by Lemma \ref{ch::lin_alg::lemma::rotated_projection_length}. \\ Therefore $\vv \cdot \ww = ||\proj(\ww \rightarrow \vv)|| \spc ||\vv||$. Since the dot product is symmetric, we can swap $\vv$ and $\ww$; this gives the desired result.
\end{proof}

\subsection*{Length and angle in $\R^n$}

The algebraic characterization of the dot product in $\R^2$ generalizes straightforwardly to $\R^n$.

\begin{defn}
    (Dot product in $\R^n$).

    We define the \textit{dot product of $\vv, \ww \in \R^n$} to be

    \begin{align*}
        \vv \cdot \ww = \sum_{i = 1}^n ([\vv]_\sE)_i ([\ww]_\sE)_i.
    \end{align*}
\end{defn}

\begin{theorem}
    (Length in $\R^n$).

    For all $\vv \in \R^n$ we have $||\vv|| = \sqrt{\vv \cdot \vv}$.
\end{theorem}

\begin{proof}
    We have $\vv \cdot \vv = \sum_{i = 1}^n ([\vv]_\sE)_i^2$. Take the square root of both sides to obtain the result.
\end{proof}

In $\R^2$, we were able to derive that $\theta(\vv, \ww) = \arccos(\hvv \cdot \hww)$. This result inspires our definition of angle in $\R^n$.

\begin{defn}
    (Angle in $\R^n$). 
    
    We define the \textit{angle between $\vv, \ww \in \R^n$} to be

    \begin{align*}
        \theta(\vv, \ww) := \arccos(\hvv \cdot \hww).
    \end{align*}
\end{defn}

\subsubsection*{Vector projection in terms of dot produt}

In the case of $n = 2$, we could have proved the following theorem immediately after defining $\vv \cdot \ww := \sproj(\vv \rightarrow \ww) \spc ||\ww||$ for $\vv, \ww \in \R^2$ in Definition [...]. The usefulness of this next theorem, however, wouldn't have been apparent until knowing that $\vv \cdot \ww = \sum_{i = 1}^n ([\vv]_\sE)_i ([\ww]_\sE)_i$.

\begin{theorem}
\label{ch::lin_alg::thm::vector_proj_dot_product}
    (Vector projection in $\R^n$ terms of dot product in $\R^n$).
    
    If $\vv, \ww \in \R^n$, then
    
    \begin{align*}
        \boxed
        {
            \proj(\vv \rightarrow \ww) = \frac{\vv \cdot \ww}{||\ww||} \hww = \frac{\vv \cdot \ww}{\ww \cdot \ww} \ww
        }
    \end{align*}
    
    In the case $n = 2$, if we didn't know the algebraic characterization of the dot product, this theorem would be a bit of a tautology. However, knowing the algebraic characterization of the dot product allows for easy computation of the expressions $\vv \cdot \ww$ and $\ww \cdot \ww$ whenever we know the coordinates of $\vv$ and $\ww$ relative to the standard basis\footnote{Actually, we only need to know the coordinates of $\vv$ and $\ww$ relative to some \textit{orthonormal} basis! Orthonormal bases, also known as \textit{self-dual bases}, are discussed in Section \ref{ch::bilinear_forms_metric_tensors::section::self-duality}.} for $\R^2$.
\end{theorem}

\begin{proof}
   Since $\vv \cdot \ww = \sproj(\vv \rightarrow \ww) \spc ||\ww||$, then $\sproj(\vv \rightarrow \ww) = \frac{\vv \cdot \ww}{||\ww||}$, and, by the first fact of Theorem \ref{ch::lin_alg::thm::vector_proj_Signed_magnitude}, $\proj(\vv \rightarrow \ww) = \sproj(\vv \rightarrow \ww) \hww = \frac{\vv \cdot \ww}{||\ww||} \hww$.
\end{proof}

\subsubsection*{The law of cosines}

The next theorem presents the law of cosines as it is best understood, which is as a consequence of the equivalence between the geometric and algebraic dot product formulas.

\begin{theorem}
\label{ch::lin_alg::thm::law_of_cosines}
    (Law of cosines in $\R^n$). 
    
    We have

    \begin{align*}
        ||\vv - \ww||^2 = ||\vv||^2 + ||\ww||^2 - 2 ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww)).
    \end{align*}
    
    Note that when $\theta(\vv, \ww) = 0$, we recover the Pythagorean theorem.
\end{theorem}

\begin{proof}
    We have $||\vv - \ww||^2 = (\vv - \ww) \cdot (\vv - \ww) = \vv \cdot \vv - 2 \vv \cdot \ww + \ww \cdot \ww = ||\vv||^2 + ||\ww||^2 - 2 ||\vv|| \spc ||\ww|| \cos(\theta)$.
\end{proof}

\newpage

\subsection*{Inner product spaces}

This section generalizes the notion of the dot product, which we only know to be applicable to $\R^n$, to general vector spaces.

\begin{defn}
    \label{ch::lin_alg::defn::inner_product}
    (Inner product).
    
    Let $V$ be a vector space over a field $K$. An \textit{inner product} on $V$ is a function $\langle \cdot, \cdot \rangle:V \times V \rightarrow K$ with the following properties:

    \begin{enumerate}
        \item (Bilinearity). The functions $\vv \mapsto \langle \vv, \ww \rangle$ and $\ww \mapsto \langle \vv, \ww \rangle$ are linear for all $\vv, \ww \in V$. In other words $\langle \cdot, \cdot \rangle$ is ``linear in each argument''.
        \item (Symmetry). $\langle \vv, \ww \rangle = \langle \ww, \vv \rangle$ for all $\vv, \ww \in V$.
        \item (Nondegeneracy). $\langle \vv, \vv \rangle = 0 \implies \vv = \mathbf{0}$ for all $\vv \in V$.
    \end{enumerate}
\end{defn}

\begin{remark}
    (Inner product conventions).

    Some authors define inner products to satisfy the ``positive-definitness'' criterion rather than the nondegeneracy criterion. A function $V \times V \rightarrow K$ is \textit{positive-definite} iff $\langle \vv, \vv \rangle \geq 0$ for all $\vv \in V$, with $\langle \vv, \vv \rangle = 0$ occurring only when $\vv = \mathbf{0}$. A \textit{negative-definite} inner product is defined similarly and involves the condition $\langle \vv, \vv \rangle \leq 0$ for all $\vv \in V$.
\end{remark}

\begin{defn}
    (Inner product space).
    
    Let $V$ be a vector space over $K$. Iff there is an inner product $\langle \cdot, \cdot \rangle$ on $V$, then $V$ is called a \textit{vector space with inner product}, or an \textit{inner product space}.
\end{defn}

\begin{example}
    \mbox{} \\ \indent
    The dot product in $\R^n$ is an inner product on $\R^n$. (Proof left as exercise). 
    
    The dot product in $K^n$, defined analogously to the dot product in $\R^n$, is in general \textit{not} an inner product because it is not positive-definite. For example, we have $\begin{pmatrix} 3 \\ 3 \end{pmatrix} \cdot \begin{pmatrix} 3 \\ 3 \end{pmatrix} = 0$ when these vectors are elements of $\Z/9\Z$. (In $\Z/9\Z$, we have $3 \cdot 3 = 9 = 0$).
\end{example}

\subsubsection*{Length and orthogonality with respect to an inner product}

\begin{defn}
    (Length of a vector with respect to an inner product). 
    
    Let $V$ be an inner product space. In analogy to the fact that the length of a vector in $\R^n$ can be expressed using the dot product in $\R^n$ (see Theorem \ref{ch::lin_alg::thm::length_in_Rn}), we define the \textit{length of a vector $\vv \in V$ with respect to the inner product on $V$} to be $||\vv|| := \sqrt{\langle \vv, \vv \rangle}$.
\end{defn}

\begin{defn}
    (Angle between vectors with respect to an inner product). 
    
    Let $V$ be an inner product space. In analogy to the the fact that the angle between vectors $\vv_1, \vv_2 \in \R^n$ is $\arccos(\hat{\vv}_1 \cdot \hat{\vv}_2)$, we define the \textit{angle $\theta$ between vectors $\vv_1, \vv_2 \in V$ with respect to the inner product on $V$} to be $\theta := \arccos(\langle \hat{\vv}_1, \hat{\vv}_2 \rangle)$. Note that $\hat{\vv}_i = \frac{\vv_i}{||\vv_i||} = \frac{\vv_i}{\sqrt{\langle \vv_i, \vv_i \rangle}}$.
\end{defn}

\begin{remark}
    (Geometric inner product).
    
    Let $V$ be an inner product space. Then $\langle \vv_1, \vv_2 \rangle = ||\vv_1||\spc||\vv_2|| \cos(\theta)$, where $\theta$ is the angle between $\vv_1$ and $\vv_2$ with respect to the inner product on $V$. (This fact is the generalization of the geometric dot product in $\R^n$, which was discussed in Theorem \ref{ch::lin_alg::thm::dot_prod_and_angle}).
\end{remark}

\begin{theorem}
\label{ch::bilinear_forms_metric_tensors::thm::Cauchy_Schwarz}
     (Cauchy-Schwarz inequality for vector spaces over $\R$).
     
     Let $V$ be a vector space over $\R$ with inner product. Then the \textit{Cauchy-Schwarz inequality} holds: \\ ${\langle \vv_1, \vv_2 \rangle \leq ||\vv_1|| \spc ||\vv_2|| \text{ for all $\vv_1, \vv_2 \in V}}$. 
     
     Note, the Cauchy-Schwarz inequality is equivalent to the statement that the angle $\theta$ in $V$ between $\vv_1$ and $\vv_2$ with respect to the inner product on $V$ satisfies $\theta \in [0, 2 \pi)$.
\end{theorem}

\begin{proof}
    Define $f:\R \rightarrow [0, \infty) \subseteq \R$ by $f(k) = \langle k \vv_1 + \vv_2, k \vv_1 + \vv_2 \rangle = k^2 \langle \vv_1, \vv_1 \rangle + 2 k \langle \vv_1, \vv_2 \rangle + \langle \vv_2, \vv_2 \rangle$. Set $a := \langle \vv_1, \vv_1 \rangle$, $b := 2 \langle \vv_1, \vv_2 \rangle$, and $c := \langle \vv_2, \vv_2 \rangle$, so that $f(k) = ak^2 + bk + c$.
    
    Since $\langle \cdot, \cdot \rangle$ is positive-definite, then $f$ is nonnegative, and therefore must have either one or zero real roots. According to the quadratic formula, one real root occurs when $b^2 - 4ac = 0$, and zero real roots occur when $b^2 - 4ac < 0$. So, we must have $b^2 - 4ac \leq 0$. 
    
    Using our expressions for $a, b$, and $c$, we see that $(4 \langle \vv_1, \vv_2 \rangle)^2 - 4(\langle \vv_1, \vv_2 \rangle)(\langle \vv_2, \vv_2 \rangle) \leq 0$. Thus \\ ${\langle \vv_1, \vv_2 \rangle^2 \leq \langle \vv_1, \vv_1 \rangle \langle \vv_2, \vv_2 \rangle = ||\vv_1||^2 ||\vv_2||^2}$. Take the square root of each side to obtain the result.  
\end{proof}

\begin{defn}
    (Orthogonality of vectors with respect to an inner product). 
    
    Let $V$ be an inner product space. We say vectors $\vv_1, \vv_2 \in V$ are \textit{orthogonal with respect to the inner product on $V$} iff the angle between $\vv_1$ and $\vv_2$ is $\frac{\pi}{2}$. That is, $\vv_1, \vv_2 \in V$ are orthogonal iff $\langle \vv_1, \vv_2 \rangle = 0$.
\end{defn}

\newpage

\begin{defn}
    (Orthonormal basis with respect to an inner product).
    
     Let $V$ be a finite-dimensional vector space with inner product $\langle \cdot, \cdot \rangle$. We say a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$ is \textit{orthonormal (with respect to $\langle \cdot, \cdot \rangle$)} iff
     
     \begin{itemize}
         \item $||\ee_i|| = 1$ for all $i$
         \item $\ee_i$ and $\ee_j$ are orthogonal to each other when $i \neq j$
     \end{itemize}
     
     That is, $E$ is an orthonormal basis iff $\langle \ee_i, \ee_j \rangle = \delta^i{}_j$ for all $i, j$.
\end{defn}

\begin{theorem}
\label{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}
    (Gram-Schmidt algorithm).
    
    Let $V$ be a finite-dimensional inner product space. Given any basis $E = \{\ee_1, ..., \ee_n\}$ for $V$, we can use the following \textit{Gram-Schmidt algorithm} to convert $E$ into an orthonormal basis $\hU = \{\huu_1, ..., \huu_n\}$.
    
    First, we ``orthogonalize'' the basis $E$ into a basis $F = \{\ff_1, ..., \ff_n\}$. Set $\ff_1 := \ee_1$, and, for $i \geq 2$, set
    
    \begin{align*}
        \ff_i := \ee_i - \proj(\ff_i \rightarrow \spann(\ee_1, ..., \cancel{\ee_i}, ..., \ee_n)) = \ee_i - \sum_{j \neq i} \proj(\ff_i \rightarrow \ee_j) = \ee_i - \sum_{j \neq i} \frac{\langle \ff_i, \ee_j \rangle}{\langle \ee_j, \ee_j \rangle}, \quad i \geq 2.
    \end{align*}
    
    (In the last equality in the line above, we've used an analogue of Theorem \ref{ch::lin_alg::thm::vector_proj_dot_product} to express vector projections in terms of inner products). 
    To obtain the orthonormal basis $\hU = \{\huu_1, ..., \huu_n\}$, we just normalize the orthogonal basis $F$, and set $\huu_i := \frac{\ff_i}{||\ff_i||}$.
\end{theorem}

\begin{remark}
    The above theorem reveals that the algebraic dot product (in $\R^2) can also be discovered as an orthogonality condition between vectors. When $\vv_1, \vv_2 \in \R^2$ are orthogonal, they form a right triangle, so Pythagorean theorem gives $||\vv_1||^2 + ||\vv_2||^2 = ||\vv_1 - \vv_2||^2$. Use $||\vv_i||^2 = \sum_{j = 1}^2 (([\vv_i]_\sE)_j)^2$ to discover that we must have $([\vv_1]_\sE)_1 ([\vv_2]_\sE)_1 + ([\vv_1]_\sE)_2 ([\vv_2]_\sE)_2 = 0$.
\end{remark}

\newpage

\section{Eigenvectors and eigenvalues}

[This section not required for rest of book]

\begin{defn}
    (Eigenvectors and eigenvalues of a linear function).
    
    Let $V$ and $W$ be vector spaces over a field $K$, and let $\ff:V \rightarrow W$ be a linear function. A vector $\vv \in V$ is said to be an \textit{eigenvector (of $\ff$)} iff there is a scalar $c \in K$ such that $\ff(\vv) = c \vv$. Iff $\vv$ is indeed an eigenvector of $\ff$, then the $c \in K$ for which $\ff(\vv) = c \vv$ is said to be the \textit{eigenvalue corresponding to $\vv$}.
    
    In other words, the eigenvectors of a linear function are the vectors that get sent to scalar multiples of themselves by the function, and the eigenvalues corresponding to those eigenvectors are the scalars involved in said scalar multiples.
\end{defn}

\begin{remark}
    (``Characteristic vectors'').
    
    The word ``eigen'' within ``eigenvector'' is German. Back when eigenvectors were first defined, English-speaking mathematicians called them ``characteristic vectors''. This terminology is not used today, but is helpful to keep it in mind, as we will see that, in some cases, a linear function is completely specified if its ``characteristic vectors'' and corresponding ``characteristic values'' are known.
\end{remark}

\begin{theorem}
    Let $V$ be a vector space. A linear function $\ff:V \rightarrow V$ is invertible iff $\mathbf{0}$ is the only eigenvector of $\ff$ whose eigenvalue is $0$.
\end{theorem}

\begin{proof}
   Consider the contrapositive. Some nonzero eigenvector $\vv$ has $0$ as its eigenvalue iff $\ff$ has a nontrivial kernel, which is equivalent to $\ff$ not being invertible.
\end{proof}

\begin{remark}
    (Eigenvectors, eigenvalues, and zero).
    
    According to the above definition, $\mathbf{0}$ is an eigenvector of every linear function, with its corresponding eigenvalue being $0$. 
    
    Some authors explicitly disallow $\mathbf{0}$ from being an eigenvector of any linear function in their definition of ``eigenvector'' so that the condition ``$\mathbf{0}$ is the only eigenvector of $\ff$ whose eigenvalue is $0$'' is equivalent to the condition ``$0$ is not an eigenvalue of $\ff$''. The later is easier to say than the former. This convention is a bit too contrived, though, so we will not use it.
\end{remark}

\begin{theorem}
    (Intuition for an invertibility condition).
    
    [intuition on what a determinant is]
    
    $\ff$ is not invertible iff for any basis $E$ of $V$, the set $\ff(E)$ is linearly dependent

    [$n$-dimensional volume spanned by a linearly dependent set is $0$]
\end{theorem}

\begin{deriv}
    (Eigenvectors).

     Let $V$ and $W$ be vector spaces over a field $K$ and let $\ff:V \rightarrow W$ be a linear function. A vector $\vv \in V$ is an eigenvector of $\ff$ with eigenvalue $c$ iff

    \begin{align*}
        &\quad \quad \quad \ff(\vv) = c \vv \\
        &\iff \ff(\vv) - c \vv = \mathbf{0} \\
        &\iff \ff(\vv) - c\II(\vv) = \mathbf{0} \\
        &\iff (\ff - c\II)(\vv) = \mathbf{0} \\
        &\iff \vv \in \ker(\ff - c\II).
    \end{align*}

    Thus,

    \begin{align*}
        \boxed
        {
            \text{$\vv$ is an eigenvector of $\ff$ with eigenvalue $c$ iff $\vv \in \ker(\ff - c\II)$}
        }
    \end{align*}
\end{proof}

\begin{deriv}
    (Eigenvalues).

    Let $V$ and $W$ be vector spaces over a field $K$ and let $\ff:V \rightarrow W$ be a linear function.
    
    A scalar $c \in K$ is an eigenvalue of $\ff$ if and only if $\ker(\ff - c\II)$ is nonempty, which is the case if and only if $\ff - c\II$ is not invertible. This is equivalent to the condition $\det(\ff - c\II) = 0$. Thus,

    \begin{align*}
        \boxed
        {
            \text{$c$ is an eigenvalue of $\ff$ iff $\det(\ff - c\II) = 0$}
        }
    \end{align*}
\end{deriv}

\begin{}
