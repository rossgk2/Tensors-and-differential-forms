\chapter{Introduction to tensors}
\label{ch::motivated_intro}

In this chapter, we introduce the idea of a \textit{tensor}, since tensors underpin differential forms. There are two key ideas that we must formalize before we define what a tensor is.

One of the ideas is that of a ``multilinear element''. Recall that elements of vector spaces (vectors) can be thought of as ``linear elements''. Thus, we can say ``linear functions respect the decomposition of linear elements''. After defining the notion of \textit{multilinear function}, we will define the notion of ``multilinear elements''. Multilinear elements will be objects that (in a sense) are respected by multilinear functions. Formally, mutlilinear elements will be elements of \textit{tensor product spaces}. 

There are two main contributions of tensor product spaces to the overarching theory of tensors: tensor product spaces formalize the structure of how ``multilinear things'' behave, and they allow multilinear functions to be identified with linear functions. Tensor product spaces do not account for the entire theory of tensors, though, even though the name might make you think this. One more key idea, described below, is required.

The second key idea in the theory of tensors is to think of linear functions as vectors- that is, as elements of vector spaces. We achieve this by decomposing linear functions into linear combinations of simpler linear functions. Most introductory linear algebra classes approach this idea by proving the fact that the set of $m \times n$ matrices form a vector space. We take this idea and run with it to discover \textit{dual spaces}. Soon after, we utilize the two key ideas, (1) tensor product spaces and (2) dual spaces, to prove the theorem which underlies the definition of a \textit{$(p, q)$ tensor}.

\section{Multilinear functions and tensor product spaces}

\begin{defn}
    (Multilinear function).
    
    Let $V_1, ..., V_k, W$ be vector spaces over a field $K$. We say a function $\ff:V_1 \times ... \times V_k \rightarrow W$ is \textit{$k$-linear}, or \textit{multilinear}, iff for all $\vv_1 \in V_1, ..., \vv_i \in V_i, ..., \vv_k \in V_k$, the function $\ff_i:V_i \rightarrow W$ defined by $\ff_i(\vv_1, ..., \vv_i, ..., \vv_k) = \ff(\vv_1, ..., \vv_i, ..., \vv_k)$ is linear. In other words, $\ff$ is multilinear iff it is ``linear in each argument''. 
    
    Note that a $1$-linear function is simply a linear function. A $2$-linear function is called a \textit{bilinear function}.
\end{defn}

\begin{example}
    (Example of a multilinear function). 
    
    The dot product on $\R^n$ is a bilinear function on $\R^n \times \R^n$.
\end{example}

\begin{defn}
    (Vector space of multilinear functions).
    
    If $V_1, ..., V_k, W$ are vector spaces over a field $K$, then we use $\LLLL(V_1 \times ... \times V_k \rightarrow W)$ to denote the vector space over $K$ of $k$-linear functions $V_1 \times ... \times V_k \rightarrow W$ under the operations of function addition and function scaling.
    
    Notice, the case $k = 1$ is consistent with our definition in Theorem \ref{ch::lin_alg::thm::vector_space_linear_functions} of $\LLLL(V_1 \rightarrow W)$ as being the set of linear functions $V_1 \rightarrow W$.
\end{defn}

\begin{proof}
     The proof that $\LLLL(V_1 \times ... \times V_k \rightarrow W)$ is indeed a vector space is left as an exercise.
\end{proof}

We know from our study of linear algebra that the decomposition of vectors is respected by linear functions (see Remark \ref{ch::lin_alg::rmk::linear_means_vector}). Now, having just been introduced to the notion of a \textit{multilinear} function. Natural questions are then: ``What objects do multilinear functions respect? Could these be thought of as 'multilinear elements'?`` 

The following definition gives the answer and formalizes this notion. ``Multilinear elements'' will be called \textit{tensors}.

\newpage

\begin{defn}
\label{ch::motivated_intro::defn::tensor_product_space}
    \smallcite{book::SM}{307} (Tensor product space).
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces over a field $K$. The \textit{tensor product space} $V_1 \otimes ... \otimes V_k$ is defined to be the vector space over $K$ whose elements are from the set $V_1 \times ... \times V_k$, where the elements are also subject to an equivalence relation $=$, which will be specified soon. We also denote a typical element of $V_1 \otimes ... \otimes V_k$ as $\vv_1 \otimes ... \otimes \vv_k$, rather than as $(\vv_1, ..., \vv_k)$. 
    
    The equivalence relation is defined by the condition that
    
    \begin{align*}
        \vv_1 \otimes ... \otimes \vv_{i - 1} &\otimes \vv_{i} \otimes \vv_{i + 1} ... \otimes \vv_k \\
        &+ \\
        \vv_1 \otimes ... \otimes \vv_{i - 1} &\otimes \vv_{j} \otimes \vv_{i + 1} ... \otimes \vv_k \\
        &= \\
        \vv_1 \otimes ... \otimes \vv_{i - 1} \otimes (\vv_{i} &+ \vv_{j}) \otimes \vv_{i + 1} ... \otimes \vv_k
    \end{align*}

    for all $\vv_1, ..., \vv_k \in V$, and
    
    \begin{align*}
        c (\vv_1 \otimes ... &\otimes \vv_i \otimes ... \otimes \vv_k ) \\
        &= \\
        \vv_1 \otimes ... &\otimes (c \vv_i) \otimes ... \otimes \vv_k
    \end{align*}

    for all $\vv_1, ..., \vv_k \in V$ and $c \in K$. This is so that $\otimes$ looks like it is a multilinear function.
    
    Elements of tensor product spaces are called ``tensors''.
\end{defn}

% \begin{remark}
%     (Tensor terminology). 
    
%     Some authors use the word ``tensor'' to mean ``$(p, q)$ tensor''. (We have not defined $(p, q)$ tensors yet, but we will in Definition \ref{ch::motivated_intro::defn::pq_tensor}). We will use the word ``tensor'' to either mean an element of a tensor product space or a $(p, q)$ tensor, but we only do this when the meaning is clear from context.
% \end{remark}

We will soon see that tensors truly are multilinear elements, as their decompositions are respected by multilinear functions, but first we need to address some basics.

\begin{defn}
    (Elementary tensor). 
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces, and consider the tensor product space $V_1 \otimes ... \otimes V_k$. An element of $V_1 \otimes ... \otimes V_k$ that is of the form $\vv_1 \otimes ... \otimes \vv_k$ is called an \textit{elementary tensor}. Intuitively, an elementary tensor is a tensor that does not need to be expressed as linear combination of two or more other nonzero tensors. An element of $V_1 \otimes ... \otimes V_k$ that is not an elementary tensor is called a \textit{nonelementary tensor}.
\end{defn}

\begin{remark}
    (Extending with linearity).

    Let $V_1, ..., V_k, W$ be finite-dimensional vector spaces. Suppose we define a function $\ff:V_1 \otimes ... \otimes V_k \rightarrow W$ on elementary tensors, and suppose we desire a linear function $\gg$ that agrees with $\ff$ on elementary tensors. There does indeed exist\footnote{For an idea on how to prove this, revisit Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}.} a unique such $\gg$.
    
    Because of this, we can say things such as ``we \textit{impose} that $\ff$ be linear'', or ``we \textit{extend} the definition of $\ff$ \textit{with linearity}'', to indicate the definition ``$\ff(\TT) := \gg(\TT)$ when $\TT$ is a nonelementary tensor''.
\end{remark}

Now we present the property of tensors that motivated their definition.

\begin{theorem}
\label{ch::lin_alg::thm::universal_prop_tensor_prod}
    (Universal property of the tensor product).

    Let $V_1, ..., V_k, W$ be vector spaces, and let $\ff:V_1 \times ... \times V_k \rightarrow W$ be a multilinear function. 
    
    There exists a linear function $\widetilde{\ff}:V_1 \otimes ... \otimes V_k \rightarrow W$ uniquely corresponding to $\ff$ such that $\ff(\vv_1, ..., \vv_k) = \widetilde{\ff}(\vv_1 \otimes ... \otimes \vv_k)$.

    Equivalently, there are linear functions $\widetilde{\ff}:V_1 \otimes ... \otimes V_k \rightarrow W$ and $\gg:V_1 \times ... \times V_k \rightarrow V_1 \otimes ... \otimes V_k$, with $\gg$ defined on elementary tuples as $\gg(\vv_1, ..., \vv_k) := \vv_1 \otimes ... \otimes \vv_k$, and with $\widetilde{\ff}$ uniquely corresponding to $\ff$, such that $\ff = \widetilde{\ff} \circ \gg$.
\end{theorem}

\begin{proof}
    We define $\widetilde{\ff}$ on elementary tensors by $\widetilde{\ff}(\vv_1 \otimes ... \otimes \vv_k) := \ff(\vv_1, ..., \vv_k)$ and extend with linearity.
\end{proof}

\begin{remark}
    (The sense in which multilinear functions respect multilinear elements).
        
   This theorem tells us how multilinear elements are respected by multilinear functions: if $\ff$ is a multilinear function and $\TT = \sum_{i_1, ..., i_k} T_{i_1 ... i_k} \vv_{i_1} \otimes ... \otimes \vv_{i_k}$ is a multilinear element (a tensor), then, since $\widetilde{\ff}$ is linear, it respects the decomposition of $\TT$. So, if we squint, we can imagine that the multilinear function $\ff$ is acting on $\TT$ and preserving its decomposition by means of the linear function $\widetilde{\ff}$.
\end{remark}

Using the previous theorem to define a natural isomorphism of vector spaces yields the following result.

\begin{theorem}
\label{ch::motivated_intro::thm::multilin_fns_iso_lin_fns}
    (Multilinear functions are naturally identified with linear functions on tensor product spaces).
    
    Let $V_1, ..., V_k, W$ be finite-dimensional vector spaces. Then the vector space of multilinear functions ${V_1 \times ... \times V_k \rightarrow W}$ is naturally isomorphic to the vector space of linear functions ${V_1 \otimes ... \otimes V_k \rightarrow W}$:
    
    \begin{align*}
        \LLLL(V_1 \times ... \times V_k \rightarrow W) \cong \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W).
    \end{align*}
\end{theorem}

\begin{proof}
    To construct a linear isomorphism $\FF:\LLLL(V_1 ... \times V_k \rightarrow W) \mapsto \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W)$, we define $\FF(\ff) = \widetilde{\ff}$, where $\widetilde{\ff}$ is from Theorem \ref{ch::lin_alg::thm::universal_prop_tensor_prod}. We need to show that $\FF$ is a linear bijection. 
    
    First, we find an explicit expression for $\widetilde{\ff}$. Since $\ff = \widetilde{\ff} \circ \gg$ and since $\gg$ has a trivial kernel, then $\gg$ is invertible and $\widetilde{\ff} = \ff \circ \gg^{-1}$.

    Now we can show that $\FF$ is linear. The map is linear because, given vector spaces $Y, Z, W$, the function $\circ$ which composes linear functions, ${\circ:\LLLL(Y \rightarrow Z) \times \LLLL(Z \rightarrow W) \rightarrow \LLLL(Y \rightarrow W)}$, is a bilinear map. (Check this fact for yourself. Other consequences of this are explored in Derivation \ref{ch::bilinear_forms_metric_tensors::deriv::compos_linear_map_with_contract}). 
    
    To show $\FF$ is bijective, since it is a map between vector spaces of the same dimension, it suffices to show that it is one-to-one, i.e., that it has a trivial kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}). This is clearly the case because if $\FF(\ff) = \widetilde{\ff} = \mathbf{0}$, then $\ff = \widetilde{\ff} \circ \gg = \mathbf{0} \circ \gg = \mathbf{0}$.
\end{proof}

\begin{theorem}
\label{ch::motivated_intro::thm::basis_dim_tensor_product_space}
    (Basis and dimension of a tensor product space). 
    
    Let $V_1, ..., V_k$ be finite-dimensional vector spaces with bases $E_1, ..., E_k$, respectively, where $E_i = \{\ee_{i1}, ..., \ee_{in_i}\}$, and where $\dim(V_i) = n_i$. Then $V_1 \otimes ... \otimes V_k$ is a $n_1 n_2 ... n_k$ dimensional vector space with basis
    
    \begin{align*}
        \bigcup_{i = 1}^k \bigotimes_{\vv \in E_i} \vv = 
        \{ \ee_{1i_1} \otimes ... \otimes \ee_{ki_k} \mid i_k \in \{1, ..., n_k\} \}.
    \end{align*}
\end{theorem}

\begin{proof}      
      To show that this set spans $V_1 \otimes ... \otimes V_k$, it suffices to show it spans the set of elementary tensors in $V_1 \otimes ... \otimes V_k$, since any tensor in $V_1 \otimes ... \otimes V_k$ is a linear combination of elementary tensors. For an elementary tensor $\vv_1 \otimes ... \otimes \vv_k \in V_1 \otimes ... \otimes V_k$, we have $\vv_1 \otimes ... \otimes \vv_k = \sum_{i_1, ..., i_k} ([\vv_1]_{E_1})^{i_1} ... ([\vv_k]_{E_k})^{i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k}$ by the seeming-multilinearity of $\otimes$.
      
      To show linear independence, assume that $\sum_{i_1, ..., i_k} T_{i_1 ... i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k} = \mathbf{0}$. We must show that $T_{i_1 ... i_k} = 0$ for all $i_1, ..., i_k$. Note that a tensor in $V_1 \otimes ... \otimes V_k$ is the zero tensor iff\footnote{($\implies$). We have $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k = 0 \cdot (\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k) = \mathbf{0}$. ($\impliedby$). Suppose $\sum_{i_1, ..., i_k} T_{i_1 ... i_k} \ee_{i_1} \otimes ... \otimes \ee_{i_k}$ is the zero tensor. By the previous direction, $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k$ is also the zero tensor. So the two are equal.} it is an elementary tensor of the form $\vv_1 \otimes ... \otimes \vv_{i - 1} \otimes \mathbf{0} \otimes \vv_{i + 1} \otimes ... \otimes \vv_k$ for some $i$. Due to the linear independence of the bases $E_1, ..., E_k$, it is impossible to obtain a $\mathbf{0}$ in any position of the tensor product unless all $T_{i_1 ... i_k}$ are $0$.
\end{proof}

\begin{theorem}
    \label{ch::motivated_intro::thm::tensor_product_associative}
    (Associativity of tensor product). 
    
    Let $V_1, V_2, V_3$ be finite-dimensional vector spaces. Then there are natural isomorphisms
    
    \begin{align*}
        (V_1 \otimes V_2) \otimes V_3 \cong V_1 \otimes V_2 \otimes V_3 \cong V_1 \otimes (V_2 \otimes V_3).
    \end{align*}
    
    That is, these spaces are ``the same'', since an element of one can ``naturally'' be identified as an element of the other. (See Definition \ref{ch::lin_alg::defn::linear_iso} for a discussion of linear isomorphisms). These identifications are ``natural'' in the sense that they do not depend on a choice of basis (see Definition \ref{ch::lin_alg::defn::natural_iso}).

    Because of this isomorphism, we will always assume $\vv_1 \otimes (\vv_2 \otimes \vv_3) = \vv_1 \otimes \vv_2 \otimes \vv_3 = (\vv_1 \otimes \vv_2) \otimes \vv_3$ for all $\vv_1 \in V_1, \vv_2 \in V_2, \vv_3 \in V_3$.
\end{theorem}

\begin{proof}
    We construct a linear isomorphism $\FF:V_1 \otimes V_2 \otimes V_3 \rightarrow (V_1 \otimes V_2) \otimes V_3 \rightarrow V_3$; construction of a linear isomorphism $V_1 \otimes V_2 \otimes V_3 \rightarrow V_1 \otimes (V_2 \otimes V_3)$ is similar.

    We define $\FF$ on elementary tensors by $\vv_1 \otimes \vv_2 \otimes \vv_3 \overset{\FF}{\mapsto} (\vv_1 \otimes \vv_2) \otimes \vv_3$ and extend with linearity.
    
    $\FF$ is onto, as, since any ${\SS \in (V_1 \otimes V_2) \otimes V_3}$ is a linear combination of elementary tensors, $\SS = \sum_{i, j, k} T_{ijk} (\vv_i \otimes \vv_j) \otimes \vv_k$, then if $\TT := \sum_{i,j,k} T_{ijk} \vv_i \otimes \vv_j \otimes \vv_k$ we have $\FF(\TT) = \SS$.

    Since $\FF$ is a linear and onto map between vector spaces of the same dimension, it is a linear isomorphism.    
\end{proof}

\newpage

\section{Introduction to $(p, q)$ tensors}
\label{ch::motivated_intro::sec::motivated_intro}

Now we will derive the theorem which generalizes the two key notions (thinking of linear functions as vectors and ``multilinear elements'') discussed at the beginning of the chapter. Since we now have familiarity with the second key notion, then hopefully ``accidentally'' discovering how the first key notion is involved and formalizing it as we go is not too ambitious.

The theorem we will discover is that when $V$ and $W$ are finite-dimensional vector spaces, there is a natural isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$, where $V^*$ is the \textit{dual vector space} to $V$. We can see that the two key ideas (the first being thinking of linear functions as vectors and the second being ``multilinear elements'') are represented in this theorem with formal notation: the theorem includes a dual vector space $V^*$, which (we will see) indicates that thinking of linear functions as vectors is involved, and it also includes the tensor product $\otimes$, which indicates that multilinear structure is involved.

We first need a lemma.

\begin{lemma}
    (Standard basis matrices).

    Let $K$ be a field, and consider the standard bases $\sF = \{ \sff_1, ..., \sff_m\}$ and $\sE = \{\see_1, ..., \see_n\}$ for $K^m$ and $K^n$. The $m \times n$ matrix whose only nonzero entry is a $1$ in the $ith$ row and $j$th column is equal to $\sff_i \see_j^\top$. For example, if $m = 4$ and $n = 3$, we have

    \begin{align*}
        \sff_3 \see_2 = 
        \begin{pmatrix}
            0 \\
            0 \\
            1 \\
            0
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 & 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0
        \end{pmatrix}.
    \end{align*}
\end{lemma}

Now we begin our investigation.

\begin{deriv}
\label{ch::motivated_intro::deriv::decomposition_of_a_linear_function}
    (Decomposition of a linear function).

    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces over a field $K$, and consider the matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$.
    
    Let $a_{ij}$ be the $ij$ entry of $[\ff(E)]_F$. Making use of the lemma, we have

    \begin{align*}
        [\ff(E)]_F = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \sff_i \see_j^\top.
    \end{align*}

    Applying the linear isomorphism $\FF^{-1}$ between $m \times n$ matrices and linear functions $V \rightarrow W$ from Theorem \ref{ch::lin_alg::thm::linear_functions_matrices_isomorphism} to this equation causes  \begin{itemize}
        \item $[\ff(E)]_F$ to be replaced with $\ff$
        \item matrix multiplication to be replaced by function composition $\circ$
        \item $\see_j^\top$ to be replaced the element $\phi^{\see_j}$ of $\LLLL(V \rightarrow K)$ whose matrix relative to $E$ is $\see_j^\top$ (so $\phi^{\ee_j}(\vv) = \see_j^\top [\vv]_E$)
        \item $\sff_i$ to be replaced by the element $\tsff_i$ of $\LLLL(K \rightarrow W)$ whose standard matrix is $\sff_i$ (so $\tsff_i(c) = c\sff_i$)
    \end{itemize}

    So, we have

    \begin{align*}
        \ff = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \tsff_i \circ \phi^{\ee_i},
    \end{align*}

    where $\phi^{\ee_i} \in \LLLL(V \rightarrow K)$ and $\tsff_i \in \LLLL(K \rightarrow W)$.
\end{deriv}

\newpage

Since we have seen $\LLLL(V \rightarrow K)$ and $\LLLL(K \rightarrow W)$ have fundamental roles in the above decomposition, we make the following definitions.

\begin{defn}
\label{ch::motivated_intro::defn::dual_space_1}
    (Dual vector space).
    
    Let $V$ be a vector space over a field $K$. The \textit{dual vector space} $V^*$ to $V$ is the vector space $V^* := \LLLL(V \rightarrow K)$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::iso_V_to_linear_functions_K_V}
    (Natural identification of $W$ with $\LLLL(K \rightarrow W)$).

    Let $W$ be a vector space over a field $K$. We define a natural linear isomorphism $\sim:W \rightarrow \LLLL(K \rightarrow W)$ by $\sim(\ww) = (c \mapsto c\ww)$, and use the notation $\tww := \sim(\ww)$. That is, we have $\tww(c) = c\ww$.
\end{defn}

Using these definitions together with the result from the end of the above derivation yields the following theorem.

\begin{theorem}
\label{ch::motivated_intro::thm::decomposition_of_a_linear_function}
    (Decomposition of a linear function).
    
    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces. Any linear function $\ff:V \rightarrow W$ can be decomposed as
    
    \begin{align*}
        \ff = \sum_{\substack{i \in \{1, ..., n\} \\ j \in \{1, ..., m\}}} a_{ij} \tww_i \circ \phi_j \text{ for some $a_{ij} \in K$},
    \end{align*}
    
    where $\phi_j \in V^*$, $\ww_i \in W$, and where we recall from the previous definition that $\tww_i(c) = c\ww_i$.
\end{theorem}

\begin{proof}
    See the previous derivation.
\end{proof}

One final ``big leap'' will complete our discovery. Recall, our original goal was to show ${\LLLL(V \rightarrow W) \cong W \otimes V^*}$. We've seen that dual spaces (i.e. $V^*$) have become involved. Tensor product spaces (i.e. $\otimes$) have not, yet, but will now.

\begin{deriv}
\label{ch::motivated_intro::deriv::linear_functions_V_W_iso_W_Vstar}
    ($\LLLL(V \rightarrow W) \cong W \otimes V^*$ naturally).
    
    We know from the previous theorem that $\ff \in \LLLL(V \rightarrow W)$ decomposes as a sum of linear functions of the form $\tww \circ \phi$, where $\phi \in V^*$ and $\ww \in W$. The idea is to take advantage of this decomposition and define a linear isomorphism $\FF:\LLLL(V \rightarrow W) \rightarrow W \otimes V^*$ that sends ${(\tww \circ \phi) \in \LLLL(V \rightarrow W)}$ to the elementary tensor $\ww \otimes \phi \in W \otimes V^*$:
    
    \begin{align*}
        \underbrace{\tww \circ \phi}_{\in \LLLL(V \rightarrow W)} \overset{\FF}{\longmapsto} \underbrace{\ww \otimes \phi}_{\in W \otimes V^*}.
    \end{align*}
    
    We need to show that $\FF$ is a linear bijection. Ultimately, this will be the case because $\circ$ is a bilinear map, because $\ww \mapsto \tww$ is a linear isomorphism, and because $\otimes$ appears to be bilinear.
    
    First, we show $\FF$ is linear on \textit{elementary compositions}, which we define to be linear functions of the form $(\ww \circ \phi) \in \LLLL(V \rightarrow W)$, where $\phi \in V^*$ and $\ww \in W$. These elementary compositions are similar to elementary tensors in the sense that they are not fundamentally a linear combination of two or more other elements of the form $\ww \circ \phi$.
    
    So, we need to show that
    
    \begin{align*}
        \FF(\ff_1 + \ff_2) &= \FF(\ff_1) + \FF(\ff_2) \\
        \FF(c\ff) &= c\FF(\ff),
    \end{align*}
    
    for all elementary compositions $\ff_1, \ff_2, \ff \in \LLLL(V \rightarrow W)$ and scalars $c \in K$.
    
    More explicitly, we need $\FF$ to satisfy
    
    \begin{align*}
        \tww_1 \circ \phi + \tww_2 \circ \phi 
        &\overset{\FF}{\longmapsto}
        \ww_1 \otimes \phi + \ww_2 \otimes \phi
        \\
        \tww \circ \phi_1 + \tww \circ \phi_2 
        &\overset{\FF}{\longmapsto}
        \ww \otimes \phi_1 + \ww \otimes \phi_2
        \\
        c (\tww \circ \phi)
        &\overset{\FF}{\longmapsto}
        c (\ww \otimes \phi),
    \end{align*}
    
    for all $\ww, \ww_1, \ww_2 \in W$, $\phi, \phi_1, \phi_2 \in V^*$, and $c \in K$.
    
    The above is achieved because $\circ$ is bilinear\footnote{The fact that $\circ$ is bilinear might seem rather abstract. It may be helpful to note that a familiar consequence of $\circ$ being bilinear is the fact that matrix multiplication distributes over matrix addition. So, for example, $\AA(\BB + \CC) = \AA \BB + \AA \CC$)}, because $\ww \mapsto \tww$ is a linear isomorphism, and because $\otimes$ is bilinear:
    
    \begin{align*}
        \tww_1 \circ \phi + \tww_2 \circ \phi 
        = (\tww_1 + \tww_2) \circ \phi
        = \widetilde{(\ww_1 + \ww_2)} \circ \phi
        &\overset{\FF}{\longmapsto}
        (\ww_1 + \ww_2) \otimes \phi = \ww_1 \otimes \phi + \ww_2 \otimes \phi
        \\
        \tww \circ \phi_1 + \tww \circ \phi_2 
        = \tww \circ (\phi_1 + \phi_2)
        &\overset{\FF}{\longmapsto}
        \ww \otimes (\phi_1 + \phi_2)
        = \ww \otimes \phi_1 + \ww \otimes \phi_2
        \\
        c (\tww \circ \phi)
        = (c \tww) \circ \phi
        &\overset{\FF}{\longmapsto}
        (c \ww) \otimes \phi
        = c (\ww \otimes \phi).
    \end{align*}
    
    So, $\FF$ is linear on elementary compositions. We now \textit{impose} that it be linear on nonelementary compositions to ensure its action on any defined $\ff \in \LLLL(V \rightarrow W)$ is defined, as such an $\ff$ is a linear combination of elementary compositions. This ``shows'' that $\FF$ is linear for any $\ff \in \LLLL(V \rightarrow W)$.
    
    The bijectivity of $\FF$ now follows easily. $\FF$ is onto because any nonelementary tensor coorresponds to a ``nonelementary composition'', i.e., a linear combination of elementary compositions. $\FF$ is one-to-one because it is one-to-one when restricted to elementary compositions; the linearity of $\FF$ implies that this extends to ``nonelementary compositions''. These are the main ideas of how to prove bijectivity; the explicit check is left to the reader.
\end{deriv}

So, we have proved the following theorem.

\begin{theorem}
\label{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}
    ($\LLLL(V \rightarrow W) \cong W \otimes V^*$ naturally). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces. Then there is a natural isomorphism
    
    \begin{align*}
        \LLLL(V \rightarrow W) \cong W \otimes V^*.
    \end{align*}
    
    This isomorphism is natural because it does not depend on a choice of basis. (See Definition \ref{ch::lin_alg::defn::natural_iso}).
\end{theorem}

\begin{proof}
    See the previous derivation.
\end{proof}

\begin{remark}
    (The two key ideas). 
    
    Now that we have gone through the derivation, we can specifically see how the two key ideas- thinking of linear functions as vectors and ``multilinear elements''- have manifested.
    
    We thought of the linear function $\ff:V \rightarrow W$ as a vector when we decomposed it into a linear combination of ``elementary compositions''. The notion of dual spaces allowed us to further abstract away the component $\phi \in V^* = \LLLL(V \rightarrow K)$ in the ``elementary composition'' $\ww \circ \phi$.
    
    In order distill ``elementary compositions'' $\ww \circ \phi$ down into objects which express the key aspects of their bilinear structure, we used the seeming-bilinearity of $\otimes$ to think of these elementary compositions as ``multilinear elements'' (speficially, we thought of them as ``bilinear elements'').
\end{remark}

Here is a quick aside that isn't entirely relevant to our immediate study of tensors. When we defined inner products (Definition \ref{ch::lin_alg::defn::inner_product}), we gave motivation for why inner products are called ``inner products'', and said that we would later explain the relevance of ``outer products''. Here is that explanation. 

\begin{defn}
\label{ch::motivated_intro::rmk::outer_products}
    (Outer products).

    Let $K$ be a field. We define the \textit{outer product of $\ww \in K^m$ and $\vv \in K^n$} to be the matrix $\ww \vv^\top \in K^{m \times n}$. 
    
    Now let $V$ and $W$ be finite-dimensional vector spaces. Throughout this chapter, we've seen that matrices of the form $\ww \vv^\top$ are identified with linear functions of the form $\ww \circ \phi \in \LLLL(V \rightarrow W)$, where $\phi \in V^*$, and that those linear functions are identified with tensors of the form $\ww \otimes \phi \in W \otimes V^*$. Thus the significance of outer products is that they are fundamentally at play whenever tensors are involved.
    
    Since when $\vv \in K^n$ and $\ww \in K^m$, the matrix $\ww \vv^\top$ can be identified with the tensor $\ww \otimes \vv \in W \otimes V$, then the tensor product $\otimes$ is unfortunately sometimes referred to as ``the outer product''. Some authors even take advantage of the fact that for $\vv \in K^n$ and $\ww \in K^n$ the function $(\vv, \ww) \mapsto \ww \vv^\top$ is bilinear, and define the tensor product on $K^n$ so that it actually is the outer product: ``$\ww \otimes \vv := \ww \vv^\top$''.

    Note: there is no sort of fundamental duality between inner products (Definition \ref{ch::lin_alg::defn::inner_product}) and outer products. The two topics are relatively unrelated.

    There is one last definition whose name sounds like it is related to inner products and outer products, and that is the \textit{exterior product of vectors}. We discuss this in Remark \ref{ch::antisymmetry::rmk::exterior_product}.
\end{defn}

\newpage

\section{Dual vector spaces}

Dual vector spaces are crucial to the theorem $\LLLL(V \rightarrow W) \cong W \otimes V^*$ because they allow us to think of linear functions as vectors. We now restate the definition of a dual space and make some additional remarks.

\begin{defn}
\label{ch::motivated_intro::defn::dual_space_2}
    (Dual vector space).
    
    Let $V$ be a vector space over a field $K$. The \textit{dual vector space} $V^*$ to $V$ is the vector space $V^* := \LLLL(V \rightarrow K)$.

    Elements of $V^*$ have various names. They may be called \textit{dual vectors}, \textit{covectors}, \textit{linear functionals}, or even \textit{linear $1$-forms} (not to be mistaken with the notion of a \textit{differential} 1-form, which we have not yet defined).
\end{defn}

\begin{defn}
\label{ch::motivated_intro::defn::covariance_contravariance}
    (Covariance and contravariance).
    
    Let $V$ be a vector space. For reasons that will be explained later, in Remark \ref{ch::bilinear_forms_metric_tensors::rmk::covar_contarvar_real_meaning}, dual vectors (elements of $V^*$) are said to be \textit{covariant vectors}, or \textit{covectors}, and vectors (elements of $V$) are said to be \textit{contravariant vectors}.
    
    The coordinates of a covariant vector relative to a basis are indexed by lower subscripts; contrastingly, covaraint vectors themselves are indexed by upper subscripts. So, for example, we would write a linear combination of covariant vectors as $c_1 \phi^1 + ... + c_n \phi^n$.
    
    Contravariant vectors and their coordinates follow the opposite conventions. Coordinates of contravariant vectors are indexed by upper subscripts, and contravariant vectors themselves are indexed by lower subscripts. We would write a linear combination of contravariant vectors as $c^1 \vv_1 + ... + c^n \vv_n$.
    
    The deeper meaning behind covariance and contravariance will be explained in Remark \ref{ch::bilinear_forms_metric_tensors::rmk::covar_contarvar_real_meaning}.
\end{defn}

\subsection*{Bases for dual spaces}

\begin{deriv}
\label{ch::motivated_intro::deriv::induced_dual_basis}
    (Induced dual basis).
    
    Let $V$ be a finite-dimensional vector space and let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$. We can discover a basis for $V^*$ by decomposing the matrix corresponding to an element of $V^*$.
    
    Specifically, any element ${\phi \in V^* = \LLLL(V \rightarrow K)}$ is represented by its matrix $\phi(E)$ relative to $E$ (recall Derivation \ref{ch::lin_alg::deriv::matrix_relative_to_bases}),
    
    \begin{align*}
        \phi(E)
        =
        \begin{pmatrix} 
            \phi(\ee_1) & \hdots & \phi(\ee_n)
        \end{pmatrix}.
    \end{align*}
    
    We we express $\phi(E)$ as a linear combination of ``basis'' row matrices:
    
    \begin{align*}
        \phi(E) = \sum_{i = 1}^n \phi(\ee_i) \see_i^\top.
    \end{align*}
    
    Applying the linear isomorphism $\FF^{-1}$ between $1 \times n$ matrices and linear functions $V \rightarrow K$ from Theorem \ref{ch::lin_alg::thm::linear_functions_matrices_isomorphism} to this equation causes  \begin{itemize}
        \item $\phi(E)$ to be replaced with $\phi$
        \item matrix multiplication to be replaced by function composition $\circ$
        \item $\see_i^\top$ to be replaced the element $\phi^{\see_i}$ of $\LLLL(V \rightarrow K)$ whose matrix relative to $E$ is $\see_i^\top$; that is, $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$
    \end{itemize}

    Therefore
    
    \begin{align*}
        \phi = \sum_{i = 1}^n \phi(\ee_i) \phi^{\ee_i},
    \end{align*}

    where $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$.
    
    We see that any $\phi \in V^*$ is a linear combination of $\phi^{\ee_1}, ..., \phi^{\ee_n}$, so $\phi^{\ee_1}, ..., \phi^{\ee_n}$ span $V^*$. Since $\see_1^\top, ..., \see_n^\top$ are linearly independent, and as $\FF^{-1}$ is a linear isomorphism, then $\phi^{\ee_1}, ..., \phi^{\ee_n}$ are also linearly independent (recall Theorem \ref{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence}). Thus, the set ${E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}}$ is a basis for $V^*$.
    
    Since $E^*$ depends on $E$, we call $E^*$ the \textit{dual basis for $V^*$ induced by $E$}.
\end{deriv}

\begin{theorem}
\label{ch::motivated_intro::deriv::dim_dual_space}
    (Dimension of dual space to a finite-dimensional vector space).
    
    If $V$ is a finite-dimensional vector space, then $\dim(V^*) = \dim(V)$.
\end{theorem}

\begin{proof}
    In the previous derivation, we started with the assumption that $V$ is $n$-dimensional, and eventually saw that the dual basis induced by a choice of basis for $V$ also contains $n$ elements.
\end{proof}

\begin{theorem}
\label{ch::motivated_intro::thm::characterizations_of_induced_dual_basis}
    (Characterizations of an induced dual basis).
     
    Let $V$ be a finite-dimensional vector space. If $E = \{\ee_1, ..., \ee_n\}$ is a basis for $V$, then the dual basis $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ for $V^*$ induced by $E$ is characterized by the following equivalent conditions:
             
    \begin{enumerate}
        \item $\phi^{\ee_i}$ is the element of $V^*$ whose matrix $\phi^{\ee_i}(E)$ relative to $E$ is $\phi^{\ee_i}(E) = \see_i^\top$. That is, $\phi^{\ee_i}(\vv) = \see_i^\top [\vv]_E$.
        \item $\phi^{\ee_i}(\vv) = ([\vv]_E)^i$.
        \item $\phi^{\ee_i}(\ee_j) = \delta_{ij}$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    The first and second items are obviously equivalent. We show that the second condition is equivalent to the third condition; we show $(\phi^{\ee_i}(\vv) = ([\vv]_E)^i \iff \phi^{\ee_i}(\ee_j) = \delta_{ij})$. To prove the forward direction, substitute $\vv = \ee_j$. For the reverse direction, we have $\phi^{\ee_i}(\vv) = \sum_{j = 1}^n ([\vv]_E)^j \phi(\ee_j) = \sum_{j = 1}^n ([\vv]_E)^j \delta_{ij} = ([\vv]_E)^i$.
\end{proof}

\begin{remark}
\label{ch::motivated_intro::rmk::unnatural_iso_V_V*}
    (An ``unnatural'' isomorphism $V \cong V^*$).
    
    Suppose we've chosen a basis $E = \{\ee_1, ..., \ee_n\}$ for $V$, so that we have the induced dual basis ${E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}}$ for $V^*$. We can define a linear isomorphism $V \rightarrow V^*$ that is defined on basis vectors by $\ee_i \mapsto \phi^{\ee_i}$.
    
    This isomorphism is \textit{not} natural (see Definition \ref{ch::lin_alg::defn::natural_iso}) because it depends on the basis $E$ for $V$. Additionally, the function $\ee_i \mapsto \phi^{\ee_i}$ is onto but not one-to-one when $V$ is finite-dimensional, because when $V$ is infinite-dimensional the cardinality of $V^*$ is strictly greater than the cardinality of $V$.    
\end{remark}

\begin{remark}
    (We don't always have to choose induced bases).
    
    We don't have to pick a basis of $V$ to pick a basis for $V^*$. Derivation \ref{ch::motivated_intro::deriv::induced_dual_basis} showed that when $V$ is finite-dimensional, then $V^*$ is finite-dimensional. Therefore, when $V$ is finite-dimensional, we can pick an \textit{arbitrary} basis for $V^*$.
\end{remark}

\subsection*{The double dual}

The following theorem explains why $V^* = \LLLL(V \rightarrow K)$ is called the ``dual vector space'' to $V$.
\begin{theorem}
\label{ch::motivated_intro::thm::V_iso_double_dual}
    ($V \cong V^{**}$ naturally). 
    
    Let $V$ be a finite-dimensional vector space. The function $\vv \mapsto \Phi_\vv$, where $\Phi_\vv:V^* \rightarrow K$ is the element of $(V^*)^* = V^{**}$ defined by $\Phi_\vv(\phi) = \phi(\vv)$, is a natural linear isomorphism $V \rightarrow V^{**}$. Thus $V \cong V^{**}$ naturally.

    This theorem tells us that not only is $V^*$ the dual space to $V$, but that, since $(V^*)^* \cong V$ naturally, it is also true in some sense that $V$ is the dual space to $V^*$.
\end{theorem}

\begin{proof}
    Define $\FF:V \rightarrow V^{**}$ by $\FF(\vv) = \Phi_\vv$. We need to show that $\FF$ is linear, one-to-one, and onto. Checking linearity is straightforward; $\FF$ is linear regardless of the dimensionality of $V$. As for showing that $\FF$ is bijective, recall from Theorem \ref{ch::motivated_intro::deriv::dim_dual_space} that since $V$ is finite-dimensional we have $\dim(V) = \dim(V^{**})$. Thus, to show that $\FF$ is bijective it suffices, to show that $\FF$ has a trivial kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}).
    
    We have the following: if $\FF(\vv) = \mathbf{0}$, then $\Phi_\vv$ is the zero function, and so $\Phi_\vv(\phi) = \phi(\vv) = 0$ for all $\phi \in V^*$. One would think that this most recent statement implies $\vv = \mathbf{0}$, and this is indeed the case, because the contrapositive of $((\forall \phi \in V^* \spc \phi(\vv) = 0) \implies \vv = \mathbf{0})$, which is $(\vv \neq \mathbf{0} \implies (\exists \phi \in V^* \spc \phi(\vv) \neq 0))$, is clearly true: if $\vv \neq \mathbf{0}$, then for any basis $E$ of $V$, some component $([\vv]_E)^i \neq 0$ of $\vv$ is nonzero, and thus the element of $V^*$ defined by $\vv \mapsto ([\vv]_E)^i$ sends $\vv$ to a nonzero scalar, as desired.
\end{proof}

For more on this theory of duality, see Derivation \ref{ch::appendix::deriv::function_input_duality} in the appendix.

\newpage

\subsection*{Corresponding elements of dual spaces induced by bases}

\begin{defn}
    (Corresponding elements of dual spaces induced by bases).
    
    Let $V$ be a finite-dimensional vector space with basis $E = \{\ee_1, ..., \ee_n\}$, let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ be the dual basis for $V^*$ induced by $E$, and let $E^{**} = \{\Phi_{\phi^{\ee_1}}, ..., \Phi_{\phi^{\ee_n}}\}$ be the dual basis for $V^{**}$ induced by $E^*$. Additionally, let $\FF:V \rightarrow V^*$ be the isomorphism that sends $\ee_i \mapsto \phi^{\ee_i}$ and let $\GG:V^* \rightarrow V^{**}$ be the isomorphism that sends $\phi^{\ee_i} \mapsto \Phi_{\phi^{\ee_i}}$. We define the notation $\phi^\vv := \FF(\vv)$ and $\Phi_\phi := \GG(\phi)$.
\end{defn}

\begin{theorem}
    \label{ch::motivated_intro::thm::Phiphiv_eq_Phiv}
    ($\Phi_{\phi^\vv} = \Phi_\vv$).

    Assume the hypotheses of the previous definition. We have $\Phi_{\phi^\vv} = \Phi_\vv$, where $\Phi_\vv$ is defined by $\Phi_\vv(\phi) := \phi(\vv)$ (this is the definition for $\Phi_\vv$ given in Theorem \ref{ch::motivated_intro::thm::V_iso_double_dual}).
\end{theorem}

\begin{proof}
    To prove the theorem, we check that $\GG(\FF(\vv)) = \Phi_\vv$, where $\Phi_\vv(\phi) := \phi(\vv)$, holds for all $\vv \in V$. Since $\vv = \sum_{i = 1}^n ([\vv]_E)^i \ee_i$, we have $\FF(\vv) = \sum_{i = 1}^n ([\vv]_E)^i \phi^{\ee_i}$ and $\GG(\FF(\vv)) = \sum_{i = 1}^n ([\vv]_E)^i \Phi_{\phi^{\ee_i}}$. Thus, $\Big(\GG(\FF(\vv))\Big)(\phi) = \sum_{i = 1}^n \Big( ([\vv]_E)^i \Phi_{\phi^{\ee_i}}(\phi) \Big)$. Applying Theorem \ref{ch::motivated_intro::thm::characterizations_of_induced_dual_basis}, we have $([\vv]_E)^i = \phi^{\ee_i}(\vv)$ and $\Phi_{\phi^{\ee_i}}(\phi) = ([\phi]_{E^*})^i$, so \\ ${\sum_{i = 1}^n \Big( ([\vv]_E)^i \Phi_{\phi^{\ee_i}}(\phi) \Big) = \sum_{i = 1}^n \Big( \phi^{\ee_i}(\vv) ([\phi]_{E^*})^i \Big) = \sum_{i = 1}^n \Big( ([\phi]_{E^*})^i \phi^{\ee_i}(\vv) \Big) = \Big(\sum_{i = 1}^n ([\phi]_{E^*})^i \phi^{\ee_i}\Big)(\vv) = \phi(\vv) = \Phi_\vv(\phi)}$.
\end{proof}

\begin{theorem}
    \label{ch::bilinear_forms_metric_tensors::thm:vv_E_eq_phi_vv_Estar}
    
    ($[\vv]_E = [\phi^\vv]_{E^*}$).
    
    Assume the hypotheses of the previous definition. We have $[\vv]_E = [\phi^\vv]_{E^*}$ for any $\vv \in V$.
\end{theorem}

\begin{proof}
    The theorem is true because $\FF(\ee_i) = \phi^{\ee_i}$. A more explicit check is left to the reader.
\end{proof}

\begin{remark}
    (The misleading star notation for dual vectors).
    
    Some authors use $\vv^*$ to denote $\phi^\vv$, while also using $\{\ee_1^*, ..., \ee_n^*\}$ to denote an \textit{arbitrary} basis of $V^*$. This notation is misleading because it is suggestive of the false equation $\ee_i^* = \ee_i^*$, where the first $*$ is from the definition of $\vv^*$ and where the second $*$ is part of a basis vector $\ee_i^*$. The equation is false because it is equivalent to the following claim: ``if $\{\epsilon^1, ..., \epsilon_n\}$ is an arbitrary basis of $V^*$ then $\phi^{\ee_i} = \epsilon^i$'', which is false because it is possible to pick a basis for $V^*$ that is not the induced dual basis.
\end{remark}

\subsection*{The dual transformation}

\begin{defn}
\label{ch::motivated_intro::defn::dual_transf}
    (Dual transformation).
    
    Let $V$ and $W$ be finite-dimensional vector spaces, and let $\ff:V \rightarrow W$ be a linear function.
    The \textit{dual transformation of $\ff$}, also called the \textit{transpose of $\ff$}, is the linear function $\ff^*:W^* \rightarrow V^*$ defined by $\ff^*(\psi) = \psi \circ \ff$.
\end{defn}

\begin{theorem}
    (Dual transformation is represented by transpose matrix).

    Let $V$ and $W$ be finite-dimensional vector spaces with bases $E$ and $F$, and let $E^*$ and $F^*$ be the induced dual bases for $V^*$ and $W^*$. Consider a linear function $\ff:V \rightarrow W$. Recall from Derivation \ref{ch::lin_alg::deriv::matrix_relative_to_bases} that $[\ff(E)]_F$ denotes the matrix of $\ff:V \rightarrow W$ relative to $E$ and $F$. The matrix $[\ff^*(F^*)]_{E^*}$ of $\ff^*:W^* \rightarrow V^*$ relative to $F^*$ and $E^*$ is $[\ff^*(F^*)]_{E^*} = [\ff(E)]_F^{\top}$.
\end{theorem}

\begin{proof}
    Let $E = \{\ee_1, ..., \ee_n\}$, $F = \{\ff_1, ..., \ff_m\}$, $E^* = \{\epsilon^1, ..., \epsilon^n\}$, $F^* = \{\delta^1, ..., \delta^n\}$. We will show that the $ij$ entry of $[\ff^*(F^*)]_{E^*}$ is the $ji$ entry of $[\ff(E)]_F$.
    
    The $j$th column of $[\ff^*(F^*)]_{E^*}$ is $[\ff^*(\delta^j)]_{E^*}$. The $i$th entry of this column is $([\ff^*(\delta^j)]_{E^*})_i$. By Theorem \ref{ch::bilinear_forms_metric_tensors::thm::coords_vector_dual_vector}, we have $([\ff^*(\delta^j)]_{E^*})_i = \ff^*(\delta^j)(\ee_i)$. We have $\ff^*(\delta^j)(\ee_i) = (\delta^j \circ \ff)(\ee_i) = \delta^j(\ff(\ee_i)) = ([\ff(\ee_i)]_F)_j$, which is the $ji$ entry of $[\ff(E)]_F$, i.e., the $ij$ entry of $[\ff(E)]_F^\top$.
\end{proof}

\begin{remark}
    (Motivations for defining the dual transformation).
    
    The previous theorem reveals a new way to motivate the definition of the dual transformation. The first motivated definition, which we have already seen in Definition \ref{ch::motivated_intro::defn::dual_transf}, is ``given a linear function $\ff:V \rightarrow W$, the dual transformation is the natural linear function $W^* \rightarrow V^*$''. The second motivated definition, which is informed by the previous theorem, is ``if $\AA$ is the matrix of $\ff$ with respect to some bases, what linear transformation does $\AA^\top$ correspond to?''.
\end{remark}

\begin{theorem}
     (Dual of a composition).
     
     Let $V, W$, and $Y$ be finite-dimensional vector spaces, and let $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ be linear functions. Then $(\gg \circ \ff)^* = \ff^* \circ \gg^*$. 
     
     This fact is what underlies the fact $(\AA \BB)^\top = \BB^\top \AA^\top$, which tells how to transpose a matrix-matrix product.
\end{theorem}

\begin{proof}
    For all $\psi \in Y^*$ we have $(\gg \circ \ff)^*(\psi) = \psi \circ (\gg \circ \ff) = (\psi \circ \gg) \circ \ff = \gg^*(\psi) \circ \ff$, where $\gg^*(\psi) \in V^*$. Then for all $\psi \in Y^*$ we have $\gg^*(\psi) \circ \ff = \ff^*(\gg^*(\psi)) = (\ff^* \circ \gg^*)(\psi)$.
\end{proof}

\newpage

\section{$(p, q)$ tensors}

Before our detour into the world of dual spaces, we derived Theorem \ref{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}, and learned that when $V$ and $W$ are finite-dimensional vector spaces, there is a natural isomorphism $\LLLL(V \rightarrow W) \cong W \otimes V^*$. One implication of this theorem is of particular interest: the vector space of linear functions $V \rightarrow V$ is naturally isomorphic to $V \otimes V^*$. The characterization of linear maps $V \rightarrow V$ as elements of $V \otimes V^*$ lends itself to generalization; we can generalize the notion of linear function by ``tensor-producting in'' more copies of $V$ and $V^*$! This is the idea behind the following definition.

\begin{defn}
\label{ch::motivated_intro::defn::tensor_space_on_a_vector_space}
    (Tensor space on a vector space).
    
    Let $V$ be a vector space. We define a \textit{tensor space on $V$} to be a vector space of the form $V_1 \otimes ... \otimes V_k$, where each $V_i$ is either $V$ or $V^*$.
    
    The \textit{type} of a tensor space on $V$ is the sequence of positive integer superscripts and subscripts constructed in the pattern of the following examples\footnote{It is possible to give a formal definition of this notion of ``type'', but any such definition will be verbose and not particularly illustrative of the concept.}$^{,}$ \footnote{This convention goes against Definition \ref{ch::motivated_intro::defn::covariance_contravariance}, which sets the convention that vectors are associated with subscripts and covectors are associated with superscripts. This is necessary because we want the pattern of subscripts and superscripts in a tensor's type to mirror the pattern of subscripts and superscripts in a tensor's coordinates.}, in which     superscripts correspond to $V$ and subscripts correspond to $V^*$:
    
    \begin{itemize}
        \item The type of $V \otimes V^* \otimes V$ as a tensor space on $V$ is $^1{}_1{}^1$.
        \item The type of $V^* \otimes (V)^{\otimes 3}$ as a tensor space on $V$ is $_1{}^3$.
        \item The type of the tensor space $(V)^{\otimes 2} \otimes V^* \otimes V$ as a tensor space on $V$ is $^2{}_1{}^1$.
    \end{itemize}
    
    The \textit{type} of a tensor on $V$ is the type of the tensor space on $V$ of which the tensor is a member.
\end{defn}

\begin{defn}
\label{ch::motivated_intro::defn::pq_tensor_coords}
    (Coordinates of a tensor).

    Let $V$ be a finite-dimensional vector space over a field $K$ with basis $E = \{\ee_1, ..., \ee_n\}$, let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$, and let $T = V_1 \otimes ... \otimes V_k$ be a tensor space on $V$. The \textit{coordinates of a tensor $\TT \in T$ relative to $E$ and $E^*$} are the scalars in $K$ indexed by $i_1, ..., i_k$ such that
    
    \begin{itemize}
        \item $i_\ell$ is a superscript iff $V_\ell = V$ and $i_\ell$ is a subscript iff $V_\ell = V^*$.
        \item When we write both subscripts and superscripts at the same notational height, and denote the scalar indexed by $i_1, ..., i_k$ by $T(i_1, ..., i_k)$, we have 
        
        \begin{align*}
            \TT = \sum_{i_1, ..., i_k \in \{1, ..., n\}} T(i_1, ..., i_k) \spc \ww(i_1) \otimes ... \otimes \ww(i_k),
        \end{align*}
        
        where $\ww(i_\ell) = \ee_{i_\ell}$ iff $V_\ell = V$ and $\ww(i_\ell) = \epsilon^{i_\ell}$ iff $V_\ell = V^*$.
    \end{itemize}
    
    For example, the coordinates of a tensor $\SS \in V^* \otimes V \otimes V^*$ are the $S_{i_1}{}^{i_2}{}_{i_3} \in K$ such that
    
    \begin{align*}
        \SS = \sum_{i_1, i_2, i_3 \in \{1, ..., n\}} S_{i_1}{}^{i_2}{}_{i_3} \spc \epsilon^{i_1} \otimes \ee_{i_2} \otimes \epsilon^{i_3}.
    \end{align*}
    
    % \begin{align*}
    %     \TT = \sum_{\substack{i_1 ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \spc \ee_{i_1} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \epsilon^{j_q}.
    % \end{align*}
    
    Each coordinate of a tensor can be thought of as occupying a position in a           ``multidimensional matrix'', where each $i_k$ is associated with an orthogonal axis. (E.g., if a tensor's type is $^1{}_1$ or $_1{}^1$, then that tensor's coordinates are stored in a two-dimensional matrix).
\end{defn}

\begin{defn}
    \label{ch::motivated_intro::defn::pq_tensor}
    
    ($(p, q)$ tensor).
    
    Let $V$ be a vector space. We define the vector space $T_{p,q}(V)$ of \textit{$(p, q)$ tensors on $V$} to be the tensor space on $V$ of type $^p{}_q$. That is, the vector space $T_{p,q}(V)$ of $(p, q)$ tensors on $V$ is defined to be $T_{p,q}(V) := V^{\otimes p} \otimes (V^*)^{\otimes q}$.
\end{defn}

Recall, the below theorem is what motivated our definition of tensors in the first place.

\begin{theorem}
    ($(1, 1)$ tensors are naturally identified with linear functions).
    
    Since $T_{1,1}(V) = V \otimes V^*$ and $V \otimes V^* \cong \LLLL(V \rightarrow V)$ naturally, then $T_{1,1}(V) \cong \LLLL(V \rightarrow V)$ naturally. 
\end{theorem}

\begin{theorem}
    (Coordinates of $(p, q)$ tensors).

    Let $V$ be a vector space over a field $K$, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$. Since $(p, q)$ tensors are of type $^p{}_q$, then the coordinates of $\TT \in T_{p,q}(V)$ relative to $E$ and $E^*$ are of the form $T^{i_1 ... i_p}{}_{j_1 ... j_q} \in K$, with all of the superscripts occurring before all of the subscripts. Using coordinates to express $\TT$, we have

     \begin{align*}
        \TT = \sum_{\substack{i_1, ..., i_p \in \{1, ..., n\} \\ j_1, ..., j_q \in \{1, ..., n\}}} T^{i_1 ... i_p}{}_{j_1 ... j_q} \ee_{i_1} \otimes ... \otimes \ee_{i_k} \otimes ... \otimes \ee_{i_p} \otimes \epsilon^{j_1} \otimes ... \otimes \epsilon^{j_q}.
    \end{align*}
\end{theorem}

\begin{example}
\label{ch::motivated_intro::thm::linear_fn_cooresp_to_1_1_tensor}
    (Linear function corresponding to a $(1, 1)$ tensor).

    Let $V$ be a vector space, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and let $E^* = \{\epsilon^1, ..., \epsilon^n\}$ be a basis for $V^*$. Any $(1, 1)$ tensor on $V$, $\TT \in T_{1,1}(V)$, can be expressed as

    \begin{align*}
        \TT = \sum_{i,j \in \{1, ..., n\}} T^i{}_j \ee_i \otimes \epsilon^j.
    \end{align*}

    Recall from Derivation \ref{ch::motivated_intro::deriv::linear_functions_V_W_iso_W_Vstar} that $(1, 1)$ tensors are naturally identified with elements of $\LLLL(V \rightarrow V)$ via the linear isomorphism $\GG:T_{1,1}(V) = V \otimes V^* \rightarrow \LLLL(V \rightarrow V)$ defined on elementary tensors by $\GG(\ee_i \otimes \epsilon^j) := \widetilde{\ee}_i \circ \epsilon^j$, where $\widetilde{\ee}_i \in \LLLL(K \rightarrow V)$ is defined by $\widetilde{\ee}_i(c) = c\ee_i$. Thus, the corresponding element of $\LLLL(V \rightarrow V)$ is

    \begin{align*}
        \GG(\TT) = \sum_{i,j \in \{1, ..., n\}} T^i{}_j \widetilde{\ee}_i \circ \epsilon^j.
    \end{align*}
\end{example}

\begin{remark}
    ($\delta^{ij}$ vs. $\delta^i{}_j$ vs. $\delta^i{}_j$).
    
    One may be confused when they consider that there are three ways to write the Kronecker delta: $\delta^{ij}, \delta^i{}_j$, and $\delta_{ij}$. The difference between these functions of $i$ and $j$ is straightforward: $\delta^{ij}$ is used to denote coordinates of a $(2, 0)$ tensor, $\delta^i{}_j$ is used to denote coordinates of a $(1, 1)$ tensor, and $\delta_{ij}$ is used to denote coordinates of a $(0, 2)$ tensor. All three functions of $i$ and $j$ are defined, as usual, to be $1$ when $i = j$ and $0$ otherwise.
\end{remark}

\begin{defn}
    (Valence and order of a $(p, q)$ tensor). 
    
    The \textit{valence} of a $(p, q)$ tensor is the tuple $(p, q)$. The \textit{order} of a $(p, q)$ tensor is $p + q$.
\end{defn}

\begin{theorem}
\label{ch::motivated_intro::thm::four_fundamental_isos}

    (Four fundamental natural isomorphisms for $(p, q)$ tensors). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$. Then there exist natural isomorphisms
    
    \begin{empheq}[box = \fbox]{align*}
        \LLLL(V_1 \times ... \times V_k \rightarrow W) &\cong \LLLL(V_1 \otimes ... \otimes V_k \rightarrow W)
        \\
        \LLLL(V \rightarrow W) &\cong W \otimes V^*
        \\
        (V \otimes W)^* &\cong V^* \otimes W^*
    \end{empheq}
    
    Importantly, application of the first and third line, together with the fact that $Y \cong Y^{**}$ naturally for any vector space $Y$, yields
    
    \begin{align*}
        T_{p,q}(V) \cong (T_{p,q}(V))^{**} = (V^{\otimes p} \otimes (V^*)^{\otimes q})^{**} \cong ((V^*)^{\otimes p} \otimes V^{\otimes q})^* = \LLLL((V^*)^{\otimes p} \otimes V^{\otimes q} \rightarrow K)
        \cong
        \LLLL((V^*)^{\times p} \times V^{\times q} \rightarrow K).
    \end{align*}
    
    so we have the natural isomorphism
    
    \begin{align*}
        \boxed
        {
            T_{p,q}(V) = V^{\otimes p} \otimes (V^*)^{\otimes q}
            \cong
            \LLLL((V^*)^{\times p} \times V^{\times q} \rightarrow K)
        }
    \end{align*}
\end{theorem}

\begin{proof}
    The first line in the first box is Theorem \ref{ch::motivated_intro::thm::multilin_fns_iso_lin_fns}, and the second line in the first box is Theorem \ref{ch::motivated_intro::thm::lin_V_W_iso_W_otimes_V}. We need to prove the third line in the first box; we need to prove that \textit{taking the dual distributes over the tensor product}.
    
    We do so by defining an isomorphism $\FF$ in the ``reverse'' direction. We define this isomorphism ${\FF:V^* \otimes W^* \rightarrow (V \otimes W)^*}$ on elementary tensors and extend linearly: $\FF(\phi \otimes \psi) := f_{\phi \otimes \psi} \in (V \otimes W)^*$, where $f_{\phi \otimes \psi}:V \otimes W \rightarrow K$ is defined by $f_{\phi \otimes \psi}(\vv \otimes \ww) = \phi(\vv) \psi(\ww)$.

    It's simple to check that $\FF$ is linear. To see that $\FF$ is bijective, since it is a map between vector spaces of the same dimension, it suffices to show that it is one-to-one, i.e., that it has a trivial kernel (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}). So, assume $f_{\phi \otimes \psi} = 0$. We need to show $\phi \otimes \psi = \mathbf{0}$.
    
    We have $f_{\phi \otimes \psi}(\vv, \ww) = \phi(\vv) \psi(\ww) = \mathbf{0}$ for all $\vv \in V$ and $\ww \in W$. Note that in general, a map into a one-dimensional vector space is not the zero function iff it has a trivial kernel. Therefore, it cannot be be the case that both $\psi \neq 0$ and $\phi \neq 0$; if this were true, then both $\phi$ and $\psi$ would have trivial kernels, and $\phi(\vv) \psi(\ww)$ would be nonzero for all $(\vv, \ww) \neq (\mathbf{0}, \mathbf{0})$, which is a contradiction. So, at least one of $\phi$ and $\psi$ is the zero function. This means that we either have $\phi \otimes \psi = \mathbf{0} \otimes \psi = \mathbf{0}$ or $\phi \otimes \psi = \phi \otimes \mathbf{0} = \mathbf{0}$. We see $\phi \otimes \psi = \mathbf{0}$ in all cases, as desired.
\end{proof}

\begin{remark}
    (The four-fold nature of $(p, q)$ tensors).
    
    We have defined a $(p, q)$ tensor to be an element of a tensor product space; a $(p, q)$ tensor is a ``multilinear element''. Due to the important natural isomorphisms of the previous theorem we can think of $(p, q)$ tensors in the four ways depicted by this diagram:
    
    \begin{center}
        \begin{tikzcd}
         \substack{\text{multilinear element}
         \\ \text{(element of a tensor product space)}} \arrow[<->]{dd} &  & \text{multilinear function} \arrow[<->]{ll} \\
                                               &  &                                       \\
        \substack{\text{linear element} 
        \\ \text{(element of a vector space)}} \arrow[<->]{rr}      &  &  \text{linear function} \arrow[<->]{uu}     
        \end{tikzcd}
    \end{center}
    
    It's instructive to apply these interpretations to vectors and to dual vectors. Vectors are $1$-linear elements by definition, and they are less obviously linear functions because they are naturally identifiable with elements of $V^{**}$. Dual vectors are linear functions by definition, and they are less obviously $1$-linear elements because they form a vector space.
\end{remark}

\begin{remark}
\label{ch::motivated_intro::rmk::many_defs_tensor}
    (There are many definitions of ``tensor'').
    
    There are many ways to define the notion of a ``tensor''. Here are three common ways to define what a tensor is that differ from our definition.
    
    \begin{itemize}
        \item (A physicist's definition of a tensor). Physicists and engineers most commonly define tensors to be ``multidimensional matrices'' that follow the ``the tensor transformation law'' (which is really a change of basis formula; we will derive this in Theorem \ref{ch::bilinear_forms_metric_tensors::thm::ricci}). This definition of tensor is clearly unmotivated, as it describes how tensors behave before explaining what they really are.
        \item (The more ``concrete'' but less insightful mathematical definition of a tensor). Mathematicians often define a $(p, q)$ tensor to be a multilinear function $(V^*)^{\times p} \times V^{\times q} \rightarrow K$. This definition is equivalent to the one we have used (we see why in Theorem \ref{ch::motivated_intro::thm::four_fundamental_isos}), but it is less preferable because it obscures the concept of a ``multilinear element'' that tensor product spaces so nicely capture.
        \item (Another physicist's definition of a tensor). Physicists also occasionally define an ``order-$n$ tensor'' on $V$ to be\footnote{See p. 7 and p. 19 of Chapter 2 in \cite{BonetWood} for a treatment of tensors in this way.} a linear function that sends an order-$(n - 1)$ tensor to a vector in $V$, where an order-$1$ tensor is defined to be a vector in $V$. Thus, an order-$2$ tensor is an element of $\LLLL(V \rightarrow V) \cong V \otimes V^*$, and an order-$3$ tensor is an element of $\LLLL(\LLLL(V \rightarrow V) \rightarrow V) \cong \LLLL(V \otimes V^* \rightarrow V) \cong V \otimes V^* \otimes V^*$. We can use induction to prove that an $n$-order tensor of this definition is identified with an element of $V \otimes (V^{*})^{n - 1} = T_{1, n - 1}(V)$.
    \end{itemize}
\end{remark}