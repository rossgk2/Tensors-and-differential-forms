\chapter*{To do}

This is a list of items that must be completed before I consider this book truly complete.

\section*{GitHub readme}

More things about this book that are original that I should call attention to:

\begin{itemize}
    \item Organization of four fundamental isomorphisms for tensors
    \item Story of diff forms, once I've finished ch\_diff\_forms\_investigation.tex
    \item Using dual spaces to organize the theorem about coordinates of vectors and covectors in a chart (Theorem \ref{ch::manifolds::thm::induced_bases_in_a_chart})
\end{itemize}

\section*{Standardize theorem and proof style}

\begin{itemize}
    \item Statement of hypotheses.
    \begin{itemize}
        \item Possible styles:
        \begin{itemize}
            \item “Let $x$ have $P(x)$, $y$ have $Q(y)$, and $z$ have $R(z)$. Then … $S(x, y, z)$”
            \item “If $x$ has $P(x)$ and $y$ has $Q(y)$ then …”
            \item “$Q(x)$ for all $x$ with $P(x)$”.
            \item “Let $V$ and $W$ be vector spaces. If $\ff:V \rightarrow W$ is a linear function, then …”
            \item “If $\ff:V \rightarrow W$ is a linear function, where $V$ and $W$ are vector spaces, then …”
        \end{itemize}
        \item Definite changes to make:
        \begin{itemize}
            \item For proofs of trivial kernel, always say ``Let $\ff(\vv) = \mathbf{0}$. We need to show $\vv = \mathbf{0}$.'' Make sure the ``We need to show $\vv = \mathbf{0}$'' part is never skipped.
        \end{itemize}
    \end{itemize}
    \item Restatement of hypotheses in proofs
    \begin{itemize}
        \item shorthand: “If $f$ has <most relevant hypothesis from theorem statement>, then blah blah blah…” 
    \end{itemize}
    \item Go through the book and use fullindent command
    \item Which equations should be in the align environment and which should be inline?
    \item Which align environment equations should have ``for all $x$'' inside them, like

    \begin{align*}
        y = f(x) \text{ for all $x$},
    \end{align*}
    
    and which equations should have ``for all $x$'' after the equation, in inline text?
    \item Don't explicitize objects if their explicitization isn't need in the theorem statement. Explicitize them in the proof only as necessary. (e.g. don't state "let $E = \{\ee_1, ..., \ee_n\}$ be a basis of $V$ if it is not necessary to have explicit descriptions of the basis vectors.
    \begin{itemize}
        \item Especially, don't state "let $V$ be a vector space over a field $K$" if $K$ is not needed in the theorem statement. Instead, say "let $K$ be the field under $V$" in the proof.
        \item Add a definition of \textit{under $K$} in the definition of vector space over $K$.
    \end{itemize}
    \item Add more explanatory text in-between definitions, lemmas, theorems where needed.
\end{itemize}


\section*{Logic and proofs}

\begin{itemize}
    \item Superset notation
    \item Need a defn of $S^{\times n}$ since $V^{\times n}$ is used in the tensors section. Note that $S^n$ is alternative notation for $S^{\times n}$
    \item Explanation of how to write a proof of the form $\forall x \spc P(x) \implies (Q(x) \implies R(x))$. For example, $\forall \spc \text{linear functions $\ff$}: \spc \text{$\ff$ is one-to-one} \implies (\text{$\vv_1, ..., \vv_k$ are linearly independent} \implies \text{$\ff(\vv_1), ..., \ff(\vv_k)$ are linearly independent})$.
    \item Maybe put subsection on := vs. = somewhere else. Maybe write a subsection about definitions and theorems and put it there.
    \item Add remark that ``iff'' is shorthand for ``if and only if''.
    \item Proof of $\forall x \in S \spc P(x)$ is formally a proof by cases; we don’t write out the case $x \notin S$ because false hypothesis is obviously true
    \item Remove $-$ as notation for set difference and use $\\$ instead. Add an explanation that $-$ isn’t used because $S - T$ is often defined to mean $\{ s - t | s \in S \text{ and } t \in T\}$
\end{itemize}

\section*{Misc.}

\begin{itemize}
	\item remark that inner product called such because the $\top$ in $\vv^\top \ww$ is ``inside'' the $\vv$ and $\ww$
    \item Use $()$ for bases instead of $\{\}$. Make a remark after definition of basis about $()$ vs. $\{\}$ (or maybe this should be obvious given discussion about in logic and proofs chapter)
    \item vector fields should be denoted as $v$, not $\VV$, since a continuous map sending $\pp \mapsto v_\pp$ is a vector field
    \item in ``coordinates of tensors'' section of ``bilinear forms'' chapter, make sure never see expressions like $([\FF]_E)^i_j$, only $([\FF]_E)^i{}_j$
\end{itemize}

\section*{Dot product}

After much thought, I've determined that approach in which angle is defined first is too pedagogically confusing. Said approach quickly establishes that the geometric and algebraic formulas for the dot product are equal, but it doesn’t quickly give a sense as to why these formulas must be equal. A beginner using this approach isn’t guaranteed to be strongly motivated to ask ``why?'', since it’s clearly established that it is so.

The best approach will be to go back to the geometric $\implies$ algebraic via linearity and algebraic $\implies$ geometric due to invariance under rotation.

\subsubsection*{Overview of in-progress sections on dot product}

As I've worked on it, I've found that the best approach has a lot to do with rotation, since the notion of rotation is fundamentally derived from the notion of signed angle. Here's an overview of the dot-product-related sections of the book that are in progress:

\vspace{.25cm}

\textbf{Geometry in $\R^2$}
\begin{itemize}
	\item Length subsection: very short, defines the usual Euclidean norm. There's a great inductive derivation of the Euclidean norm later on that should be copy-pasted into this subsection (see ``Geometry in $\R^n$'' below). \textit{Pretty much complete! just needs a copy-paste.}
	\item Angle subsection: defines signed angle and unsigned angle. \textit{Has a couple proofs that need finishing and details that need checking, noted in the below ``More angle to-dos'', but the core concepts have been covered. Mostly done.} 
	\begin{itemize}
		\item End of angle section includes a ``For dot product section'' subsubsection; keep this in mind.
	\end{itemize}
	\item Rotations subsection: rotations are defined as preserving signed angle. \textit{Pretty much complete! only one very easy result needs to be proved.}
	\item Trigonometry subsection: existence of circular coordinates ($\cos$, $\sin$) derived from existence theorem about signed angles \textit{Some minor details need finishing; essentially done.}
	\item Projections subsection. \textit{About halfway done.}
	\begin{itemize}
		\item The beginning of this subsection includes an outline of how to define orthogonality and vector projections. Need to implement this outline.
		\item The end of this subsection has an unfinished proof that $\sproj(\vv \rightarrow \ww) = \sproj(\RR_t(\vv) \rightarrow \RR_t(\ww))$. Finish that!
		\item Otherwise, the subsection is complete.
	\end{itemize}
	\item Dot product subsection
	\begin{itemize}
		\item Proof of geometric $\implies$ algebraic subsubsection - \textit{seems complete; just needs commentary.}
		\item Proof of geometric $\impliedby$ algebraic subsubection - \textit{almost done, just needs simplification of the ``preserved quantities'' theorem.}
		\begin{itemize}
			\item Requires knowing that algebraic dot product is preserved by rotation. currently, a ``preserved quantities'' Theorem is at the beginning of the subsubsection. I got a little lost here, and didn't realize that it's simple: ``if length is preserved, then since algebraic dot product is a function of length, so is algebraic dot product''. since rotations preserve length they preserve algebraic dot product. so, really only a single line of the stated preserved quantities theorem is needed. State that as a lemma and find a better place for the preserved quantities theorem.
			\item Remove the remark after the preserved quantities theorem.
		\end{itemize}
	\end{itemize}
\end{itemize}

\textbf{Geometry in $\R^n$}
\begin{itemize}
	\item Move the inductive derivation of the Euclidean norm to the length subsection in ``Geometry of $\R^2$''.
\end{itemize}

\textbf{Length and angle [relocate the below to new updated sections above]}
\begin{itemize}
	\item Has content that should be reorganized... obviously...
\end{itemize}

When both the dot product and orientation sections are done, I want to make some final edits that clarify how both are about different aspects of signed angle. Dot product happens to be the easiest way to generalize unsigned angle. Orientation is just the sign of signed angle.

\subsubsection*{More angle to-dos}

Content

\begin{itemize}
    \item Add uses of the two types of angle (signed angle and unsigned angle) from intro of angle section to within angle section
\end{itemize}

Cleanup

\begin{itemize}
    \item Write $(-a) \bmod n$ instead of $-a \bmod n$ for clarity.
    \item Check that $\bmod 2\pi$ appears where it should and doesn't where it shouldn't
    \item Check that correct intervals are used for angles: $[0, 2\pi)$ for signed (not $(-\pi, \pi]$), $[0, \pi]$ for unsigned
    \item Remove all mention of unit speed curves; redid defn of trig so don't need unit speed curves for that anymore
\end{itemize}

\section*{Planes}

I'm deciding which of three characterizations of planes to use as the definition of a plane. Whichever characterization I choose, be sure to add this remark:

\begin{center}
	Planes in $\R^n$ are sometimes called \textit{hyperplanes} when $n > 3$.
\end{center}

Here are the three characterizations. (Note, the below is restricted to $\R^n$ rather than inner product spaces for simplicity. Of course after choosing an approach we'll eventually show how it applies to inner product spaces.)

\begin{enumerate}
	\item (Calculus characterization). A plane in $\R^n$ is the graph of a function $f:\R^{n - 1} \rightarrow \R$ that satisfies $\frac{\pd f(x_1, ..., x_i, ..., x_{n - 1})}{\pd x_i} = \text{const}$ for all $i$.
	\item (Implicit equation characterization). A plane in $\R^n$ is a solution set to an equation of the form $(c_1 x_1 + ... + c_n x_n + d = 0, \text{ where $x_1, ..., x_n \in \R$})$.
	\item (Special case of orthogonal complement characterization). A plane in $\R^n$ is a vector space of the form $\pp + \{\ww \in \R^n \mid \hat{\nn} \cdot \ww = 0 \}$ for some $\hat{\nn} \in \R^n$.
\end{enumerate}

If we define the concept of orthogonal complements, and prove that orthogonal complements are vector subspaces, then we get two more equivalent characterizations:

\begin{enumerate}
	\item[4.1.] (Orthogonal complement of $1$-dimensional subspace characterization). A plane in $\R^n$ is a vector space of the form $\pp + \{\hat{\nn}\}^\perp$, where $\hat{\nn} \in \R^n$.
	\item[4.2.] ($(n - 1)$-dimensional subspace characterization). A plane in $\R^n$ is a vector space of the form $\pp + W$, where $\pp \in \R^n$ and $W$ is a $(n - 1)$-dimensional subspace of $\R^n$.
\end{enumerate}

So which characterization is best to start with?

The subspace characterization (4.2.) is the most tempting, but that's only because I forget that it's specifically an $(n - 1)$-dimensional subspace that's involved, not an arbitrary one. (``Plane'' isn't just another word for ``vector space''!)

Keeping this in mind, I first thought that since the orthogonal complement characterization makes clear something that might otherwise be easily overlooked- the $n - 1$ stipulation-, it must be the best.

But then I realized, in order to know the orthogonal complement characterization easily communicates something that might otherwise be lost, one has to be aware of the existence of that thing that might be lost. And one would only know of the that thing which might be lost in the first place if they had already stumbled upon it by starting with a more natural definition and working out the consequences!

So, I ask myself again, what is the most \textit{natural} definition? Well, as I noted above, ``a plane is a set spanned by some number of vectors'', is what's most tempting and intuitive- but it's also incorrect. The calculus characterization is both intuitive and correct. \textit{So the calculus characterization is the correct first definition of a plane.}

\subsubsection*{Constructive proofs}

The most obvious proofs of (4.2) isn't constructive; it's just an invocation of the facts that an orthogonal complement is a vector space and that when $W$ is a vector subspace of a vector space $V$ we have $\dim(W^\perp) = \dim(V) - \dim(W)$.

To see how to compute the span of the subspace characterization from an implicit equation and vice-versa, constructive proofs are necessary. See the commented out proofs here:

\vspace{.25cm}

\textit{See commented out proofs in the .tex file that generated this PDF.}
\begin{comment}
	\subsection*{(3) $\implies$ (4.2)}
	
	$(\xx - \pp) \cdot \widehat{\nn} = 0 \iff \xx \cdot \widehat{\nn} + d = 0 \iff (x, y, z) \cdot (a, b, c) = 0 \iff ax + by + cz + d = 0$.
	
	$ax + by + cz + d = 0 \iff z = -(a/c)x - (b/c)y - d \iff$
	
	\begin{align*}
		\begin{pmatrix}
			x \\ y \\ z
		\end{pmatrix}
		=
		\begin{pmatrix}
			x \\ y \\ -(a/c)x - (b/c)y - d
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\ 0 \\ -a/c
		\end{pmatrix}
		x
		+
		\begin{pmatrix}
			0 \\ 1 \\ -b/c
		\end{pmatrix}
		y
		-
		\begin{pmatrix}
			0 \\ 0 \\ d
		\end{pmatrix} \\
	\end{align*}
	
	$\iff$
	
	\begin{align*}
		\begin{pmatrix}
			x \\ y \\ z
		\end{pmatrix}
		\in
		\left\{-d \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}
		+
		\spann\left(
		\left\{
		\begin{pmatrix} 1 \\ 0 \\ -a/c \end{pmatrix},
		\begin{pmatrix} 0 \\ 1 \\ -b/c \end{pmatrix}
		\right\}
		\right)
	\end{align*}
	
	\subsection*{(4.2) $\implies$ (3): example}
	
	Example: $P = \{\pp\} + \spann(\{\vv_1, \vv_2\})$, where $\pp = (1, 2, 0)^\top$, $\vv_1 = (1, 0, 1)^\top$, $\vv_2 = (0, 1, 1)^\top$.
	
	We have $(x, y, z)^\top \in P$ iff there exist $c_1, c_2$ such that $(x, y, z) = (0, 1, 1) + c_1(1, 0, 1)^\top + c_2(0, 1, 1)^\top$, iff there exist $c_1, c_2$ such that $(x, y, z) - (0, 1, 1) = c_1(1, 0, 1)^\top + c_2(0, 1, 1)^\top$, iff there exist $c_1, c_2$ such that
	
	\begin{align*}
		\begin{pmatrix}
			1 & 0 \\
			0 & 1 \\
			1 & 1
		\end{pmatrix}
		\begin{pmatrix}
			c_1 \\ c_2
		\end{pmatrix}
		=
		\begin{pmatrix}
			x-1 \\ y-2 \\ z
		\end{pmatrix}
	\end{align*}
	
	The augmented matrix is:
	
	\begin{align*}
		\left(\begin{array}{cc|c}
			1 & 0 & x-1 \\
			0 & 1 & y-2 \\
			1 & 1 & z
		\end{array}\right)
	\end{align*}
	
	And its RREF is
	
	\begin{align*}
		\text{rref}
		\left(\begin{array}{cc|c}
			1 & 0 & x-1 \\
			0 & 1 & y-2 \\
			1 & 1 & z
		\end{array}\right)
		=
		\left(\begin{array}{cc|c}
			1 & 0 & x-1 \\
			0 & 1 & y-2 \\
			0 & 0 & z - x - y + 3
		\end{array}\right)
	\end{align*}
	
	Since $(x, y, z)^\top \in P$, the corresponding system must have solutions. Therefore there cannot be any rows of all zeros, and we must have
	
	\begin{align*}
		z - x - y + 3 = 0,
	\end{align*}
	
	which is an implicit equation, as desired.
	
	\subsection*{(4.2) $\implies$ 3: general theory}
	
	Let $\pp, \vv_1, ..., \vv_k \in \R^n$, where $k < n$, and define $P := \{\pp\} + \spann(\{\vv_1, ..., \vv_k\})$. Suppose $\xx \in P$, so $\xx = \pp + c_1 \vv_1 + ... + c_k \vv_k$. Then if $\EE = (\vv_1, ..., \vv_k)$ and $\cc = (c_1, ..., c_k)^\top$ we have $\xx = \pp + \EE \cc$ and thus $\EE \cc = \xx - \pp$. This matrix-vector equation corresponds to the the augmented matrix
	
	\begin{align*}
		\begin{pmatrix}
			\EE \cc & \mid & \xx - \pp
		\end{pmatrix}.
	\end{align*}
	
	Row-reduce this matrix to obtain
	
	\begin{align*}
		\text{rref}
		\begin{pmatrix}
			\EE \cc & \mid & \xx - \pp
		\end{pmatrix}
		=
		\begin{pmatrix}
			\FF & \mid & \yy
		\end{pmatrix}
		=
		\begin{pmatrix}
			f_{11} & f_{21} & \hdots & f_{1n} & y_1 \\
			f_{21} & f_{22} & \hdots & f_{2n} & y_2 \\
			\vdots & \vdots &  & \vdots \\
			0 & 0 & \hdots & 0 & y_{r + 1} \\
			\vdots & \vdots & \hdots & \vdots & \vdots  \\
			0 & 0 & \hdots & 0 & y_m
		\end{pmatrix}.
	\end{align*}
	
	Notice that $k < n$, then there are infinitely many solutions to $\FF \xx = \mathbf{0}$, and so $\FF$ must have some rows filled with zeros at the bottom of its RREF.
	
	Since $\xx \in P$, there must be a solution to the system of equations described by the augmented matrix, and thus we cannot have a row of the form $(0, 0, ..., 0, y_s)$ for any $y_s \neq 0$; we must have $y_s = 0$ for all $s \in [r + 1, m]$.
	
	\subsection*{Conclusions}
	
	\begin{itemize}
		\item \textit{Hyperplanes} are ``affine spans'' of the above kind for which $n - k = 1$. I.e. they are sets described by exactly one implicit equation of the above kind. I.e. are those for which $n - k = 1$.
		\item $\{ \text{hyperplanes} \} = \{ \text{sets described by $\sum_i (c_i x_i) + d = 0$}\}$.
		\begin{itemize}
			\item Above shows $\subseteq$. ``Span to implicit'' shows $\supseteq$.
		\end{itemize}
		\item if $\vv_1, \vv_2 \in \R^4$ then $\spann(\{\vv_1, \vv_2\})$ is described by $4 - 2 = 2$ implicit equations, i.e. by $2$ hyperplanes
	\end{itemize}
\end{comment}

\vspace{.25cm}

These constructive proofs also provide a bridge between (1), (2), (3) and (4.1), (4.2) that isn't orthogonal complements, if orthogonal complements are to be avoided for some reason. (One possible reason: maybe introducing an entire new concept just for the sake of facilitating one proof isn't really worth it! Though, in the below ``Planes in the system of linear equations section'', we will later see there is actually one more reason to introduce orthogonal complements...)

\subsection*{Planes in the system of linear equations section}

I believe that the current situation in the systems of linear equations chapter is that the result about the number of solutions to linear equations is proved by using the fact that $        \{\text{solns}\} = \{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} \text{ for any $\vv \in \{\text{solns}\}$}$.

I would really like to use planes to prove this theorem, since this theorem is most intuitively remembered by thinking about intersections of planes. That might not be better, though, since the existing proof is much more elegant than the one with planes would be.

At the very least, make sure there's a remark on how the solution set to a system of linear equations is an intersection of planes. (If we have chosen to introduce orthogonal complements, then this is equivalent to saying \textit{any} vector space is an intersection of planes, because eEvery vector subspace $W$ is the kernel of some linear function, namely, the function $\vv \mapsto \proj(\vv \rightarrow W^\perp)$.)

\section*{Systems of linear equations}

\begin{itemize}
    \item Since $\{\text{solns}\} = \ker(\AA) + \{\vv_p\}$ then it must be true that $\rref(\AA | \bb) = (\rref(\AA) | \vv_p)$. Must $\vv_p$ be unique? Yes. (Because there exists a product $\EE$ of elementary matrices such that $\rref(\AA) = \EE \AA$; we have $\vv_p = \EE \bb$. So, might be best to start chapter with solving equations with augmented matrices rather than the theoretical result solns = $\{\text{solns}\} = \ker(\AA) + \{\vv_p\}$, since the former result secretly includes the later.
\end{itemize}

\section*{Tensors}

\begin{itemize}
    \item Current defn of tensor product space (and wedge product space) relies on fact that if $T_1 \sim T_2$ and $S_1 \sim S_2$ then $T_1 + S_1 \sim T_2 + S_2$, which is not too hard to prove. This fact is required to show that the defn is the vector space spanned by elements of the form $\vv_1 \otimes ... \otimes \vv_k$, which is the crucial intuition. Stating this fact obscures the details, so should switch defn to a more informal one, that being the crucial intuition.
    \item Then, add a remark about how one can more formally construct tensor and wedge product spaces by using $F(V \times W)/\sim$ for various $\sim$. See Warner for how to do this, and add a section on quotient vector spaces and $F(V \times W)/\sim$ to the appendix. 
    \item Check if remark on covariance and contravariance is correct in light of the fact that I’ve shown vectors transform with $[\EE]_F$ and covectors transform with its transpose inverse. Aren’t covectors supposed to be the ones that transform with the matrix that isn’t a transpose inverse? Maybe the answer is that the change of basis matrix is considered to be the one containing the vectors from the new basis taken relative to the old basis, i.e. $[\FF]_E$.
    \item Why is it necessary that covector components be indexed oppositely to covectors? Same question goes for vector components and vectors.
    \item After Ricci law: example of changing basis for a purely covariant tensor, a purely contravariant tensor, and a not too complicated mixed tensor
    \item Also, the metric tensor can have its coordinates changed!
    \item Remark: summary of physicist notation. Include the previous remark and the one about the physicist notation regarding the metric tensor.
\end{itemize}

\subsubsection*{Etc.}

\begin{itemize}
    \item choosing an inner product implies a choice of basis, because it implies a choice of orthonormal basis. revisit the remark on when unnatural isomorphism is natural
    \item Replace $V \totimes W := \LLLL(V^* \times W^* \rightarrow K)$ with $V \totimes W := V^{**} \totimes W^{**} = L(V^* \times W^* \rightarrow K)$.
\end{itemize}

\section*{Antisymmetry chapters}

\begin{itemize}
    \item \textbf{Shift framing from “construction of wedge product” to “relationship between wedge and otimes”.}
    \item Pretty sure the following is done but need to double check. 
    \begin{itemize}
        \item (Theorem). There is a natural isomorphism $\Lambda^k(V) \cong \alt(V^{\otimes k}$). That is, every wedge product can be identified with an antisymmetric tensor.
        \begin{itemize}
            \item Lemma. Every antisymmetric tensor $\TT$ is of the form $\TT = \alt(\TT)$.
    
            Proof of lemma. For one containment: we've seen that $\alt(\TT)$ is antisymmetric for all $\TT$. For the other: we've also seen that $\TT$ is alternating then $\TT = \alt(\TT)$.
    
            \item Proof of theorem. $\vv_1 \wedge ... \vv_k \mapsto \alt(\vv_1 \otimes ... \otimes \vv_k)$ defines an isomorphism $\Lambda^k(V) \rightarrow \alt(V^{\otimes k})$ on elementary wedge products. Extend it with seeming-the linearity and seeming-alternatingness of $\wedge$. 
        \end{itemize}
    \end{itemize}

    \item antisymmetric tensors only make sense for $V^{\otimes k}$, not $V_1 \otimes … \otimes V_k$
    \begin{itemize}
        \item search “alternating”
        \item search “antisym”
        \item search $V_1$ with case sensitivity to find places to fix
        \item lots of edits in “as functions” section for this
        \item find places where swap and permutation are mentioned and make them all consistent with this
    \end{itemize}
    \item Add a footnote somewhere that we can't define $\wedge$ before $\det$ because we have to know that volume should be oriented before we get the idea for ``the oriented volume parallelopiped spanned by $\vv_1, ..., \vv_k$''. And to know volume should be oriented, we first have to explore the determinant and orientation.
    \item Make sure this remark is included: $\phi^{\see_1} \wedge ... \wedge \phi^{\see_k}(\vv_1, ..., \vv_k)$ is the volume of the projection of the parallelapiped spanned by $\vv_1, ..., \vv_k$ onto $\spann(\see_{i_1}, …, \see_{i_k})$.
\end{itemize}

\subsubsection{Cross product}

\begin{itemize}
    \item Do \textit{not} introduce the cross product as the vector with a magnitude equal to the area of the spanned parallelogram and direction perpendicular and positively oriented. This is very contrived. Instead, in the linear algebra chapter, after determinants and orientation, introduce the cross product as a natural object that comes out of thinking about the oriented volume of parallelepiped: $(\vv \times \ww)^\top$ is the matrix of the linear function $\uu \mapsto \det(\uu, \vv, \ww)$. That is $(\vv \times \ww) \cdot \uu = \det(\uu, \vv, \ww)$. 
\end{itemize}

\subsubsection{Hodge dual}

\begin{itemize}
    \item Emphasize that Hodge-dual is basically a generalized cross product. This perspective implies the abstract definition of the Hodge-dual that involves orthogonal subspaces, which is often pulled out of thin air in other presentations. (So maybe prove that the Hodge-dual satisfies this condition involving orthogonal subspaces as a result of it satisfying cross-product like condition. Now that I think about it, this proof will be analogous to proving that $\vv \times \ww$ is perpendicular to $\vv$ and $\ww$ by using the defn of $\times$. This means that definition of Hodge dual will be in cross product section, so can remove definition snear vector calculus stuff).
\end{itemize}

\subsection*{Orientation}

\begin{itemize}
	\item Implement the below ``New approach''.
	\item Also think about the approach with the $n$-sphere. As detailed in the write-up there, my previous idea involving the $n$-sphere wasn't an improvement over the new approach, but there might be something to the new thoughts I wrote down.
\end{itemize}

When both the dot product and orientation sections are done, I want to make some final edits that clarify how both are about different aspects of signed angle. Dot product happens to be the easiest way to generalize unsigned angle. Orientation is just the sign of signed angle.

\subsubsection*{New approach}

Define orientation in $\R^n$ by making use of projections onto standard basis. This approach can be extended to finite-dimensional inner product spaces with a given orthonormal ordered basis.

\begin{itemize}
    \item Defn. [This definition is actually the one currently used right now.] The \textit{orientation} of an ordered basis $(\ee_1, \ee_2)$ of $\R^2$ is said to be \textit{counterclockwise}, or \textit{positive} iff $|\theta_s(\ee_1, \ee_2)| < |\tttheta_s(\ee_1, \ee_2)|$ and \textit{clockwise}, or \textit{negative}, iff $|\tttheta_s(\ee_1, \ee_2)| < |\theta_s(\ee_1, \ee_2)|$.
    \item Thm. [This theorem and the rest of the outline have not been implemented yet.] $(\ee_1, \ee_2)$ is positively oriented iff $\sproj(\ee_2 \rightarrow \see_2) > 0$, i.e., iff $\ee_2$ is above $\spann(\{\see_2\})$.
    \item Defn. $(\ee_1, ..., \ee_{n - 1}, \ee_n)$ is positively oriented iff $(\ee_1, ..., \ee_{n - 1})$ is positively oriented and $\sproj(\ee_n \rightarrow \see_n) > 0$
    \item Thm. There are only two orientations*. For an ordered basis $E$ of $\R^n$ we define $\text{orn}(E)$ to be $1$ iff $E$ is positively oriented and $-1$ if $E$ is negatively oriented.
    \begin{itemize}
        \item *This result is not suprising in this approach. It is suprising in the current approach and the other alternative approach.
    \end{itemize}
    \item Lemma 1. Orientation is preserved under linearly combining other vectors into one vector not among the others.
    \item Lemma 2. $(\ee_1, ..., c\ee_i, ..., \ee_n) \sim \sgn(c)(\ee_1, ..., \ee_i, ..., \ee_n)$.
    \item Lemma 3.1. Rotations preserve orientation.
    \item Lemma 3.2. A $2$-dimensional counterclockwise rotation by $\pi/2$ and a $2$-dimensional clockwise rotation by $\pi/2$ correspond to antisymmetry.
    \item Thm. Orientation is the sign of a multilinear function. Therefore orientation is the sign of the determinant of a multilinear function.
    \begin{proof}
        Use lemmas 1 and 2 for multilinearity. Use lemma 3 for antisymmetry. 
    \end{proof}
\end{itemize}

The key to my approach is to realize the importance of signed angle. My approach is guided by the overarching principle that the relationship between orientation and signed angle should always be this: orientation is the sign of signed angle.

\subsubsection*{Potential approach using the $n$-sphere}

In the new treatment, $n$-dimensional unsigned angle emerges from the dot product (as before), and $n$-dimensional orientation emerges from 2-dimensional orientation.

For a while I was interested in a more general treatment that defined $n$-dimensional \textit{signed} angle, and thus unified unsigned angle and orientation, since signed angle is the product of these two. The idea was, since $2$-dimensional signed angle is arclength on one of two parameterizations of a 2-sphere, maybe $n$-dimensional signed angle could be arclength on a particular parameterization of an n-sphere.

I found implementing this idea to be difficult. Yes, things are easy on a 2-sphere, since there are only two unit-speed parameterizations of the path between two points on a 2-sphere. But on an $n$-sphere, there are infinitely many such paths.

Because of this difficulty I resorted to splitting up the definition of signed angle into a definition of unsigned angle and a definition of orientation. This would make things similar to before, except at least now, hopefully each of these definitions would have something to do with $n$-spheres.

\begin{itemize}
	\item I came up with a definition of unsigned angle that does have something to do with $n$-spheres: ``unsigned angle is the minimal arclength between two points on an $n$-sphere''. (One can use the EL equations to prove that the curve of minimal length between two points on an n-sphere is a ``great circle''.)
	\item And, because I was getting lost in the complexity of all of this, I came up with a definition of the orientation of an ordered basis of two vectors that doesn't really have anything to do with $n$-spheres, and relies on the definition of orientation in $\R^2$: ``The orientation of an ordered basis of vectors $\vv, \ww \in \R^n$ is a tuple of signs, the $i$th sign being $\text{orn}\Big(\proj(\vv, \spann(\see_i, \see_{i + 1})), \proj(\ww, \spann(\see_i, \see_{i + 1}))\Big)$.''
\end{itemize}

After taking a step back, I'm wondering if there is a way that the difficulty of there being infinitely unit-speed parameterizations of the $n$-sphere.

Maybe one could define an equivalence relation on parameterizations, so that two of them are considered to be equal if their arclength functions are the same, and so that the negation of a parameterization is its reverse parameterization.

Three things have to be shown at this point: (1) orthonormalization preserves orientation, (2) rotation preserves orientation, and (3) the antisymmetry property of ordered bases' orientations. (Perhaps slight rephrasings of these facts more amenable to this presentation will be required. Though, it doesn't seem like that much rephrasing will be necessary...)

I suspect that (1) is taken care of, since one uses orthonormal coordinate systems when considering parameterizations.

Maybe (2) comes into play in showing that the signed angle between $\vv\in \R^3$ and $\ww \in \R^3$ can be computed by rotating $\spann(\vv, \ww)$ so that the image of $\vv$ aligns with $\see_1$ and the image of $\ww$ aligns with some vector in $\spann(\see_1, \see_2)$; then $2$-dimensional orientation can be invoked. (This would require defining $2$-dimensional rotations in $\R^n$, which is easy to do.) So, the usefulness of rotation preserving the equivalence relation would be that the computation of $\theta_s(\vv, \ww)$ for $\vv, \ww \in \R^3$ is equal to $\theta_s(\RR(\vv), \RR(\ww))$ for $\RR(\vv), \RR(\ww) \in \R^2$. Maybe a similar thing can be done with vectors in $\R^n$.

If this is true, then (3) follows once everything is reduced to the case of $\R^2$, since (3) is true in $\R^2$.

\subsubsection*{GitHub rant}

I need to improve the rant about orientation in tensors GitHub readme, adding bits about how defn of rotations gets negatively affected by the typical tangle. The typical approach is:

\begin{itemize}
	\item Define orientation of an ordered basis to be sign of the determinant of the ordered basis. (The typical explanation for this is: determinants should be antisymmetric since, if you investigate simple examples, the orientation of ordered bases related by antisymmetry should be the same. A deeper explanation explains that ordered bases related by antisymmetry should be the same because rotations should preserve orientation; the antisymmetry relation is obtained by using a rotation of angle pi/2. Both explanations, including the deeper one, are circular. We can't rigorously know that rotations preserve orientation since orientation hasn’t been defined yet.)
	\item Define rotations to be linear functions with determinant 1. (No rigorous argument is given for why rotations must be linear.)
	\item Prove that rotations preserve orientation.
	\item There is no distinguishment between signed and unsigned angle.
\end{itemize}

\subsubsection*{Misc. cleanup}

\begin{itemize}
	\item change all mention of inner product spaces to be inner product spaces over $\R$
	\item make sure ``finite-dimensional'' or ``$n$-dimensional'' mentioned as necessary; only specify $n$ if a dimension is needed for theorem statement
	\item move first mention of orthonormalization (Gram-Schmidt) algorithm to linear algebra chapter- probably in orthogonal complements section- since is a prerequisite for orientation section
	\item interesting, but not really necessary right now: https://analyticphysics.com/Higher\%20Dimensions/Rotations\%20in\%20Higher\%20Dimensions.htm
\end{itemize}

\section*{Rotations}

Also need to make sure that rotations are handled properly. Most of the below is implemented, but I need to make sure that the items (5) - (9) from the below are represented in ``Rotations in $n$ dimensions'', which is right after the presentation of orientation is finished.

\begin{enumerate}
    \item Define rotation in $\R^2$: a function $\R^2 \rightarrow \R^2$ is a \textit{rotation in $\R^2$} iff it preserves length, signed angle, and fixes the origin.
    \item Derive matrix of a rotation in $\R^2$.
    \item (Compositions of 2-rotations are used in orientation section).
    \item Generalize unsigned angle via the dot product; generalize the sign of signed angle, which is orientation, by finding an equivalent characterization of orientation that easily generalizes (see ``New approach'' in above ``Orientation'' subsection).
    \item Define signed angle in $\R^n$: $\theta_s(\vv, \ww) := \orn((\vv, \ww)) \theta_u(\vv, \ww)$.
    \item Define rotation in $\R^n$: a function $\R^n \rightarrow \R^n$ is a \textit{rotation in $\R^n$} iff it preserves length, singed angle, and fixes the origin.
    \item Prove that $\{\text{rotations in $\R^n$}\} = \{ \text{linear functions on $\R^n$ that preserve length and orientation} \}$
    \item Of course we have $\{\text{linear functions on $\R^n$ that preserve length and orientation}\} \\ = \{\text{linear functions on $\R^n$ with determinant $1$}\}$. So $\{\text{rotations in $\R^n$}\} \\ = \{\text{linear functions on $\R^n$ with determinant $1$}\}$.
    \item We can also prove a retroactively satisfying fact (since compositions of 2-rotations were helpful in our exploration of orientation): $\{\text{compositions of $2$-dimensional rotations in $\R^n$}\} = \{\text{rotations in $\R^n$}\}$. 
\end{enumerate}

\section*{Manifolds and differential forms}

\subsection*{Finally, a deep understanding of differential forms}

\begin{itemize}
    \item Finish end of story about differential forms; diff forms are wedge products of covector fields because the involvement of $d$ implies involvement of covectors
    \begin{itemize}
        \item \textbf{Do the ``write out this derivation'' thing in diff\_forms\_investigation after the end of the aside.}
    \end{itemize}
    \item Finish aside about tangent vectors. Figure out isomorphism between space of equivalence classes of curves and vector space of derivations. This will probably be helpful: \url{https://people.ucsc.edu/~rmont/classes/ManifoldsI/Lectures/TangentSpace.pdf}
    \item I've been using the notation $\Omega^k \FF^*$ incorrectly. You can't just apply $\Omega^k \FF^*$ to an $\ell$-blade when $\ell \neq k$. Instead, define and use $\Omega \FF^*$ for this purpose.
    
    First define $\Lambda \ff$. $\Lambda \ff$ is a map from the union of all $k$th exterior powers over $V$ to the union of all exterior powers over $W$. So $\Lambda \ff(T)$ is understood to be $\Lambda^k \ff(T)$ for the appropriate $k$. $\Omega \FF^*$ is defined similarly.
    \begin{itemize}
        \item  This might be getting into graded algebra territory.
    \end{itemize}    
    \item Warner is best diff geo book. DEFINITELY better than Lee. Gets to tangent vectors quickly, on like p. 13, and does linear algebra with abstract tensors and wedges (although doesn’t do a great job at explaining the involvement of dual spaces) before using said linear algebra on manifolds. Maybe update citations to reference Warner.
\end{itemize}

Also add this remark from Hubbard and Hubbard somewhere:

\begin{itemize}
    \item Every 1-blade is identifiable with a linear function of the form $\vv \mapsto \ww \cdot \vv$ for some vector $\ww$, which computes the work done by $\ww$ along $\vv$
    \item Every 2-blade is identifiable with an alternating multilinear function of the form $(\vv_1, \vv_2) -> \det(\ww, \vv_1, \vv_2)$ for some vector $\ww$, which computes the flux of $\ww$ through the oriented area $\vv_1 \wedge \vv_2$ (since $\det(\ww, \vv_1, \vv_2) = \ww \cdot (\vv_1 \times \vv_2) = ||\ww|| \sproj(\ww \rightarrow \vv_1 \times \vv_2)$)
    \item Every 3-blade is identifiable with an alternating linear function of the form $(\vv_1, \vv_2, \vv_3) -> \rho \spc \det(\vv_1, \vv_2, \vv_3)$ for some real number $\rho$, which computes the mass of density $\rho$ contained in the oriented volume $\vv_1 \wedge \vv_2 \wedge \vv_3$
\end{itemize}

\subsection*{Finally, a way to formalize the mnemonic}

Thinking about linear combinations of the coordinate function may help me determine what vector space is involved in the theorem about coordinates of vectors and covectors in a chart, Theorem \ref{ch::manifolds::thm::induced_bases_in_a_chart}. Could the key idea be that
$C^\infty(M \rightarrow \R^n)$ is a finite dimensional vector space over $C^\infty(M \rightarrow R)$, since for $\FF \in C^\infty(M \rightarrow \R^n)$ we have $\FF = \sum_i F^i x^i$, where $F^i \in C^\infty(M \rightarrow \R)$?


\subsection*{Read \textit{Vector and Tensor Analysis with Applications}}

Read \textit{Vector and Tensor Analysis with Applications} by A.I. Borisenko and I.E. Tarapov to learn about the simple ideas behind things like the covariant derivative.

\begin{itemize}
    \item Q. This book assumes that all changes of bases in $\R^n$ (let's say the bases are $E$ and $F$) can be specified by knowing the dot products $\ee_i \cdot \ff_j$. This leads to the question: in a finite-dimensional vector space $V$ with nondegenerate bilinear form $B$, which changes of basis can be specified by knowing $B(\ee_i, \ff_j)$ for all $i$ and $j$?
\end{itemize}

\subsection*{Etc.}

\begin{itemize}
    
    \item Figure out the unexplained step in Theorem \ref{ch::diff_forms::thm::integral_of_diff_form_actual_function_single_chart}. Somehow, $d\xx \Big( \frac{\pd}{\pd \tx^i_{(V, \yy)}} \Big) = \frac{\pd \xx}{\pd \tx^i_{(V, \yy)}}$. 
    \item Finish the section in ``Manifolds'' on frames and coframes.
    \item Figure out why the ``mnemonic'' of the ``Manifolds'' chapter works.
    \item Add the divergence theorem and the less general Stokes' theorem (and Green's theorem) after the generalized Stokes' theorem.
    \item Potentially relocate the section about differential forms in $\tOmega^k(M)$ that act on tangent vectors. Potentially the presentation of  tensors/differential forms as actual (pointwise) multilinear functions.
    \item Explain more explicitly how the cancellation in part 3 of the proof of \ref{ch::diff_forms::theorem::stokes_on_a_smooth_chart} occurs. (``all the internal boundaries in the sum $\sum_{C \in D_N(\cl(\H^k))} \int_{\pd C}$ cancel, since each boundary appears twice with opposite orientations'').
\end{itemize}