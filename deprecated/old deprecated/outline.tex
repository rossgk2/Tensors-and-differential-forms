\chapter*{Outline}

%\textbf{A motivated introduction to tensor product spaces and dual spaces}

%\begin{itemize}
%    \item Preview: two main concepts underlying the idea of a ``tensor.'' Tensor product spaces are constructed for the purpose of treating multilinear functions as linear functions. Dual spaces allow us to treat linear functions as vectors, since every linear function $V \rightarrow W$ is a sum of rank 1 linear functions, i.e., a sum of elements from the dual space.
%    \item Tensor product spaces
%    \begin{itemize}
%        \item Multilinear functions ($k$-linear functions)
%        \item Tensors as multilinear elements imply quotient space definition of tensor product space. Address associativity of $\otimes$.
%        \item Universal property of tensor product spaces
%    \end{itemize}
%    \item Motivation: discover tensor product spaces and dual spaces by discovering $\LLLL(V \rightarrow W) \cong V^* \otimes W$.
%    \begin{itemize}
%        \item Mention outer product, and link to physics/engineering chapter.
%    \end{itemize}
%    \item Finish up discussing the various ways to prove $\LLLL(V \rightarrow W) \cong V^* \otimes W$. In hindsight, there are three \textit{natural} %      isomorphisms: two ``forward'' and one ``backward.'' The one we discovered went ``forward.'' The one that's easy to find in hindsight goes ``backward.'' Lastly, asking about how coordinates of a linear function translate over yields another ``forward'' isomorphism.
%    \begin{itemize}
%        \item Mention how the outer product arises out of choosing a basis. And then mention how it shows up later in section on physics/engineering.
%    \end{itemize}
%\end{itemize}

%\textbf{Dual spaces}

%\begin{itemize}
%    \item Reminder: the importance of dual spaces in the context of tensors is that they allow us to think of linear functions as vectors. As was previously mentioned, every linear function $V \rightarrow W$ is a linear combination of elements of $V^*$.
%    \item Remark on the phrases ``dual vector,'' ''covector,'' and ``linear functional.''
%    \item Dual basis
%    \begin{itemize}
%        \item Bases aren't necessarily self-dual (orthonormal); requiring this implies a choice of basis
%    \end{itemize}
%    \item $V \cong V^{**}$ naturally
%    \item Orthonormal $\iff$ self dual
%    \item Dual transformation
%    \begin{itemize}
%        \item Let $E$ be an orthonormal basis for $V$. If the matrix of $\ff$ with respect to $E$ and $E$ is $\AA$, then the matrix of $\ff^*:V \rightarrow V$, after identifying $V \cong V^*$, with respect to $E$ and $E$ is $\AA^\top$.
%    \end{itemize}
%\end{itemize}
    
%\textbf{$\binom{p}{q}$ tensors}
%\begin{itemize}
%    \item Begin the convention of denoting contravariance and covariance with upper and lower indicies. Note that $p$ is above and $q$ is below in ``$\binom{p}{q}$ tensor.''
%    \item Valence ($\binom{p}{q}$) and order ($p + q$) of a tensor.
%    \item The three most important isomorphisms
%    \begin{itemize}
%        \item Universal property of tensor product spaces (already proved)
%        \item $\LLLL(V \rightarrow W) \cong W \otimes V^*$ (already proved)
%        \item Prove that dual distributes over tensor product: $(V \otimes W)^* \cong V^* \otimes W^*$
%    \end{itemize}
%    \item Other useful natural isomorphisms: $V \otimes F \cong V$, $V \otimes W \cong W \otimes V$, $(V \otimes W)^* \cong V^* \otimes W^*$.
%    \item Definition: $\binom{p}{q}$ tensor. Note that $\binom{p}{q}$ tensors are generalized linear maps in the sense that they are identifiable with elements of $\text{Hom}(V^{\otimes q}, V^{\otimes p}$.
%    \item Define of covariant and contravariant tensors as $\binom{0}{q}$ and $\binom{p}{0}$ tenors, respectively. Mention ``vectors'' and ``dual vectors'' (``covectors'') again.
%    \item Push-forward of vectors, $\otimes^p f:T^p_0(V) \rightarrow T^p_0(W)$, and pull-back of dual vectors, $\otimes_q f^*:T^0_q(W) \rightarrow T^0_q(V)$. Note that the pull-back is often denoted (misleadingly) as $f^*T$.
%    \item Revisit the definition of $\binom{p}{q}$ tensors.
%    \begin{itemize}
%        \item $\binom{p}{q}$ tensors are ``multilinear elements'' (because of tensor product spaces) but also ``generalized linear transformations'' (because of $\LLLL(V \rightarrow W) \cong V^* \otimes W$). 
        
%        Consider how this is true for vectors and for dual vectors. Vectors are $1$-linear elements by definition, and they are less obviously ``generalized linear transformations'' because they are naturally identifiable with elements of $V^{**}$. Dual vectors are linear functions, and they are less obviously $1$-linear elements because they form a vector space.
%    \end{itemize}
%\end{itemize}

%\textbf{Bilinear forms and metric tensors}
%\begin{itemize}
%    \item Bilinear form defn, metric tensor defn, inner product defn (is a positive-definite metric tensor)
%    \begin{itemize}
%        \item Positive definite implies nondegenerate
%    \end{itemize}
%    \item dual transf after identifications

%\begin{theorem}
%    If $V$ is a finite-dimensional vector space and $U$ is any orthonormal basis for $V$, then the matrix of $\ff^*:V \rightarrow V$ (after identifying $V \cong V^*$) with respect to $U$ and $U$ is $[\ff(E)]_E^\top$., where $[\ff(E)]_E$ is the matrix of $\ff$ with respect to $E$ and $E$.
%\end{theorem}

%\begin{proof}
%    Let $U = \{\hat{\uu}_1, ..., \hat{\uu}_n\}$. The $j$th column of $[\ff(U)]_U$ is $[\ff(\hat{\uu}_j)]_E$, so the $ij$ entry of $[\ff(U)]_U$ is $\langle \ff(\hat{\uu}_j), \hat{\uu}_i \rangle$. Similarly, the $ij$ entry of the matrix of $\ff^*$ with respect to $E$ and $E$ is $\langle 
%    \ff^*(\hat{\uu}_j), \hat{\uu}_i \rangle$. By the condition on $\langle, \rangle$ imposed by using the dual transformation after identifying $V \cong V^*$, $\langle \ff^*(\hat{\uu}_j), \hat{\uu}_i \rangle = \langle \hat{\uu}_j, \ff(\hat{\uu}_i) \rangle = \langle \ff^(\hat{\uu}_i), \hat{\uu}_j \rangle$. But this is the $ji$ entry of $[\ff(U)]_U$, i.e., the $ij$ entry of $[\ff(U)]_U^{\top}$. Thus $[\ff(U)]_U = [\ff(U)]_U^{\top}$.
%\end{proof}

%    \item Bilinear forms and natural ``musical'' isomorphisms $V \cong W^*$, $W \cong V^*$; induced bilinear form on the duals
%    \item Special case of orthonormal bases: $g_{ij} = \delta_{ij}$
%    \item Talk about bilinear form induced by choice of basis (dot product?)
%    \item $B(\vv, \ww) = [\vv]_E^\top \gg [\ww]_F$, where $g_{ij} = B(\ee_i, \ff_j)$. In this formula, $\gg$ is the matrix that represents a $\binom{1}{1}$ tensor. Despite this, we think of $\gg$ as being $\binom{0}{2}$ tensor due to the fact that it expresses the action of a bilinear form on $V$, which is an element of $\LLLL(V \times V \rightarrow F) \cong \LLLL(V \otimes V \rightarrow F) = (V \otimes V)^* \cong V^* \otimes V^* \cong T^0_2(V)$.
%    \begin{itemize}
%        \item We could technically think of $\gg$ as obtained from $B$ by tracing through the previously mentioned isomorphisms, but explicitly doing this would be difficult, since we only understand the ``forward'' version of the isomorphism $(V \otimes V)^* \rightarrow V^* \otimes V^*$ as the inverse of its ``reverse.'' Instead, we can notice that every bilinear form is determined by how it acts on the tuples of basis vectors, $(\ee_i, \ff_j)$. Therefore it is identifiable with a matrix, i.e., with a $\binom{1}{1}$ tensor, whose $^i_j$ entry is $B(\ee_i, \ff_j)$.
%        \end{itemize}
%    \item Def: a metric tensor on $V$ is a nondegenerate symmetric bilinear form on $V$. (So every inner product is a metric tensor, but not every metric tensor is an inner product. An inner product is a positive-definite metric tensor- in Appendix). Covariant metric tensor $\gg$, with $_{ij}$ entry $g_{ij}$ and contravariant metric tensor $\gg^{-1}$ with $^{ij}$ entry $g^{ij}$
%\end{itemize}

%\textbf{Coordinates of tensors}

%\begin{itemize}
%    \item Two facts about coordinates: $([\vv]_E)_i = ([\vv]_{E^{**}})_i = \epsilon_i(\vv)$ and $([\phi]_{E^*})_i = \phi(\ee_i)$.
%    \item In general (when we might not have a metric tensor), define $v^i := ([\vv]_E)_i$, $\phi_i := ([\phi]_{E^*})_i$.
%    \item When we do have a metric tensor, then additionally define $v_i := (\phi_\vv)_i$. Here $\phi_\vv$ is obtained through the musical isomorphism, which is natural. Prove $v^i = \sum_j v_j g^{ij}$ and $v_i = \sum_j v^j g_{ij}$. 
%    \item When we have a metric tensor $g$, then $V \cong V^*$ naturally, so we can convert between covariant and contravariant tensors.
%    \begin{itemize}
%        \item Staggered index notation for $\binom{p}{q}$ tensors. Raise index $i$ of $T_{ij}$: $T^i{}_j := \sum_k g^{ik} T_{kj}$. Raise index $i$ of $T_{ji}$: $T_j{}^i := \sum_k g^{ik} T_{jk}$. 
        
%        To write an arbitrary index for a $\binom{p}{q}$ tensor, we \textit{could} come up with notation for arbitrary staggering of indcies. Instead, to keep things neater, we use the convention of using $T^{i_1 ... i_p}{}_{j_1 ... j_q}$; then we can raise/lower indices in whatever order necessary by multiplying by $g^{ij}$ or $g_{ij}$ and summing as necessary.
%    \end{itemize}
%    \item Def: $[\cdot]_{E, F}$
%    \item $\delta^i_j$ vs. $\delta^{ij}$ vs. $\delta_{ij}$
%    \item Changing basis
%    \begin{itemize}
%        \item Induced change of basis on dual spaces
%        \item Ricci transformation for $\binom{p}{q}$ tensors, justification of the words ``covariant'' and ``contravariant.''
%   \end{itemize}
%    \item Tensor contraction
%    \begin{itemize}
%        \item Result of asking: ``how does composition composition of linear maps generalize?''
%        \item There is a natural bilinear form $C$ on $V$ and $V^*$ defined by $C(\vv, \phi) = \phi(\vv)$. Define $\binom{k}{\ell}$ tensor contraction on an elementary tensor $\vv^1 \otimes ... \otimes \vv^p \otimes \phi_1 \otimes ... \otimes \phi_q$ with use of $C$. This implies an equivalent definition on basis elements.
%        \item Trace as a special case. An element of $\LLLL(V, V)$ with matrix $\AA = (a_{ij})$ coorresponds to the $\binom{1}{1}$ tensor $\sum_{ij} a^j_i \epsilon_j \otimes \ee^i$, which contracts to $\sum_{ij} a^j_i \epsilon_j(\ee^i) = \sum_{ij} a^j_i \delta_{ji} = \sum_i a^i_i = \tr(\AA)$.
%        \item Discuss what happens to components; upper indices get ``contracted against'' lower indices.
%        \item Contraction is basis-independent
%        \item Induced contraction between any two $\binom{p}{q}$ tensors, given a bilinear $g$ form on $V$. (Recall that $g$ allows for conversion between covariant and contravariant).
%        \begin{itemize}
%            \item Example: double contraction (double dot product) is the induced contraction between order $2$ tensors. (The valence doesn't matter because the result is a scalar).
%        \end{itemize}
%    \end{itemize}
%\end{itemize}

%\textbf{Exterior powers}
%\begin{itemize}
%    \item Define $\alt$, and $T \wedge S := \alt(T \otimes S)$
%    \item Properties of $\wedge$. $\wedge$ is still ``multilinear.'' Address associativity of $\wedge$, alternatingness of $\wedge$. Show alternatingness iff $\vv \wedge \vv = 0$ when $\text{char}(F) > 2$.
%    \item $\Lambda^k(V) := \alt(V^{\otimes k}) = \alt(T^k_0(V))$.
%    \item Basis, dimension of $\Lambda^k(V)$.
%    \item The three most important isomorphisms, translated to exterior powers
%    \begin{itemize}
%        \item Universal property of $k$th exterior power. An alternating multilinear map uniquely $f:V^{\times k} \rightarrow W$ uniquely corresponds to a linear map $h:\Lambda^k(V) \rightarrow W$, where $f = h \circ g$. Proof similar to that for tensor product spaces.
%        \item $\alt(V^* \otimes W) \cong \alt(\LLLL(V \rightarrow W))$.
%        \item $\Lambda^k(V^*) \cong \Lambda^k(V)^*$. Proof analagous to the one for tensors. Define an isomorphim on elementary tensors by $\TT = \phi^1 \wedge ... \wedge \phi^k \mapsto f_\TT, f_\TT(\vv_1, ..., \vv_k) = (\phi^1 \owedge ... \owedge \phi^k)(\vv_1, ..., \vv_k)$, and extend using alternatingness and multilinearity.
%    \end{itemize}
%    \item The determinant
%    \begin{itemize}
%        \item Define det of a square matrix axiomatically. Mention how volume being additive implies that volume is signed, or oriented. Talk about permutation formula as showing existence and uniqueness. Mention that $\sgn(\sigma)$ is well-defined.
%        \item Laplace expansion: use ALA notes
%        \item Det of upper triangular
%        \item Define det of a linear function $V \rightarrow V$ as det of the linear function's matrix with respect to $E$ and $E$, where $E$ is a basis for $V$. Prove that, since we used same basis twice, det of a linear function is basis-independent.
%        \item Show that given a linear function $\ff:V \rightarrow V$, the induced map $\Lambda^{\dim(V)} f$ on $\Lambda^k(V)$ is multiplication by $\det(\ff)$. Prove using same process used to derive permutation formula of det for matrices, but to divide out by $\ee_1 \owedge ... \owedge \ee_k$ at end.
%        \item Properties of det
%        \begin{itemize}
%            \item $\ff$ is invertible iff $\det(\ff) \neq 0$
%            \item Product rule
%            \item $\det(\ff^*) = \det(\ff)$. Follows because of two facts. (1) if $\AA$ is matrix of $\ff$ wrt orthonormal bases $U_1$, $U_2$, then $\AA^\top$ is matrix of $\ff^*$ wrt $U_2^*$, $U_1^*$. (2) $\det(\AA^\top) = \det(\AA)$, which is true because a determinant is a sum of determinants of diagonal matrices $\DD$, which have the property $\det(\DD^\top) = \det(\DD)$.
%            \item $\det(\ff^{-1}) = \frac{1}{\det(\ff)}$. Follows because determinant is a group homomorphism (the product rule).
%            \item Mention that the following are in appendix: Laplace expansion, adjoint, Cramer's rule
%        \end{itemize}
%        \item Lemma. Suppose $\ff:V \rightarrow V$, so then $\ff^*:V^* \rightarrow V^*$. Then $\det(\ff^*)$ is the determinant of the matrix of $\ff^*$ relative to $E^*$ and $E^*$, which is
        
%        \begin{align*}
%            \begin{pmatrix} [\ff^*(\epsilon^1)]_{E^*} & \hdots & [\ff^*(\epsilon^n)]_{E^*} \end{pmatrix}
%            =
%            \begin{pmatrix} [\phi^1]_{E^*} & \hdots & [\phi^n]_{E^*} \end{pmatrix}.
%        \end{align*}
%        
%        Take a moment to talk about geometric interpretation of this.
        
%        We have $([\phi_i]_{E^*})_j = \phi^i(\ee_j)$ (see Theorem %\ref{ch::bilinear_forms_metric_tensors::thm::coords_vector_dual_vector}). Therefore
        
%        \begin{align*}
%            \det(\ff^*) = \det(\phi^i(\ee_j)),
%        \end{align*}
        
%        where $(\phi^i(\ee_j))$ is the matrix with $ij$ entry $\phi^i(\ee_j)$.
        
%        \item Lemma. $(\epsilon^1 \owedge ... \owedge \epsilon^k)(\ee_1, ..., \ee_k) = \det(\epsilon^i(\ee_j) = \det(\II) = 1$. 
        
%        \begin{proof}
%            \begin{align*}
%                \epsilon^1 \owedge ... \owedge \epsilon^k = \oalt(\epsilon^1 \ootimes ... \ootimes \epsilon^k) = \frac{1}{k!} \sum_{\sigma \in S_n} \sgn(\sigma) \epsilon^{\sigma(1)} \ootimes ... \ootimes \epsilon^{\sigma(k)}
%            \end{align*}
            
%            \begin{align*}
%                (\epsilon^{\sigma(1)} \ootimes ... \ootimes \epsilon^{\sigma(k)})(\ee_1, ..., \ee_k) &= \Big(\frac{1}{k!} \sum_{\sigma \in S_n} \sgn(\sigma) \epsilon^{\sigma(1)} \ootimes ... \ootimes \epsilon^{\sigma(k)}\Big)(\ee_1, ..., \ee_k) \\
%                &= \frac{1}{k!} \sum_{\sigma \in S_n} \sgn(\sigma) \epsilon^{\sigma(1)}(\ee_1) ... \epsilon^{\sigma(k)}(\ee_k).
%            \end{align*}
            
%            Since $\epsilon^i(\ee_j) = \delta^i_j$, the only permutation $\sigma \in S_n$ for which the sum argument is nonzero is the identity permutation. Therefore the sum is $k!$, so the final result is $\frac{k!}{k!} = 1$.
%        \end{proof}
        
%        \item Theorem. Let $\ff:V \rightarrow V$, so $\ff^*:V^* \rightarrow V^*$. Consider $\Lambda^n(V^*) = \alt(T^0_k(V))$. If $\phi^i = \ff^*(\epsilon^i)$, then $\phi^1 \owedge ... \owedge \phi^n = \det(\ff^*) \epsilon^1 \owedge ... \owedge \epsilon^n$. Apply both sides to $(\ee_1, ..., \ee_n)$ to see $(\phi^1 \owedge ... \owedge \phi^n)(\ee_1, ..., \ee_n) = \det(\phi^i(\ee_j))$. Extend with multilinearity and alternatingness to see $(\phi^1 \owedge ... \owedge \phi^n)(\vv_1, ..., \vv_n) = \det(\phi^i(\vv_j))$.
        
%        \item Above thm holds for any $k \leq n$. Argue using vector subspaces.
        
%        \item $\Lambda^k(V^*) \cong (\Lambda(V))^*$ naturally? \url{https://math.stackexchange.com/questions/18595/exterior-power-of-dual-space/18628#18628}
%        \item Push-forward $\Lambda^k f$ and pullback $\Lambda^k f^*$
%    \end{itemize}
%    \item Orientation of finite-dimensional vector spaces. 
%        \begin{itemize}
%            \item The notion of ``same orientation'' for a $2$-dimensional vector space; swap-negate and linearly combine principle for 2D.
%            \item Consider ordered permuted standard bases of $\R^n$. We say that two ordered bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$ have the same orientation iff the $2$-dimensional subspaces $\spann(\ee_i, \ee_j)$ and $\spann(\ff_i, \ff_j)$ have the same orientation for all $i, j \in \{1, ..., n\}$.
%            \item The above defn implies ordered orthonormal bases of an $n$-dimensional vector space have the same orientation iff the swap-negate and linearly combine thing holds for ordered bases of $n$ vectors.
%            \item There are two equivalence classes of ``having the same orientation.''
%            \item Each equivalence class can be identified with an element of $\Lambda^n(V^*)$, up to a scalar multiple.
%            \item The above implies that $\det$ tracks orientation: the equivalence class $[\{\ff_1, ..., \ff_n\}]_{\sim}$ can be identified with the equivalence class $[\ff_1 \wedge ... \wedge \ff_n]_{\sim}$. If $\{\ee_1, ..., \ee_n\}$ is another basis for $V$ and $\ff$ is the linear function satisfying $\ff(\ee_i) = \ff_i$ (i.e. $\ff = (\II_V)_{E,F}$) then $\ff_1 \wedge ... \wedge \ff_n$ has the same orientation as $\ee_1 \wedge ... \wedge \ee_n$ iff $\det(\ff) > 0$.
%            \begin{itemize}
%                \item More concrete way of seeing how the determinant is involved: think about the determinant of the matrix with columns; it's invariant under swap-negate and linearly combining columns into others.
%            \end{itemize}
%            \item Translate the above into a statement about $\Lambda^n(V)^* \cong \alt(\LLLL(V^{\times n} \rightarrow K))$. See Lee.
%        \end{itemize}
%        \item From here, probably switch over to using actual function interpretation of exterior powers.
%        \item Volume form $\mu$
%        \begin{itemize}
%            \item Define volume form as $n$-form $\mu$ for which $\mu(\XX_1, ..., \XX_n) = \det(\XX_1, ..., \XX_n)$, where the $\det$ on the RHS is the function on $\XXXX(M)^{\times n} \times M$ sending $(\XX_1, ..., \XX_n, \xx) \mapsto \det(\XX_1(\xx), ..., \XX_n(\xx))$. Use ``8.15. Proposition'' of Sjmaar.
%            \item Sjmaar p. 108 examples about how $\mu$ relates to $ds$, $dA$, $dV$
%            \item Use Lee Prop. 15.29 and 15.31 give formulas about volume forms
%        \end{itemize}
%        \item Hodge duality.
%        \begin{itemize}
%            \item Let $V$ be a finite-dimensional vector space and consider an ordered basis $\{\ee_1, ..., \ee_n\}$ for $V$.
            
%            Let $\sigma_W \in S_k$ and $\sigma_{W^\perp} \in S_{n - k}$. Set $W$ to be the oriented subspace of $V$ spanned by the ordered basis $\{\ee_{\sigma(1)}, ..., \ee_{\sigma(k)}\}$, and give $W^\perp \subseteq V$ the orientation specified by $\{\ee_{\sigma(k + 1)}, ..., \ee_{\sigma(n)}\}$.
            
%            Equivalently, the orientations chosen on $W$ and $W^\perp$ can be represented as elements of $\Lambda^k(W) \subseteq \Lambda^k(V)$ and $\Lambda^{n - k}(W^\perp) \subseteq \Lambda^{n - k}(V)$, respectively. The orientation on $W$ is specified by $\ee_{\sigma(1)} \wedge ... \wedge \ee_{\sigma(k)}$ and the orientation of $W^\perp$ is specified by $\ee_{\sigma(k + 1)} \wedge ... \wedge \ee_{\sigma(n)}$.
            
%            The \textit{Hodge dual}, or \textit{Hodge star}, is the map $*:\Lambda^k(W) \rightarrow \Lambda^{n - k}(W^\perp)$ which acts on the orientation chosen for a subspace $W \subseteq V$ to the orientation chosen for $W^\perp$.
            
%            It is enough to say that $*$ ``acts'' on $\ee_{\sigma(1)} \wedge ... \wedge \ee_{\sigma(k)}$ as the action on this element of $\Lambda^k(W)$ is implied: $*$ is a map of one-dimensional vector spaces, so we know
            
%            \begin{align*}
%                *(\ee_{\sigma(1)} \wedge ... \wedge \ee_{\sigma(k)}) = 
%            \end{align*}
            
%        \end{itemize}
        
        
%\end{itemize}
    
%\textbf{Differential forms}
%\begin{itemize}
%    \item Don't have to use $\owedge$ (interpretation of diff form at a point as actual alteranting multilinear function), can use $\wedge$ (interpretation of diff form at a point as alternating tensor)!
 %   \item A differential form at $\xx \in M$ is an element of $\Omega^k(T_\xx(M)^*)$. Set of differential forms on $M$ denoted $\Omega^k(M)$.
  %  \item Theorem on differential forms at a point $\xx$ using basis
    
  %  \item (This bullet point uses actual fn interp of diff forms).
    
    %We have $d\ff:\Omega^k(M) \rightarrow \Omega_k(N)$, $d\ff_{\xx}:T_\xx(M) \rightarrow T_\xx(N)$, and $(d\ff_{\xx})^*:T_{\ff(\xx)}(N)^* \rightarrow T_{\xx}(M)^*$. 
    
    %The pull-back $\overline{\Omega^k} \ff^*:\Omega^k(N) \rightarrow \Omega_k(N)$ of a differential $k$-form $\omega$ on $N$ is defined by $\Big( (\overline{\Omega^k} \ff^*)(\omega) \Big)(\xx) = \Big( \Lambda^k (d\ff_\xx)^* \Big)(\omega(\ff(\xx))$. Using the argument as was made for the pull-back of an element of $\Lambda^k(W^*)$ (see [...]), we have $\Big( \Lambda^k (d\ff_\xx)^* \Big)(\omega(\ff(\xx))) = \omega(\ff(\xx)) \circ d\ff_\xx = (d\ff_\xx)^*(\omega(\ff(\xx)))$. More explicitly, $\Big((\overline{\Omega^k} \ff^*)(\omega) \Big)(\xx) (\vv_1, ..., \vv_k) = \omega(\ff(\xx))(d\ff_\xx(\vv_1), ..., d\ff_\xx(\vv_k))$.
    
    %Also by looking back at how def of $\Lambda^k \ff^*$ translates over to $\overline{\Lambda^k} \ff^*$, we see that if $\omega(\xx) = \alpha(\xx) \epsilon^1 \owedge ... \owedge \epsilon^k$, then
    
    %\begin{align*}
    %    \Big((\overline{\Omega^k} \ff^*)(\omega) \Big)(\xx) = \det((d\ff_\xx)^*) \alpha(\ff(\xx)) \spc (d\ff_\xx)^*(\epsilon^1) \owedge ... \owedge (d\ff_\xx)^*(\epsilon^k).
    %\end{align*}
    
    %Set $\delta^i = (d\ff_\xx)^*(\epsilon^i)$, i.e., $\delta^i = \epsilon^i \circ d\ff_\xx$ to restate this as 
    
    %\begin{align*}
    %    \Big((\overline{\Omega^k} \ff^*)(\omega) \Big)(\xx) = \det((d\ff_\xx)^*) \alpha(\ff(\xx)) \spc \delta^1 \owedge ... \owedge %\delta^k.
    %\end{align*}
    
    %Therefore, using that $\det((d\ff_\xx)^*) = \det(d\ff)$ and suppressing dependence on $\xx$, we have
    
    %\begin{align*}
    %    (\overline{\Omega^k} \ff^*)(\omega) = \det(d\ff) (\alpha \circ \ff) \spc \delta^1 \owedge ... \owedge \delta^k.
    %\end{align*}
    
    %Here $d\ff$ denotes the function which sends $\xx \mapsto d\ff_\xx$.
    
    %\begin{itemize}
    %    \item Proof that $\det(d\ff_\xx)$ is involved in the change of variables theorem. 
        
    %    By Lemma [...] we have $\det((d\ff_\xx)^*) = \det(\delta^i(\ee_j))$. Then $\delta^i(\ee_j) = (\epsilon^i \circ d\ff_\xx)(\ee_j) = \epsilon^i(d\ff_\xx(\ee_j))$. But $d\ff_\xx(\ee_j)$ is the directional derivative of $\ff$ in the $\ee_j$ direction, $d\ff_\xx(\ee_j) = \frac{\pd \ff(x_1, ..., x_j, ..., x_k)}{\pd x_j}$. We also have $\epsilon^i(\vv) = ([\vv]_E)^i$, so $\delta^i(\ee_j) = ([\frac{\pd \ff(x_1, ..., x_j, ..., x_k)}{\pd x_j}]_E)^i$. If $\ff(\xx) = \begin{pmatrix} f_1(\xx) \\ \vdots \\ f_k(\xx) \end{pmatrix}$, then by definition of partial derivative of a vector-valued function, we have $([\frac{\pd \ff(x_1, ..., x_j, ..., x_k)}{\pd x_j}]_E)^i = \frac{\pd f_i(\xx)}{\pd x_j}$. Therefore $\det((d\ff)^*) = \det(\frac{\pd f_i(\xx)}{\pd x_j})$.
    %    \item $\Omega^k \ff^*$ and $\overline{\Omega^k \ff^*}$ are nonstandard notation; people denote $\overline{\Omega^k} \ff^*$ by $\ff^*$, even though this can be confused with the dual transformation.
    %\end{itemize}
    %\item Notation: $dx_i := \epsilon^i$ is a basis for $\Lambda^k (T_\xx(M))^*$; $df_i, y_i := \delta^i$ is a basis for $\Lambda^k (T_{\ff(\xx)}(N))^*$
    %\item Change of variables theorem. Consider open subsets $U, V$ of $\R^k$, a diffeomorphism $\ff:V \rightarrow U$, and an integrable function $\alpha:U \rightarrow \R$ is integrable on $U$. Then
    
    %\begin{align*}
    %    \int_U \alpha = \int_V (\alpha \circ \ff) |\det(d \ff)|.
    %\end{align*}
    
    %So, we define, for $\omega = \alpha \spc dx_1 \owedge ... \owedge dx_k$,
    
    %\begin{align*}
    %    \int_U \alpha \spc dx_1 \owedge ... \owedge dx_k := \int_U \alpha.
    %\end{align*}
    
    %Using $dx_1 ... dx_n$ as a ``placeholder'' on the right side (but not on the left), as is often done in an integral, this %definition is
    
    %\begin{align*}
    %    \int_U \alpha \spc dx_1 \owedge ... \owedge dx_n := \int_U \alpha dx_1 ... dx_n.
    %\end{align*}
    
    %The idea behind the definition is that $dx_1 ... dx_n$ are ``secretly'' $dx_1 \owedge ... \owedge dx_n$. While the definition technically defines the LHS in terms of the RHS, you might think of it as ``secretly'' giving a new meaning to the old placeholder notation of the RHS.
    
    %Then when $\ff:V \rightarrow U$ is orientation preserving, $\det(d\ff) > 0$, so $|\det(d\ff)| = \det(d\ff)$, and the change of variables theorem is restated as
    
    %\begin{align*}
    %    \int_U \omega = \int_V \Big(\Omega^k \ff^*\Big)(\omega).
    %\end{align*}f
%\end{itemize}
    
\textbf{Manifolds}

Given a smooth chart $(U, \xx)$ on $M$,
\begin{itemize}
    \item The vector field defined by $\pp \mapsto \frac{\pd}{\pd x^i}\Big|_\pp$ is denoted (in an abuse of notation) by $\frac{\pd}{\pd x^i}$ and is called the \textit{$i$th coordinate vector field}. \smallcite{book::SM}{176}
    \item The smooth local frame $\{\frac{\pd}{\pd x^1}, ..., \frac{\pd}{\pd x^n} \}$, where $\frac{\pd}{\pd x^i}$ is the $i$th coordinate vector field, is called a \textit{coordinate frame} \smallcite{book::SM}{178}
    \item $\{\lambda_1|_\pp, ..., \lambda_n|_\pp\}$ denotes the dual basis for $T_\pp^*(M)$ induced by the basis $\Big\{\frac{\pd}{\pd x^1}\Big|_\pp, ..., \frac{\pd}{\pd x^1}\Big|_\pp \Big\}$ for $T_\pp(M)$ \smallcite{book::SM}{275}
    \item The covector field defined by $\pp \mapsto \lambda_i|_\pp$ is denoted (in an abuse of notation) by $\lambda_i$ and is called the \textit{$i$th coordinate covector field}
    \item The components of a covector field $\phi$ at $\pp$ relative to $\{\lambda_1, ..., \lambda_n\}$ are the covector fields $([\phi]_{\{\lambda_1, ..., \lambda_n\}})_i$ defined by $([\phi]_{\{\lambda_1, ..., \lambda_n\}})_i|_\pp = \phi_\pp\Big(\frac{\pd}{\pd x^i}\Big|_\pp\Big)$ \smallcite{book::SM}{277}
    \item The smooth (see \smallcite{book::SM}{278}) coframe $\{\lambda_1, ..., \lambda_n\}$ is called the \textit{coordinate coframe}. \smallcite{book::SM}{278}
    \item The differential of a smooth function $f:M \rightarrow \R$ is a smooth covector field. \smallcite{book::SM}{281}
    \item We have $\lambda_i = d(x^i)$, where $x^i$ is a coord fn and $d$ is its differential in sense of Theorem [...]
    \begin{empheq}[box = \fbox]{align*}
        df_\pp &= \sum_{i = 1}^n \frac{\pd f}{\pd x^i}\Big|_\pp dx^i|_\pp \\
        df &= \sum_{i = 1}^n \frac{\pd f}{\pd x^i} dx^i
    \end{empheq}
\end{itemize}


\textbf{Tangent vs. cotangent}

\begin{itemize}
    \item Tangent space at $\pp$
    \item Cotangent space at $\pp$, $T_\pp^*(M) := T_\pp(M)^*$. An element of $T_\pp(M)^*$ is called a \textit{covector (at $\pp$)}.
    \item Vector fields, frames $\{\EE_1, ..., \EE_n\}$
    \item Covector fields, coframes $\{\EEE^1, ..., \EEE^n\}$
    \item Tangent bundle: $T(M) := \bigsqcup_{\pp \in M} T_\pp(M)$. Note that a disjoint union is \textit{not} a ``pairwise disjoint union''.
    \item Cotangent bundle: $T^*(M) := \bigsqcup_{\pp \in M} T_\pp^*(M)$.
    \item Tangent bundle and cotangent bundle are smooth vector bundles.
    \item Tensor bundle: $T^p_q(T(M)) := \bigsqcup_{\pp \in M} T^p_q(T_\pp(M))$.
    \begin{itemize}
        \item Have $T^p_0(T(M)) = \bigsqcup_{\pp \in M} (T_\pp(M))^{\otimes p}$ and $T^0_q(T(M)) = \bigsqcup_{\pp \in M} (T_\pp^*(M))^{\otimes p}$. So in particular, $T^1_0(T(M)) = T(M)$, $T^0_1(T(M)) = T^*(M)$.
        \item What about $T^0_0(M)$?
        \item A map $A \subseteq T^p_q(T(M)) \rightarrow M$ is a \textit{tensor field}. A \textit{smooth tensor field} is a smooth such map. A \textit{(smooth) vector field} is a (smooth) tensor field with $q = 0$, and a \textit{(smooth) covector field} is a (smooth) tensor field with $p = 0$.
        \item The subset of $T^0_k(T(M))$ consisting of alternating tensors is denoted $\Lambda^k(T(M))$, and is $\Lambda^k(T(M)) = \bigsqcup_{\pp \in M} \Lambda^k(T_\pp^*(M))$. Is $\binom{n}{k}$ dimensional, and has basis [...]. A \textit{differential $k$-form on $M$} is a smooth map $\Lambda^k(T(M)) \rightarrow M$. The set of differential $k$-forms on $M$ is denoted $\Omega^k(M)$.
    \end{itemize}
    \item Takeaways from tangent vectors section: no way to directly pull back geometric vectors onto a manifold, but can pull back directional derivatives, so we use derivations to formalize tangent vectors, $\frac{\pd}{\pd x^i}$ is a basis for $T_\pp(M)$, $x^i$ is a coordinate function of a smooth chart
    \item coords of tangent vector, coords of cotangent vector in analogy to coords of vector and coords of dual vector
    \item use word tangent vector more
\end{itemize}


\textbf{References}
\begin{itemize}
    \item Books
    \begin{itemize}
        \item Introduction to Smooth Manifolds by John Lee
        \item Solution manual to Lee: \url{https://wj32.org/wp/wp-content/uploads/2012/12/Introduction-to-Smooth-Manifolds.pdf}
        \item Chapter 4 of Differential Topology by Victor Guillemin and Alan Pollack
        \item Chapter 7 of Mathematical Methods of Classical Mechanics by Vladimir Arnold
        \item Mathematics for Physics by Michael Stone and Paul Goldbart
        \begin{itemize}
            \item Look at Ch 11!
        \end{itemize}
        \item Vector Calculus, Linear Algebra, and Differential Forms by John Hamal Hubbard and Barbara Burke Hubbard
    \end{itemize}
    \item Slanted indices (\url{https://math.stackexchange.com/questions/73171/index-notation-for-tensors-is-the-spacing-important})
    \item Exterior derivative
    \begin{itemize}
        \item \url{https://mathoverflow.net/questions/10574/how-do-i-make-the-conceptual-transition-from-multivariable-calculus-to-different}
        \item \url{https://mathoverflow.net/questions/21024/what-is-the-exterior-derivative-intuitively}
        \item \url{https://math.stackexchange.com/questions/1270673/geometric-intuition-about-the-exterior-derivative}
        \item \url{https://math.stackexchange.com/questions/209241/exterior-derivative-vs-covariant-derivative-vs-lie-derivative}
        \item \url{https://math.stackexchange.com/questions/1908008/why-isnt-there-a-contravariant-derivative-or-why-are-all-derivatives-covarian}
    \end{itemize}
    \item Exterior powers (\url{https://kconrad.math.uconn.edu/blurbs/linmultialg/extmod.pdf})
\end{itemize}

\textbf{References not used}
\begin{itemize}
    \item Linear Algebra via Exterior Products (\url{https://www.google.com/books/edition/Linear_Algebra_Via_Exterior_Products/G1lpaPlErAIC?hl=en&gbpv=1&printsec=frontcover})
    \item Good explanation of changing bases for $(p, q)$ tensors (Ricci's transformation law) begins at bottom of p. 549 \url{https://cseweb.ucsd.edu/~gill/CILASite/Resources/15Chap11.pdf}
\end{itemize}