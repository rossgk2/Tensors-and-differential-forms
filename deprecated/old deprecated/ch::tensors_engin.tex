\chapter{Tensors in physics and engineering}
\label{ch::tensors_engin}

Choice of basis

\subsubsection{Cauchy stress tensor}

\subsubsection{Outer product}


\begin{theorem}
    If we have identified $V \cong V^*$, then every linear transformation $V \rightarrow W$ has a matrix of the form $\sum_k \ww_k \vv_k^\top = \WW^\top \VV$.
\end{theorem}

\begin{proof}
    By the discussion on rank 1 tensors above, every element of $\text{Hom}(V, W)$ has a matrix of the form $\sum_k [\ww_k]_F [\vv_k]_E^\top$, where $[\vv_k]_F = (v_{k1}, ..., v_{kn})^\top, [\ww_k]_E = (w_{k1}, ..., w_{kn})^\top$. The matrix $\sum_k [\ww_k]_F [\vv_k]_E^\top$ has $ij$ entry $\sum_k w_{ki} v_{kj}$. Let $\WW$ be the matrix whose $i$th column is $[\ww_i]_F$ and $\VV$ be the matrix whose $i$th column is $[\vv_i]_E$. Then the $ij$ entry of $\sum_k [\ww_k]_F [\vv_k]_E^\top$ is $\sum_k w_{ki} v_{kj} = \WW_{:i} \cdot \VV_{:j} = \WW_{i:}^\top \cdot \VV_{:j} = ij \text{ entry of $\WW^\top \VV$}$. Therefore $\sum_k [\vv_k]_E [\ww_k]_F^\top = \WW^\top \VV$.
\end{proof}

\subsubsection{Levi-Citiva symbol}

\subsection*{Bilinear forms, metric tensors, and coordinates of tensors}

\begin{defn}
    (The physicist's index notation, the physicist's slanted index notation).

    This definition contains a series of definitions used by physicists. The content of this definition will not be used anywhere else in this book; we present it for completeness only.
    
    Let $V$ be an $n$-dimensional vector space, let $E$ be a basis for $V$, and let $E^*$ be the induced dual basis for $V^*$. We define $v^i := ([\vv]_E)^i$ and $\phi_i := ([\phi]_{E^*})_i$.

    Now let $V$ and $W$ be finite-dimensional vector spaces with bases $E$ and $F$, and let $E^* = \{\phi^{\ee_1}, ..., \phi^{\ee_n}\}$ and $F^* = \{\psi^{\ff_1}, ..., \psi^{\ff_m}\}$ be corresponding induced dual bases of $V^*$ and $W^*$. Suppose that we also have a metric tensor $g$ on $V$ and $W$, so $V \cong W^*$ naturally via the musical isomorphism $\flat_1:V \rightarrow W^*$ that is induced by $g$. In this situation, we define $v_i := (\vv^{\flat_1})_i$.
    
    Now that these definitions are out of the way, we consider the situation most often seen in physics: suppose $V = W$, and fix a basis $E$ for $V$ (fixing a basis allows us to use the notation $v_i$). 
    
    Theorem \ref{ch::bilinear_forms_metric_tensors::coords_vectors_dual_vectors} is then restated as

    \begin{align*}
        v^i &= \sum_{j = 1}^n g^{ij} v_j \\
        v_i &= \sum_{j = 1}^n g_{ij} v^j.
    \end{align*}
    
    Due to the above two equations, one converts between the coordinates of a vector and the coordinates of its corresponding dual vector by ``multiplying by the metric''. Though, a physicist would really say that ``one converts between upper indices and lower indices by multiplying by the metric''. (Physicists also use Einstein summation notation, so this operation looks more like straightforward multiplication because, in Einstein summation notation, the sum is ``implied'', and not written).
    
    As was shown out in the previous theorem, we can use the above ``multiplication by the metric'' (i.e. we can use one of the previous two equations) to convert the coordinates of a $\binom{p}{q}$ tensor to the coordinates of a $\binom{p - 1}{q + 1}$ or $\binom{p + 1}{q - 1}$ tensor. Since we have already set up the index notation $v^i$ and $v_i$, and shown how to ``raise and lower'' indices, we define a similar notation that involves the raising and lowering of individual indices for coordinates of $\binom{p}{q}$ tensors. For simplicity, we will introduce this index notation for tensors for the case of $\binom{0}{2}$ tensors. The notation easily generalizes to $\binom{p}{q}$ tensors for any $p, q$.
    
    Consider the $\binom{0}{2}$ tensor with coordinates $(T_{ij})$. We define
    
    \begin{align*}
        T^i{}_j &:= \sum_{k = 1}^n g^{ik} T_{kj} \\
        T_j{}^i &:= \sum_{k = 1}^n g^{ik} T_{jk}
    \end{align*}
    
    Recall from the equation $v^i = \sum_{j = 1}^n g^{ij} v_j$ that the index which gets summed over is the index that gets raised. Also notice that if one index appears to the left of another index before either of the indices is raised, then that same index must appear to the left of the other index after either of the indices is raised. So, for instance, in the first equation, $i$ appears before $j$ before $i$ is raised because the coordinates of the tensor are $(T_{ij})$. Correspondingly, $i$ appears before $j$ after $i$ is raised in the coordinates $(T^i{}_j)$.
        
    The above definition implies that we can lower indices as follows:
    
    \begin{align*}
        T_{ij} &= \sum_{k = 1}^n g_{jk} T^k{}_j \\
        T_{ji} &= \sum_{k = 1}^n g_{jk} T_j{}^k.
    \end{align*}
    
    Similarly as to before, the equation $v_i = \sum_{j = 1}^n g_{ij} v^j$ dictates that the index which gets summed over is the index that gets lowered.
    
    We have not explicitly presented how this notation works for $\binom{2}{0}$ tensors or for arbitrary $\binom{p}{q}$ tensors. It would be awkward to fully write out a general description of this notation, due to the need for a notation for an arbitrary arrangement of upper and lower indices. The idea generalizes straightforwardly, though, so this is no issue. 
    
    Suppose $\TT$ is a $\binom{p}{q}$ tensor with an almost-arbitrary arrangement of upper and lower indices (think of a $\binom{p}{q}$ tensor with a ``random'' arrangement of upper and lower indices, such as $(T^{i_1 i_2}{}_{j_1}{}^{i_3}{}_{j_2}{}^{i_4 i_5}{}_{j_3})$); we only require that there be at least two lower indices and at least two upper indices. A lower index $j_k$ is raised by taking the coordinates of $\TT$, replacing $j_k$ with the dummy index $r$, and then computing $\sum_{r = 1}^n g^{u_\ell r} \cdot (\text{the coordinates of $\TT$})$, where $u_\ell$ is some other lower index. An upper index $i_k$ is raised by taking the coordinates of $\TT$, replacing $i_k$ with the dummy index $r$, and then computing $\sum_{r = 1}^n g_{v_\ell r} \cdot (\text{the coordinates of $\TT$})$, where $v_\ell$ is some other upper index.
\end{defn}
    

\subsubsection{Double contraction}

The next definition and theorem addresses the \textit{double contraction} : of $T^2_2(V)$ tensors. The double contraction is often used in physics contexts. We present the double contraction only for completeness; we do not use it in this book.

\begin{defn}
    (Double contraction).
    
    Consider $T^2_2(V) \otimes T^2_2(V)$, and note that $T^2_2(V) \otimes T^2_2(V) \cong T^4_4(V)$ naturally. The \textit{double contraction} $:$ is the map $T^2_2(V) \otimes T^2_2(V) \overset{:}{\rightarrow} T^4_0(V)$ induced by the composition of a $\binom{3}{1}$ contraction and a $\binom{4}{2}$ contraction on $T^4_4(V)$. The double contraction is also called the \textit{double dot product}.
\end{defn}

\begin{example}
    (Hooke's law).
\end{example}