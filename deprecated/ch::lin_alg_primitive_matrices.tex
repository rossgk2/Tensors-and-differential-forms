\subsection*{Coordinatization of vectors}

[now, we restate a previous defn]

\begin{defn}
\label{ch::lin_alg::defn::coordinates_relative_to_basis}

    (Coordinates of a vector relative to a basis).
    
    Let $V$ be a finite-dimensional vector space over a field $K$, and let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$. Given a vector $\vv \in V$, we define $[\vv]_E$ to be the vector in $K^{\dim(V)}$ that stores the \textit{coordinates of $\vv$ relative to the basis $E$}. Formally, $[\vv]_E$ is the tuple of scalars 
    
    \begin{align*}
        [\vv]_E := \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix} \in K^n
    \end{align*}
    
    for which
    
    \begin{align*}
        \vv = ([\vv]_E)_1 \ee_1 + ... + ([\vv]_E)_n \ee_n.
    \end{align*}
    
    We are guaranteed that such scalars exist because $E$, being a basis for $V$, spans $V$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::linear_fn_acts_on_vectors}

    (Linear function acting on a list of vectors).
    
     Let $V$ be a finite-dimensional vector space over a field $K$, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$, and consider a linear function $\ff:V \rightarrow K^m$. We define the notation
    
    \begin{align*}
        \ff(E) := \begin{pmatrix} \ff(\ee_1) & \hdots & \ff(\ee_n) \end{pmatrix}.
    \end{align*}
    
    Note that when $\ff$ is invertible and $E$ is a basis of $V$, then the columns of the matrix $\ff(E)$ are a basis of $K^m$, since invertible linear functions preserve linear independence (see Theorem \ref{ch::lin_alg::lemma::only_inv_fns_preserve_lin_indep}).
    
    We also define $\EE := \II_V(E)$, where $\II_V$ is the identity on $V$, since
    
    \begin{align*}
        \II_V(E) = \begin{pmatrix} \ee_1 & \hdots & \ee_n \end{pmatrix}.
    \end{align*}

\end{defn}

\begin{theorem}
    Taking coordinates relative to a basis is an invertible linear operation. Put differently, $[\cdot]_E$ is an invertible linear function.
\end{theorem}

\begin{proof}
    For linearity, we show that $[\vv_1 + \vv_2]_E = [\vv_1]_E + [\vv_2]_E$ and that $[c\vv]_E = c[\vv]_E$.
        
    \begin{align*}
        [\vv_1 + \vv_2]_E
        &= \Big[\Big(\sum_{i = 1}^n ([\vv_1]_E)_i \ee_i + \sum_{i = 1}^n ([\vv_2]_E)_i \ee_i\Big)\Big]_E \\
        &= \Big[\Big(\sum_{i = 1}^n \Big[ \Big(([c_1 \vv_1]_E)_i + ([\vv_2]_E)_i \Big) \ee_i \Big] \Big)\Big]_E
        = 
        \begin{pmatrix} ([\vv_1]_E)_1 + ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_1]_E)_m + ([\vv_2]_E)_m \end{pmatrix}
        =
        \begin{pmatrix} ([\vv_1]_E)_1 \\ \vdots \\ ([\vv_1]_E)_m \end{pmatrix}
        +
        \begin{pmatrix} ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_2]_E)_m \end{pmatrix} \\
        &= [\vv_1]_E + [\vv_2]_E.
    \end{align*}
        
    Now we show $[c \vv]_E = c [\vv]_E$. If $[\vv]_E = \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix}$, then $\vv = \sum_{i = 1}^n ([\vv]_E)_i \ee_i$, so $c \vv = c \sum_{i = 1}^n ([\vv]_E)_i \ee_i = \sum_{i = 1}^n c ([\vv]_E)_i \ee_i$. Thus by definition of $[\cdot]_E$ we see $[\cdot]_E(c \vv) = c [\vv]_E$. Therefore $[\cdot]_E$ is linear.
        
    $[\cdot]_E$ is invertible because it has a trivial kernel (see \ref{ch::lin_alg::thm::linear_fn_1-1_trivial_kernel}): if $[\cdot]_E(\vv) = \mathbf{0}$, then the coordinates of $\vv$ relative to $E$ are all zero, so $\vv = \mathbf{0}$.
\end{proof}

\newpage

\subsection*{Coordinatization of linear functions with matrices}

\textbf{make sure we're using ``primitive''}

\begin{deriv}
\label{ch::lin_alg::deriv::primitive_matrix}
    (Standard matrix of a linear function $K^n \rightarrow K^m$ and matrix-vector product). 
    
    The fundamental idea behind this derivation is that the action of a linear function on any vector is determined by how the function acts on each vector in a basis (recall Definition \ref{ch::lin_alg::defn::linear_function}).
    
    To start, let $K$ be a field, let $E$ be a basis for $K^n$, and consider a linear function $\ff:K^n \rightarrow K^m$. From the definition of $[\cdot]_E$ (see Definition \ref{ch::lin_alg::defn::coordinates_relative_to_basis}) we have $\cc = \sum_{i = 1}^n ([\cc]_E)_i \ee_i$, and, using the linearity of $\ff$, we have
    
    \begin{align*}
        \ff(\cc) = \ff \Big( ([\cc]_E)_1 \ee_1 + ... + ([\cc]_E)_n \ee_n \Big) = ([\cc]_E)_1 \ff(\ee_1) + ... + ([\cc]_E)_n \ff(\ee_n).
    \end{align*}
    
    Here, we see (as we have before) that the action of $\ff$ on a vector $\cc$ is determined by what $\ff(\ee_1), ..., \ff(\ee_n)$ are. Formally, we have discovered a function $\pp$ that takes as input the ordered list $\begin{pmatrix} \ff(\ee_1) & \hdots & \ff(\ee_n) \end{pmatrix}$, the vector $[\cc]_E \in K^n$, and produces $\ff(\cc) \in K^m$ as output:
    
    \begin{align*}
        \pp\Left( \begin{pmatrix} \ff(\ee_1) & \hdots & \ff(\ee_n) \end{pmatrix}, [\cc]_E \Right) := ([\cc]_E)_1 \ff(\ee_1) + ... + ([\cc]_E)_n \ff(\ee_n) = \ff(\cc).
    \end{align*}
    
    Now, we turn our attention to the ordered list of column vectors that is an input to $\pp$:
    
    \begin{align*}
        \begin{pmatrix} 
            \ff(\ee_1) & \hdots & \ff(\ee_n)
        \end{pmatrix}.
    \end{align*}
    
    This ordered list of $n$ many column vectors from $K^m$ can be interpreted to be a grid of scalars with $m$ rows and $n$ columns. In general, an $m$ by $n$ grid of scalars is called a \textit{$m \times n$ matrix}. ($m \times n$ is read as ``$m$ by $n$''). The entry in the $i$th row and $j$th column of a matrix is referred to as the \textit{$ij$ entry} of that matrix. 
    
    Of course, the above matrix isn't just ``any old matrix'': if we know what the basis $E$ is, this matrix tells us what $\ff$ \textit{is}! For this reason, the above matrix is referred to as the \textit{primitive\footnote{We have used the qualifier ``primitive'' because a linear function between finite-dimensional vector spaces only has a primitive matrix if the vector spaces are $K^n$ and $K^m$ for some $n$ and $m$. Though, as we will see, it is still possible to obtain \textit{some} kind of matrix from functions between arbitrary finite-dimensional vector spaces.} matrix of $\ff:K^n \rightarrow K^m$ relative to the basis $E$}.
    
    Recalling the notion of a linear function acting on a list of vectors from Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors}, we see that $\begin{pmatrix} \ff(\ee_1) & \hdots & \ff(\ee_n) \end{pmatrix} = \ff(E)$. That is, the primitive matrix of $\ff$ relative to the basis $E$ is $\ff(E)$. Knowing this allows us to restate the above definition of $\pp$ more compactly:
    
    \begin{align*}
        \pp(\ff(E), [\cc]_E) := ([\cc]_E)_1 \ff(\ee_1) + ... + ([\cc]_E)_n \ff(\ee_n) = \ff(\cc).
    \end{align*}
    
    Since the columns of $\ff(E)$ can vary over $K^m$, and since each of $([\cc]_E)_1, ..., ([\cc]_E)_n$ can vary\footnote{The $i$th column of $\ff(E)$ is $\ff(\ee_i)$. Each $\ff(\ee_i)$ can vary over $K^m$ because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}. The $([\cc]_E)_i$ can vary over $K$ because $[\cdot]_E:K^n \rightarrow K^n$ is an invertible linear function from a vector space to itself, and is therefore onto (recall Theorem \ref{ch::lin_alg::thm::linear_fn_1-1_iff_onto}).} over $K$, we can now restate the action of $\pp$ in terms of an arbitrary $m \times n$ matrix $\AA$ having $i$th column $\aa_i$ and an arbitrary column vector $\cc \in K^n$:
    
    \begin{align*}
        \pp\Bigg(
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA,
            \underbrace{
            \begin{pmatrix} 
                c_1 \\ \vdots \\ c_n 
            \end{pmatrix}}_\cc
            \Bigg)
            =
            c_1 \aa_1 + ... + c_n \aa_n.
    \end{align*}
    
    So, $\pp$ is really a function that accepts an $m \times n$ matrix and $n$-dimensional column vector as input and that produces an $m$-dimensional column vector as output. For this reason, we will call $\pp$ the \textit{matrix-vector product}.
    
    No one actually uses the letter $\pp$ when notating matrix-vector products. Instead, we simply establish the convention that writing a column vector to the right of a matrix indicates the evaluation of the matrix-vector product. That is, given an $m \times n$ matrix $\AA$ having $i$th column $\aa_i$ and a column vector $\cc \in K^m$, we define
    
    \begin{align*}
        \boxed
        {
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA
            \underbrace{
            \begin{pmatrix} 
                c_1 \\ \vdots \\ c_n 
            \end{pmatrix}}_\cc
            :=
            c_1 \aa_1 + ... + c_n \aa_n
        }
    \end{align*}
    
    Expanding out the columns of $\AA$, here is what the above definition looks like when written out more explicitly:
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix} 
            a_{11} & \hdots & a_{1n} \\
            \vdots & \hdots & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}}_\AA
        \underbrace{
        \begin{pmatrix} 
            c_1 \\ \vdots \\ c_n 
        \end{pmatrix}}_\cc
        :=
        c_1 
        \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}
        + ... + c_n 
        \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}.
    \end{align*}
\end{deriv}

We now restate and expand upon an important result from the previous derivation.

\begin{theorem}
    (Characterizing property of primitive matrices).
    \label{ch::lin_alg::thm::characterizing_property_of_primitive_matrix}
    
    Let $K$ be a field, let $E$ be a basis for $K^n$, and let $\ff:K^n \rightarrow K^m$ be a linear function. The primitive matrix $\ff(E)$ of $\ff:K^n \rightarrow K^m$ relative to $E$ is the unique matrix satisfying the following characterizing property:
    
    \begin{align*}
        \boxed{
            \ff(E) [\cc]_E = \ff(\cc) \text{ for all $\cc \in K^n$}
        }
    \end{align*}
    
    The right hand side of the above equation is a matrix-vector product.
\end{theorem}

\begin{proof}
   The above derivation shows that $\ff(E)$ satisfies the characterizing property. (It showed that $\pp(\ff(E), [\cc]_E) = \ff(\cc)$ for all $\cc \in K^n$). It remains for us to show uniqueness; we need to show that if $\AA$ is a matrix satisfying $\AA [\cc]_E = \ff(\cc)$ for all $\cc \in K^n$, then $\AA = \ff(E)$.
   
   So, suppose $\AA [\cc]_E = \ff(\cc)$ for all $\cc \in K^n$. Then $\AA [\cc]_E = \ff(E) [\cc]_E$ for all $\cc \in K^n$. The previous derivation shows that there is a linear function $\gg$ with $\gg(E) = \AA$. Thus, ``$\AA [\cc]_E = \ff(E) [\cc]_E$ for all $\cc$'' becomes ``${\text{$\gg(E) [\cc]_E = \ff(E) [\cc]_E$}}$''. Since $\gg(\cc) = \gg(E) [\cc]_E$ and $\ff(\cc) = \ff(E) [\cc]_E$, this means $\ff(\cc) = \gg(\cc)$ for all $\cc$, i.e. $\ff = \gg$. Since $\ff = \gg$, we have $\ff(E) = \gg(E) = \AA$, so $\AA = \ff(E)$, as desired.
\end{proof}

\begin{theorem}
    (Linear functions from matrices).
    
    If $\AA$ is an $m \times n$ matrix with entries in a field $K$, then the function $\cc \mapsto \AA \cc$ is linear, and $\AA$ is the matrix of this function relative to the standard basis for $K^n$.
\end{theorem}

\begin{proof}
    Define $\gg$ by $\gg(\cc) = \AA \cc$. We will show that $\gg$ is linear and that $\AA = \gg(\sE)$.

    Let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $K^n$. We can interpret the $i$th column $\aa_i$ of $\AA$ to be $\ff(\ee_i)$, where $\ff$ is some linear function. Thus $\AA = \ff(E)$. Since $\ff(E) [\cc]_E = \ff(\cc)$ and $\AA = \ff(E)$, we have $\AA [\cc]_E = \ff(\cc)$. Set $E = \sE$ to obtain that $\AA \cc = \ff(\cc)$. Thus, $\gg(\cc) = \AA \cc = \ff(\cc)$ for all $\cc \in K^n$, meaning $\gg = \ff$, which makes $\gg$ linear.
    
    The primitive matrix $\gg(\sE)$ of $\gg$ has $i$th column $\gg(\see_i) = \AA \see_i$. Compute the matrix-vector product $\AA \see_i$ to verify that $\AA \see_i = \aa_i$. This tells us that $\gg(\see_i) = \aa_i$, which means $\gg(\sE) = \AA$.
    
    %Since we have $\AA \cc = \ff(\cc)$ and $\ff(\sE) \cc = \ff(\cc)$, the uniqueness of primitive matrices implies that $\AA = \ff(\sE)$. \textbf{Now we have to show that $\ff(\sE) = \gg(\sE)$, where $\gg(\cc) = \AA \cc$}.
\end{proof}

\begin{theorem}
    (Properties of the matrix-vector product).
    
    Practically speaking, the linearity of the function $\cc \mapsto \AA \cc$ translates into properties of the matrix-vector product: we have $\AA(\cc + \dd) = \AA \cc + \AA \dd$ and $\AA(k\cc) = k(\AA \cc)$ for any matrix $\AA$, column vectors $\cc$ and $\dd$, and scalars $k$. 
\end{theorem}

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_vector_product}
    ($i$th entry of matrix-vector product).
    
    Let $\AA$ be an $m \times n$ matrix having $ij$ entry $a_{ij}$ in a field $K$, and let $\cc = \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} \in K^n$ be a column vector. Referring to the definition of matrix-vector product in Derivation \ref{ch::lin_alg::deriv::primitive_matrix}, we see the matrix-vector product $\AA \cc$ is equal to the following:
    
    \begin{align*}
            \AA \cc = 
            \begin{pmatrix}
                a_{11} & \hdots & a_{1n} \\
                \vdots & & \vdots \\
                a_{i1} & \hdots & a_{in} \\
                \vdots & & \vdots \\
                a_{m1} & \hdots & a_{mn}
            \end{pmatrix}
            \begin{pmatrix} c_1 \\ \vdots \\ \vdots \\ \vdots \\ c_n \end{pmatrix}
            =
            c_1
            \begin{pmatrix} a_{11} \\ \vdots \\ a_{i1} \\ \vdots \\ a_{m1} \end{pmatrix}
            +
            ...
            +
            c_n
            \begin{pmatrix} a_{1n} \\ \vdots \\ a_{in} \\ \vdots \\ a_{mn} \end{pmatrix}
            =
            \begin{pmatrix} c_1 a_{11} + ... + c_n a_{1n} \\ \vdots \\ c_1 a_{i1} + ... + c_n a_{in} \\ \vdots \\ c_1 a_{m1} + ... + c_n a_{mn} \end{pmatrix}.
    \end{align*}

    Therefore, the $i$th entry $(\AA \cc)_i$ of $\AA \cc$ is $c_i a_{i1} + ... + c_n a_{in} $, which is
    
    \begin{align*}
        \boxed
        {
            (\AA \cc)_i = (\text{$i$th row of $\AA$}) \cdot \cc
        }
    \end{align*}
    
    Here $\cdot:K^n \times K^n \rightarrow K$ denotes the \textit{dot product} of vectors in $K^n$, defined by
    
    \begin{align*}
        \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix}
        \cdot
        \begin{pmatrix} d_1 \\ \vdots \\ d_n \end{pmatrix}
        =
        c_1 d_1 + ... + c_n d_n.
    \end{align*}
    
    Since the dot product must take two column vectors as input, what we technically mean by ``$i$th row of $\AA$'' in the boxed equation is ``column vector that contains entries of $i$th row of $\AA$''.
    
    The last section of this chapter discusses the dot product in depth.
\end{theorem}

Now that we have discovered that grids of scalars- matrices- are relevant to linear algebra, we state some definitions and one theorem before continuing our investigations.

\begin{defn}
    (Set of $m \times n$ matrices with entries in $K$).
    
    When $K$ is a field, we define $K^{m \times n}$ to be the set of $m \times n$ matrices with entries in $K$.
\end{defn}

\begin{defn}
    (Matrices specified by their $ij$ entry).
    
    Recall that the entry in the $i$th row and $j$th column of a matrix is called the \textit{$ij$ entry} of that matrix. Specifying matrices by describing their $ij$th entry is relatively common. We write ``$\AA = (a_{ij})$'' iff $\AA$ is the matrix with $ij$ entry $a_{ij}$.
\end{defn}

\begin{defn}
    (Identity matrix).
    
    Let $K$ be a field, and consider the identity function on $K^n$, which is the function $\II_{K_n}:K^n \rightarrow K^n$ defined by $\II_{K^n}(\cc) = \cc$. The primitive matrix of $\II_{K^n}$ relative to the standard basis $\sE$ is called the \textit{($n \times n$) identity matrix}. Notice that since the $i$th column of the identity matrix is $\II_{K_n}(\see_i) = \see_i$, it follows that the identity matrix has a diagonal of $1$'s, with $0$'s everywhere else; its $ij$ entry is $1$ if $i = j$ and $0$ if $i \neq j$. 
    
    The $3 \times 3$ identity matrix, is, for example,
    
    \begin{align*}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{pmatrix}.
    \end{align*}
    
    We denote the identity matrix by $\II$ rather than by the more verbose notation $\II_{K^n}(\sE)$, and infer $K^n$ and $\sE$ from context.
\end{defn}

\begin{defn}
    (Invertibility of matrices).
    
    Consider $\AA \in K^{m \times n}$, and let $\ff:K^n \rightarrow K^m$ be the linear function defined by $\ff(\cc) = \AA \cc$. (So, $\ff(\sE) = \AA$). Since only same-dimension vector spaces can be linearly isomorphic, we must have $n = m$ if $\ff$ is invertible. That is, $\AA$ must be an $n \times n$ matrix- a \textit{square matrix}- for some $n$ if $\ff$ is invertible.
    
    If $\AA$ is a square matrix and $\ff$ is indeed invertible, we say that $\AA$ is \textit{invertible}, and use $\AA^{-1}$ to denote the primitive matrix of $\ff^{-1}$ relative to $\sE$.
    
    It follows quickly from the above definition a square matrix $\AA$ is invertible iff there is a unique matrix $\AA^{-1}$ for which ${\AA^{-1} \AA = \II = \AA \AA^{-1}}$.
\end{defn}

[segway to compositions of linear functions]

\begin{theorem}
    (A composition of linear functions is also linear).
    
    Let $V, W$, and $Y$ be vector spaces over the same field. If $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ are linear functions, then the composition $\gg \circ \ff$ is also a linear function.
\end{theorem}

\begin{proof}
   Left as an exercise.
\end{proof}

[segway]

\textbf{notation should reflect fact that standard basis for $K^n$ is either superset or subset of standard basis for $K^m$}

\begin{defn}
\label{ch::lin_alg::thm::matrix_matrix_product_relative_to_bases_primitive}
    (Composition of linear functions that map into $K^m$ and $K^p$). 
    
    Let $K$ be a field, and consider linear functions $\ff:K^n \rightarrow K^m$ and $\gg:K^m \rightarrow K^p$. Additionally, let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $K^n$, $\sE = \{\see_1, ..., \see_m\}$ be the standard basis for $K^m$, and $\sF = \{\sff_1, ..., \sff_p\}$ be the standard basis for $K^p$.
    
    Since $\gg \circ \ff$ is a linear function $K^n \rightarrow K^p$, it has a primitive matrix. The primitive matrix $(\gg \circ \ff)(E)$ of $\gg \circ \ff$ relative to $E$ (see Derivation \ref{ch::lin_alg::deriv::primitive_matrix}) is
    
    \begin{align*}
        (\gg \circ \ff)(E)
        =
        \begin{pmatrix}
            (\gg \circ \ff)(\ee_1) & \hdots & (\gg \circ \ff)(\ee_n)
        \end{pmatrix}
        =
        \begin{pmatrix}
            \gg(\ff(\ee_1)) & \hdots & \gg(\ff(\ee_n))
        \end{pmatrix}.
    \end{align*}
    
    As we've recently learned, one way to express the action of the linear function $\gg$ on an input vector is to use $\gg$'s primitive matrix relative to some basis. The question is- what basis? Well, in the above, $\gg$ acts on $\ff(\ee_i)$, which is a column vector in $K^m$. Recall that every element $\cc$ of $K^m$ satisfies $[\cc]_\sE = \cc$, so, the above is really
    
    \begin{align*}
        \begin{pmatrix}
            \gg([\ff(\ee_1)]_\sE) & \hdots & \gg([\ff(\ee_n)]_\sE)
        \end{pmatrix}.
    \end{align*}
    
    Now, we can make use of $\gg$'s primitive matrix $\gg(\sE)$ relative to the basis $\sE$. We have
    
    \begin{align*}
        \begin{pmatrix}
            \gg([\ff(\ee_1)]_\sE) & \hdots & \gg([\ff(\ee_n)]_\sE)
        \end{pmatrix}
        =
        \begin{pmatrix}
            \gg(\sE) [\ff(\ee_1)]_\sE & \hdots & \gg(\sE) [\ff(\ee_n)]_\sE
        \end{pmatrix}
        =
        \begin{pmatrix}
            \gg(\sE) \ff(\ee_1) & \hdots & \gg(\sE) \ff(\ee_n)
        \end{pmatrix}.
    \end{align*}
    
    Recalling that $\ff(\ee_i)$ is the $i$th column $(\ff(E))_i$ of $\ff(E)$, the primitive matrix of $\ff$ relative to $E$, we finally have
    
    \begin{align*}
        \begin{pmatrix}
            \gg(\sE) \ff(\ee_1) & \hdots & \gg(\sE) \ff(\ee_n)
        \end{pmatrix}
        =
        \begin{pmatrix} 
            \gg(\sE) (\ff(E))_1 & \hdots & \gg(\sE) (\ff(E))_n
        \end{pmatrix}.
    \end{align*}
    
    We have shown that the primitive matrix of $\gg \circ \ff$ relative to $E$ is
    
    \begin{align*}
        (\gg \circ \ff)(E) = 
        \begin{pmatrix} 
            \gg(\sE) (\ff(E))_1 & \hdots & \gg(\sE) (\ff(E))_n
        \end{pmatrix}
    \end{align*}
    
    In light of this fact, we define the \textit{matrix-matrix product} $\BB \AA$ of an $m \times n$ matrix $\AA$ with a $p \times m$ matrix $\BB$ as follows:
    
    \begin{align*}
        \boxed
        {
            \BB \AA := \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}
        }
    \end{align*}
    
    With the above definition, the primitive matrix $(\gg \circ \ff)(E)$ of $\gg \circ \ff$ relative to $E$ is expressed as the matrix-matrix product $\gg(\sE) \spc \ff(E)$:
    
    \begin{align*}
        (\gg \circ \ff)(E) = \gg(\sE) \spc \ff(E).
    \end{align*}
\end{defn}

\begin{remark}
    (Compatibility of matrices for matrix-matrix products). 
    
    The composition $\gg \circ \ff$ of linear functions $\ff$ and $\gg$ is only defined when the output space of $\ff$ is the entire input space of $\gg$, i.e., when the dimension of $\ff$'s output is the same as the dimension of $\gg$'s input. Because of this, the matrix-matrix product $\BB \AA$ of an $m \times n$ matrix with an $r \times s$ matrix $\BB$ is only defined when $r = n$, i.e., when $\BB$ has as columns as $\AA$ has rows.
\end{remark}

We now state some facts about matrix-matrix products.

\begin{remark}
    (Matrix-matrix products are associative).
    
    One would expect that $(\BB \AA) \cc = \BB (\AA \cc)$ when $\AA$ and $\BB$ are ``compatible'' matrices and when $\cc$ is a column vector. This is indeed true, because the corresponding linear functions $\ff$ and $\gg$ satisfy $(\gg \circ \ff)(\cc) = \gg(\ff(\cc))$.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_matrix_product}

    ($ij$ entry of matrix-matrix product). 
    
    Let $K$ be a field, let $\AA = (a_{ij}) \in K^{m \times n}$, and let $\BB = (b_{ij}) \in K^{m \times p}$. The $ij$ entry of the matrix-matrix product $\BB \AA$ can be computed by using the definition of matrix-matrix product (Theorem \ref{ch::lin_alg::thm::matrix_matrix_product_relative_to_bases}) together with the fact that the $i$th entry of the matrix vector product $\AA \cc$ is $(\text{$i$th row of $\AA$}) \cdot \cc$, where $\cdot$ is the dot product. We have
    
    \begin{align*}
        \BB \AA
        = 
        \BB
        \begin{pmatrix}
            \aa_1 & \hdots & \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \BB \aa_1 & \hdots & \BB \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \bb_1 \cdot \aa_1 & \hdots & \bb_1 \cdot \aa_n \\
            \vdots & & \vdots \\
            \bb_m \cdot \aa_1 & \hdots & \bb_m \cdot \aa_n
        \end{pmatrix},
    \end{align*}
    
    where $\aa_i$ is the $i$th column of $\AA$ and $\bb_i$ is the $i$th row of $\BB$. So the $ij$ entry $(\BB \AA)_{ij}$ of $\BB \AA$ is $\bb_i \cdot \aa_j$, which is
    
    \begin{align*}
        \boxed
        {
            (\BB \AA)_{ij} = (\text{$i$th row of $\BB$}) \cdot (\text{$j$th column of $\AA$})
        }
    \end{align*}
    
    As was the case in Theorem \ref{ch::lin_alg::thm::coordinates_of_matrix_vector_product}, what we mean by ``$i$th row of $\BB$'' in the boxed equation is ``column vector that contains entries of $i$th row of $\AA$''.
\end{theorem}

\begin{remark}
    (Matrix pedagogy). 
    
    Most linear algebra texts present the relationship between linear functions and matrices in the following way: first define matrices in the context of systems of linear equations (we have not seen how matrices are related to systems of linear equations), then define a linear function to be one for which $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ and $\ff(c\vv) = c\ff(\vv)$ for all vectors $\vv, \ww$ and scalars $c$, and then prove that each linear function has a primitive matrix. This is bad pedagogy; there should be no need to conjecture and prove that a matrix-vector product corresponds to the action of a linear function, because this fact is apparent from Derivation \ref{ch::lin_alg::deriv::primitive_matrix}. (Furthermore, while systems of linear equations are an important application of linear algebra, and while their study does enhance our knowledge about the kernels of linear functions, they should not be the starting point).
    
    Oftentimes, linear algebra texts present the formula for the $i$th entry of a matrix-vector product and the formula for the $ij$th entry of a matrix-matrix product as facts that should be memorized rather than understood. Be wary of this! You \textit{should not} memorize these formulas. If you can't quite remember them, try to derive them by starting with the fact that linear functions on finite-dimensional vector spaces are determined by what they do to bases, and by following the derivations given in this book!
\end{remark}

\newpage

\subsubsection*{Matrices and change of basis [NEW]}

\begin{theorem}
    [put this somewhere]

    Proof that $[\cdot]_E$ is invertible.

    $[\cdot]_E$ is invertible because it sends basis vectors to basis vectors, and therefore preserves linear independence.
\end{theorem}

\begin{theorem}
    \label{ch::lin_alg::thm::change_of_basis_for_vectors}
    
    (Change of basis).
    
    \textbf{change this theorem to be a derivation that discovers $[\ff(E)]_F$}
    
    Let $V$ be a finite-dimensional vector space with bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$. We will now discover how to relate $[\vv]_E$ to $[\vv]_F$.
    
    To start, consider the special case $V = K^n$. Then we can let $\EE$ and $\FF$ be the matrices with $i$th columns $\ee_i$ and $\ff_i$, respectively. For $\cc \in K^n$, we have $\FF [\cc]_F = \cc = \EE [\cc]_E$, so $[\cc]_F = \FF^{-1} \EE [\cc]_E$.
    
    Now, we investigate the matrix $\FF^{-1} \EE$ of the previous expression.
    
    By the definition of matrix-matrix multiplication (Definition \ref{ch::lin_alg::thm::matrix_matrix_product_relative_to_bases_primitive}), we have
    
    \begin{align*}
        \FF^{-1} \EE = \FF^{-1} \begin{pmatrix} \ee_1 & \hdots & \ee_n \end{pmatrix} = \begin{pmatrix} \FF^{-1} \ee_1 & \hdots & \FF^{-1} \ee_n \end{pmatrix}
        = \begin{pmatrix} [\ee_1]_F & \hdots & [\ee_n]_F \end{pmatrix}
        = [\cdot]_F(E) := [\EE]_F,
    \end{align*}
    
    where we have defined the notation $[\EE]_F := [\cdot]_F(E)$.
    
    Therefore
    
    \begin{align*}
        [\cc]_F = [\EE]_F [\cc]_E.
    \end{align*}
    
    We now generalize this result to one that holds when $V$ is an arbitrary finite-dimensional vector space.
    
    The matrix $[\cdot]_F(E) = [\EE]_F$ was the centerpiece of the above argument. To generalize, we notice that the primitive matrix of $[\cdot]_F$ relative to $E$ is $[\cdot]_F(E) = [\EE]_F$. The characterizing property of primitive matrices then implies that for any $\vv \in V$, we have
    
    \begin{align*}
        \boxed
        {
            [\vv]_F = [\EE]_F [\vv]_E
        }
    \end{align*}
\end{theorem}

\begin{remark}
    (Involvement of the identity in change of basis).
    
    It's also worth noting that since $[\EE]_F = [\II_V(E)]_F$, then $[\EE]_F$ is the matrix of the identity $\II_V$ on $V$. So, $[\vv]_F = [\EE]_F [\vv]_E$ can be restated as
    
    \begin{align*}
        [\cdot]_F = [\II_V(E)]_F \circ [\cdot]_E.
    \end{align*}
    
    This equation is not of much practical use, but it does give more insight; it is a good sanity check that the identity on $V$ is involved in changing bases, since representing a vector with different bases does not change the vector itself.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::thm::I_EF}
    ($(\II_V)_{E,F}^{-1} = (\II_V)_{F,E}$).
    
    Let $V$ be a vector space. The identity function $\II_V:V \rightarrow V$ on $V$ satisfies $(\II_V)_{E,F}^{-1} = (\II_V)_{F,E}$. As a corollary, we have $[\EE]_F^{-1} = [\FF]_E$.
\end{theorem}

\begin{proof}
    Given any bases $E, F$ of $V$, Theorem \ref{ch::lin_alg::thm::f_EF} defines $\ff_{E,F} := [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$. Therefore $(\II_V)_{E,F} = [\cdot]_F \circ [\cdot]_E^{-1}$. Since the definition of $\ff_{E,F}$ holds for any two bases of $V$, we can switch $E$ and $F$ to obtain $(\II_V)_{F,E} = [\cdot]_E \circ [\cdot]_F^{-1}$. The claim follows.
    
    We obtain the corollary by starting with $(\II_V)_{E,F}^{-1} = (\II_V)_{F,E}$ and taking the primitive matrices of each side, relative to $E$ and $F$, respectively.
\end{proof}

\begin{comment}
    %Unnecessary/impractical
    \begin{theorem}
        (Change of basis for linear functions). 
        
        Let $V$ and $W$ be finite-dimensional vector spaces. Let $E, G$ be bases of $V$, let $F, H$ be bases of $W$, and consider a linear function $\ff:V \rightarrow W$. Then $\ff_{E,F}$ and $\ff_{G,H}$ are related by
        
        \begin{align*}
            \ff_{G,H} = [\cdot]_H \circ [\cdot]_{F^{-1}} \circ \ff_{E,F} \circ [\cdot]_E \circ [\cdot]_{G^{-1}}.
        \end{align*}
        
        This is because $\ff_{E,F}$ was defined as $\ff_{E,F} := [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$. (See Theorem \ref{ch::lin_alg::thm::f_EF}). But $[\cdot]_H \circ [\cdot]_{F^{-1}} = (\II_W)_{F,H}$ and $[\cdot]_E \circ [\cdot]_{G^{-1}} = (\II_V)_{G,F}$, so
        
        \begin{align*}
            \ff_{G,H} = (\II_W)_{F, H} \circ \ff_{E,F} \circ (\II_V)_{G,F}.
        \end{align*}
        
        We now translate the above equation into a statement about primitive matrices. Since the primitive matrix of a composition of functions is the product of matrices taken relative to the appropriate bases (see Theorem \ref{ch::lin_alg::thm::matrix_matrix_product_relative_to_bases}), we have
    
        \begin{align*}
            [\ff(G)]_H = [\II_W(F)]_H [\ff(E)]_F [\II_V(G)]_F = [\FF]_H [\ff(E)]_F [\GG]_F = [\FF]_H [\ff(E)]_F [\FF]_G^{-1}.
        \end{align*}
        
        The last equality follows from the previous theorem.
        
    \end{theorem}
\end{comment}

\begin{theorem}
\label{thm::lin_alg::thm::change_of_bases_fns_common_special_case}
    (Change of basis for linear functions).

    [Already encapsulated in the notion of $[\ff(E)]_F$. Just use $V = W$.]
    
    The theorem that has just been stated is what people refer to when they speak of changing the bases of a linear function's matrix.
\end{theorem}

\begin{theorem}
\label{ch::lin_alg::thm::change_of_basis_with_basis_vectors}
    (Change of basis in terms of basis vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$. By the definition of $[\cdot]_F$, we have
    
    \begin{align*}
        \boxed
        {
            \ff_i = \sum_{j = 1}^n ([\ff_i]_E)_j \ee_j = \sum_{j = 1}^n ([\FF]_E)_{ji} \ee_j
        }
    \end{align*}
    
    In the last equality, we have used that $[\ff_i]_E$ is the $i$th column of $[\FF]_E$.
\end{theorem}

\begin{remark}
    (On the order of proving change of basis theorems). 
    
    Most linear algebra texts first prove the previous theorem and use it to show a version of the first equation in the box of Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors}. This approach for proving Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors} was not used because it involves quite a bit more matrix algebra than the approach supplied in this text. However, it good to know that these theorems are equivalent.
\end{remark}

=========================

\begin{remark}
    (An alternate meaning to ``matrix relative to $E$'' to be aware of).

    This remark is relatively optional. It boils down to: be aware that other people say ``matrix relative to $E$'' to mean a different thing than we do.
    
    Without further ado... consider a linear function $\ff:K^n \rightarrow K^n$, let $E$ be a basis for $K^n$, and recall that the primitive matrix of $[\cdot]_E$ relative to $\sE$ is $\EE^{-1}$, where $\EE$ is the matrix whose $i$th column is $\ee_i$.
    
    Now, consider the matrix $[\ff(E)]_E$ of $\ff$ relative to $E$, which satisfies $[\ff(\cc)]_E = [\ff(E)]_E [\cc]_E$. Since ${[\ff(\cc)]_E = \EE^{-1} \ff(\cc)}$ and $[\cc]_E = \EE^{-1} \cc$, we have $\EE^{-1} \ff(\cc) = [\ff(E)]_E \EE^{-1} \cc$; multiplying each side on the left by $\EE$, we see that $[\ff(E)]_E$ is such that $\ff(\cc) = \EE \spc \ff(E) \EE^{-1} \cc$, i.e. $\ff(\cc) = (\EE \spc [\ff(E)]_E \EE^{-1}) \cc$.
    
    This most recent equation makes it natural to associate $\ff$ with the matrix $\EE \spc [\ff(E)]_E \EE^{-1}$, so people often refer to $\EE \spc \ff(E) \EE^{-1}$ as ``the matrix of $\ff$ relative to $E$''. This convention is incompatible with the convention of our own definition, which says that the matrix of $\ff$ relative to $E$ is $[\ff(E)]_E = \EE^{-1} \ff(E)$. Furthermore, this alternate convention doesn't work when $V \neq K^n$, since when $V \neq K^n$, the vector $\ee_i \in V$ is not a tuple of scalars, making $\EE$ not a grid of scalars, i.e., not a matrix, and thus making matrix-matrix products such as $\EE \spc \ff(E) \EE^{-1}$ nonsensical.
    
    We will stick with our convention and ignore the convention which gives a special name to $\EE^{-1} \spc \ff(E) \EE$. Just be aware that many texts use this convention!
\end{remark}

\begin{remark}
    (Matrix terminology in this text).
    
    \textbf{may want to delete this remark; not really necessary}
    
    Let $\ff$ be a linear function.
    
    \begin{itemize}
        \item When $\ff:K^n \rightarrow K^n$ and $E$ is a basis for $K^n$, we refer to $\ff(E)$ as \textit{the primitive matrix of $\ff$ relative to $E$}.
        \item When $\ff:V \rightarrow V$, we refer to $[\ff(E)]_E$ as \textit{the matrix of $\ff$ relative to $E$ and $E$}, or as \textit{the matrix of $\ff$ relative to $E$}.
        \item When $\ff:K^n \rightarrow K^n$ and $E$ is a basis for $K^n$, some refer to the matrix $\EE \spc \ff(E) \EE^{-1}$ as ``the matrix of $\ff$ relative to $E$''. We will not use this convention.
        %\item When $\ff:V \rightarrow V$, the matrix of $\ff_{E,E}$ relative to $\sE$ (in the alternate convention) is equal to the matrix of $\ff$ relative to $E$: we have $\II \spc \ff_{E,E}(\sE) \II^{-1} = \ff_{E,E}(\sE) = [\ff(E)]_E$.
    \end{itemize}
\end{remark}

\subsubsection*{Optional: relationship between primitive matrices and matrices with respect to bases}

We now investigate the relationship between primitive matrices and matrices with respect to bases. It turns out that primitive matrices can be interpreted to be matrices with respect to bases, and that matrices with respect to bases can be interpreted to be primitive matrices!

\begin{theorem}
\label{ch::lin_alg::thm::f_EF}

    (Matrices with respect to bases are the primitive matrices of linear functions $K^n \rightarrow K^m$).
    
    The previous remark discussed how primitive matrices can be regarded as matrices with respect to bases. This theorem explores the converse of the last remark: we will see that every matrix with respect to bases can be viewed as a primitive matrix of a linear function $K^n \rightarrow K^m$. (Note, this converse was already explored to some extent in Derivation \ref{ch::lin_alg::deriv::matrix_wrt_bases}, as in that theorem, we interpreted $[\ff(E)]_F$ as the primitive matrix of $[\cdot]_F \circ \ff:V \rightarrow K^m$).
    
    Let $V$ and $W$ be a finite-dimensional vector spaces with bases $E$ and $F$, respectively. We know from Derivation \ref{ch::lin_alg::deriv::matrix_wrt_bases} that every linear function $\ff:V \rightarrow W$ corresponds to the matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$, where $[\ff(E)]_F$ is characterized by the equation $[\ff(\vv)]_F = [\ff(E)]_F [\vv]_E$. Rephrase this equation as $([\cdot]_F \circ \ff)(\vv) = [\ff(E)]_F [\vv]_E$ and set $\cc = [\vv]_E$ to obtain
    
    \begin{align*}
        ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1})(\cc) = [\ff(E)]_F \spc \cc.
    \end{align*}
    
    Now we define $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ by
    
    \begin{align*}
        \boxed
        {
            \ff_{E,F} := [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}
        }
    \end{align*}

    so that the above rephrases as
    
    \begin{align*}
        \ff_{E,F}(\cc) = [\ff(E)]_F \spc \cc = [\ff(E)]_F [\cc]_\sE,
    \end{align*}
    
    where $\sE$ is the standard basis for $K^n$. Here, we have used the fact that for $\cc \in K^n$, we have $[\cc]_\sE = \cc$.
    
    Looking at the characterizing property of a primitive matrix (see the last box of Derivation \ref{ch::lin_alg::deriv::primitive_matrix}), we see that $[\ff(E)]_F$ is the primitive matrix of $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ relative to $\sE$. Recall from the previous remark that we can notate the primitive matrix of $\ff_{E,F}$ relative to $\sE$ by $\ff_{E,F}(\sE)$ to restate the fact of the previous sentence:
    
    \begin{align*}
        \boxed
        {
            \underbrace{\ff_{E,F}(\sE)}_{\text{primitive matrix of $\ff_{E,F}$ relative to $\sE$}}
            =
            \underbrace{[\ff(E)]_F}_{\text{matrix of $\ff$ relative to $E$ and $F$}}
        }
    \end{align*}

    We can additionally understand $\ff_{E,F}$ to be the ``induced'' linear function for which this diagram commutes:
    
    \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRRkAjFVqMWbAGIA9YAB0FULAFsAFOwCUXbrxAZseAkRHlx9Zq0QgA6nr5HBp0mOqWpNuYuVr1tnW5xGCgAc3giUAAzACcIVSQyEBwIJDMJKzZkJQBjKAgcCgB9AFEAAgBeMpKQagY6ACMYBgAFfmMhEBisUIALHAcQWPikACZqFKQAZndJaxBshTyC4ulKsulB4YTEdMnEGYzPEAAVIuAS0mldOsbmtqcTG26+gZ5ouJ2k-fGj+ZOglwgA
        \begin{tikzcd}
            V \arrow[d, "{[\cdot]_E}"'] \arrow[r, "\ff"] & W \arrow[d, "{[\cdot]_F}"] \\
            K^{\dim(V)} \arrow[r, "{\ff_{E,F}}"']            & K^{\dim(W)}
        \end{tikzcd}
    \end{center}
        
    To say the diagram ``commutes'' is just another way of saying $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$. We think of $\ff_{E,F}$ as being \textit{induced} by the choice of $\ff$.
    
    %Note that the input of $\ff_{E, F}$ is best thought of as being expressed relative to the basis $E$ of $V$, since
        
    %\begin{align*}
    %    \ff_{E, F}([\vv]_E) = [\ff(\vv)]_F.
    %\end{align*}
\end{theorem}

\begin{remark}
\label{ch::lin_alg::rmk::primitive_matrix_as_matrix_wrt_bases}

    (Primitive matrix as special case of matrix relative to bases). 
    
    The primitive matrix $\ff(E)$ of a linear function $\ff:V \rightarrow K^m$ relative to $E$ is the matrix $\ff(E) = [\ff(E)]_{\sE}$ of $\ff:V \rightarrow K^m$ relative to the bases $E$ and $\sE$, where $\sE$ is the standard basis for $K^m$, and where $\ff(E)$ is the matrix resulting from a linear function $V \rightarrow K^m$ acting on a list of vectors (see Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors}). The key fact here is that $\left[\begin{pmatrix} c_1 \\ \vdots \\ c_m \end{pmatrix}\right]_\sE = \begin{pmatrix} c_1 \\ \vdots \\ c_m \end{pmatrix}$ for all column vectors $\begin{pmatrix} c_1 \\ \vdots \\ c_m \end{pmatrix} \in K^m$.
\end{remark}
