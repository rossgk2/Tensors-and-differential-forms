\subsection*{Orientation of finite-dimensional vector spaces}
\label{ch::lin_alg::section::orientation}
%\begin{itemize}
%    \item \url{https://arxiv.org/pdf/1103.5263.pdf}
%    \item \url{https://en.wikipedia.org/wiki/Cartan\%E2\%80\%93Dieudonn\%C3\%A9_theorem}
%    \item \url{https://en.wikipedia.org/wiki/Rotor_(mathematics)#:~:text=A\%20rotor\%20is\%20an\%20object,the\%20Cartan\%E2\%80\%93Dieudonn\%C3\%A9\%20theorem).}
%    \item \url{https://en.wikipedia.org/wiki/Geometric_algebra#Rotating_systems}
%    \item \url{https://www.euclideanspace.com/maths/algebra/clifford/d4/transforms/index.htm}
%\end{itemize}

\textit{Orientation} is the mathematical formalization of the notions of ``clockwise'' and ``counterclockwise''; it is the notion which distinguishes different ``rotational configurations'' from each other.

Formally, an \textit{orientation} for an $n$-dimensional inner product space\footnote{We need to use inner product spaces so that we can speak of orthonormal ordered bases.} will be an orthonormal ordered basis for that space, subject to an equivalence relation, so that orientations are considered equivalent iff one of them can be ($n$-dimensionally) rotated into the other. We will say that equivalent orthonormal ordered bases are ``rotationally equivalent''.

In the intuitive case of $n = 2$, we will see that the rotational equivalence notion leads to the key \textit{antisymmetry} property of orthonormal ordered bases. We will also see that- even in $n$-dimensions- an arbitrary orthonormal ordered basis is rotationally equivalent to to one of two orientations. When an arbitrary orthonormal ordered basis is rotationally equivalent to the one, it is said to be ``positively oriented''; when it is rotationally equivalent to the other, it is said to be ``negatively oriented''.

After the above is established, the remainder of this section is devoted to computing the orientation of arbitrary orthonormal ordered bases relative to a chosen orientation. Relying on antisymmetry alone, we are able to compute orientations of orthonormal ordered bases that are permutations of the chosen orientation. To compute the orientation of other orthonormal bases, we have to engage a bit with the notion of rotation in $n$-dimensions, since this is, after all, the fundamental concept behind rotational equivalence. The idea is that the orientation of an arbitrary orthonormal ordered basis is equal to the orientation of a ``close-by'' permuted ordered orthonormal basis.

The culminating fact of this section is that the determinant ``tracks'' orientation: the determinant of an ordered orthonormal basis for $K^n$ gives the orientation of that basis.

\subsection*{First notions of orientation}

\subsubsection*{First notions of orientation in two dimensions}

\begin{defn}
    (Ordered basis).
    
    An \textit{ordered basis} for a vector space is a simply a tuple containing some ordering of the basis vectors for that space.
    
    For example, if $V$ is a 2-dimensional vector space and has basis $E = \{\ee_1, \ee_2\}$, then $(\ee_1, \ee_2)$ and $(\ee_2, \ee_1)$ are both ordered bases of $V$. We have $(\ee_1, \ee_2) \neq (\ee_2, \ee_1)$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::2-rotation}
    ($2$-dimensional rotation).
    
    Let $V$ be a $2$-dimensional inner product space, and let $\hU$ be an orthonormal ordered basis for $V$.
    A \textit{$2$-dimensional rotation on $V$} is a linear function $\RR_\theta:V \rightarrow V$ whose matrix relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix},
        \text{ where $\theta \in [0, 2\pi)$}.
    \end{align*}
\end{defn}

\begin{defn}
    (Equivalence under rotation for $2$-dimensional inner product spaces).

    We define orthonormal ordered bases $\hU = (\huu_1, \huu_2)$ and $\hW = (\hww_1, \hww_2)$ of a 2-dimensional inner product space\footnote{Note, this is the first time we have required that the vector space under consideration be an inner product space. We need this constraint so that we can speak of orthonormal bases.} to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).

    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

We now see how rotational equivalence leads to the key property- antisymmetry- of orthonormal ordered bases.

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_2_dimensions}

    (Antisymmetry of orthonormal ordered bases for a $2$-dimensional inner product space).
    
    Let $\hU = (\huu_1, \huu_2)$ be an orthonormal ordered basis of a $2$-dimensional inner product space. When $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2} \}$, the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \pm
        \begin{pmatrix}
            0 & -1 \\
            1 & 0
        \end{pmatrix}.
    \end{align*}
    
    Computing $\RR_\theta(\hU) = \RR_\theta((\huu_1, \huu_2)) = (\RR_\theta(\huu_1), \RR_\theta(\huu_2))$ for $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$, we see that the following ordered bases are rotationally equivalent:
    
    \begin{align*}
        (\huu_2, \huu_1 ) &\sim (-\huu_1, \huu_2) \sim (\huu_1, - \huu_2) \\
        (\huu_1, \huu_2) &\sim (-\huu_2, \huu_1) \sim (\huu_2, -\huu_1)
    \end{align*}
\end{theorem}

\subsubsection*{First notions of orientation in $n$ dimensions}

To study orientation in $n$ dimensions, we generalize the key notion of rotational equivalence by ``extending'' $2$-dimensional rotations so that they can be applied to elements of $n$-dimensional inner product spaces. 

\begin{defn}
    (Extension of a function).

    Let $X$ and $Y$ be sets, let $X_1 \subset X$ be subset of $X$, and consider a function $f:X_1 \rightarrow Y$. The \textit{extension of $f$ to $X$} is the function $f_{\text{ext}}$ defined by

    \begin{align*}
        f_{\text{ext}}(x) =
        \begin{cases}
            f(x) & x \in X_1 \\
            x & x \notin X_1 
        \end{cases}.
    \end{align*}
    
    We will speak frequently of extensions of $2$-dimensional rotations. In fact, we will engage in a bit of abuse of notation: when $V$ is an inner product space, we will say ``the $2$-dimensional rotation $\RR_\theta$ on $V$'' to mean ``the extension of the $2$-dimensional rotation $\RR_\theta$ to $V$'', and use $\RR_\theta$ to denote $(\RR_\theta)_{\text{ext}}$.
    
    It's helpful to spell out a further detail. If $\RR_\theta$ is a $2$-dimensional rotation on a finite-dimensional inner product space and $\hU$ is an orthonormal basis of the space, then the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is    
    
    \begin{align*}
        [\RR_\theta(\hU)]_{\hU} =
        \kbordermatrix
        {
             & & & \text{$i$th column} &  & \text{$j$th column}   \\
             & 1 & \hdots & \cos(\theta) & 0 & -\sin(\theta) & 0 \\
             & 0 & & 0 & \vdots & 0 & \vdots \\
             & 0 & & \vdots & 1 & \vdots & \vdots \\
             & \vdots & & 0 & \vdots & 0 & \vdots \\
             & 0 & \hdots & \sin(\theta)  & 0 & \cos(\theta) & 1
        }.
    \end{align*}
    
    (The columns other than the $i$th and $j$th columns are the columns of the $n \times n$ identity matrix).
\end{defn}

Now that we are equipped with $2$-rotations that can be applied on finite-dimensional inner product spaces, we can generalize our definition of rotational equivalence to $n$ dimensions.

\begin{defn}
    (Equivalence under rotation).
    
    We define orthonormal ordered bases $\hU$ and $\hW$ of a finite-dimensional inner product space to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).
    
    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

[copy-pasted]

This fundamental result leads to the following [...].

[copy-pasted]

\begin{defn}
    (Orientation of permuted ordered bases).

    \textbf{LOOK OVER THIS. MAYBE EDIT IN LIGHT OF NEW PREVIOUS THEOREM}
    
    This means that there are only two equivalence classes\footnote{I find this relatively surprising. My intuition is that there would be something like $2^n$ or $n!$ equivalence classes of ``equivalence under rotation'' in $n$ dimensions, but nope! There are $2$ equivalence classes of ``equivalence under rotation'' for every $n$.} of ``equivalence under rotation''.
    
    We can now begin to set up the notion of orientation. An \textit{orientation for the $n$-dimensional inner product space $V$} is a choice of an orthonormal ordered basis $\hU$ for $V$. When $V$ is given the orientation $\hU$, then the \textit{orientation of a permutation of $\hU$ (relative to $\hU$)} is said to be \textit{positive} iff that permutation of $\hU$ is rotationally equivalent to $\hU$, and is said to be \textit{negative} otherwise. Per the previous paragraph, every permutation of $\hU$ is either positively oriented or negatively oriented relative to $\hU$.
\end{defn}

\begin{remark}
\label{ch::lin_alg::rmk::formalization_ccw_cw}
    (The formalization of ``counterclockwise'' and ``clockwise'').
    
    At the beginning of this section, we said that orientation would formalize the notions of ``clockwise'' and ``counterclockwise''. This formalization has been achieved by the previous definition.
    
    A \textit{counterclockwise rotational configuration} is another name for the orientation given to $\R^3$ by the standard basis, \textit{when we use the normal} human \textit{convention} of drawing the ordered basis $\sE = (\see_1, \see_2, \see_3)$ such that $\sE$ can be rotated so that $\see_1$ points out of the page, $\see_2$ points to the right, and $\see_3$ points upwards on the page. In this visual convention, the direction of each basis vector corresponds to its position in $\sE$. Counterclockwise rotational configurations are also called ``right handed coordinate systems''.
    
    A \textit{clockwise rotational configuration} then corresponds to the ordered bases which are not rotationally equivalent to $\sE$. One such ordered basis, $(-\see_1, \see_2, \see_3)$, can be depicted using the visual convention just established by drawing $\see_1$ as pointing into the page (i.e. $-\see_1$ points out of the page), $\see_2$ as pointing to the right, and $\see_3$ as pointing upwards on the page. Clockwise rotational configurations are also called ``left handed coordinate systems''.
    
    We could have easily picked a different visual convention (i.e. a different permutation of in/out, left/right, up/down) to represent the ordering of the basis that is considered to orient the space.
\end{remark}

At this point, we need some definitions and facts about $n$-rotations before we complete our development of orientation.

\subsection*{Orientation in $n$ dimensions}

\begin{defn}
    ($n$-dimensional rotations by Euler angles).

    Let $V$ be an $n$-dimensional inner product space and let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis for $V$. An \textit{($n$-dimensional) Euler rotation} is a function $\RR_{\theta_1, ..., \theta_k}:V \rightarrow V$ of the form ${\RR_{\theta_1, ..., \theta_k} = \RR_{k, \theta_k} \circ ... \circ \RR_{1, \theta_1}}$, where, for each $i$, $\RR_{i, \theta_i}$ is the $2$-dimensional rotation by $\theta_i \in [0, 2\pi)$ on the subspace $\spann(\hU - \{\huu_i\})$. The angles $\theta_1, ..., \theta_k$ are called \textit{Euler angles}.
\end{defn}

\begin{theorem}
    All Euler rotations are orthogonal linear functions.
\end{theorem}

\begin{proof}
    This follows because $2$-dimensional rotations are orthogonal linear functions, and since a composition of orthogonal linear functions is another orthogonal linear function.
\end{proof}

% \begin{theorem}
%     Outline of proof that determinant tracks orientation:

%     \begin{itemize}
%         \item arbitrary ordered basis of $V$ -> orthonormal ordered basis of $V$ via Gram-Schmidt
%         \item orthonormal ordered basis of $V$ -> permutation of $\hU$ via Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
%         \item Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}: any ordered basis is rotationally close to a permuted orthonormal basis
%     \end{itemize} 

%     ``To do so, we first choose $\alpha, \beta, \gamma$ such that $\RR(\hww_1) = \huu_1$.''    
% \end{theorem}

\begin{comment}
\begin{lemma}
    (Standard matrix of Euler rotation).

    Let $V$ be a $3$-dimensional inner product space and let $\hU$ be an orthonormal ordered basis for $V$. Consider a rotation $\RR_{\alpha, \beta, \gamma}$ by the Euler angles $\alpha, \beta, \gamma \in [0, 2\pi)$. The matrix of $\RR_{\alpha, \beta, \gamma}$ relative to $\hU$ and $\hU$ is the product of the standard matrices of $\RR_\alpha$, $\RR_\beta$, and $\RR_\gamma$, and is equal to

    \begin{align*}
        &\begin{pmatrix}
            \cos(\gamma) & -\sin(\gamma) & 0 \\ \sin(\gamma) & \cos(\gamma) & 0 \\
            0 & 0 & 1
        \end{pmatrix} 
        \begin{pmatrix}
            \cos(\beta) & 0 & \sin(\beta) \\
            0 & 1 & 0 \\
            -\sin(\beta) & 0 & \cos(\beta)
            \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & \cos(\alpha) & -\sin(\alpha) \\
            0 & \sin(\alpha) & \cos(\alpha)
        \end{pmatrix} \\
        = 
        &\begin{pmatrix}
            \cos(\gamma)\cos(\beta) & \cos(\gamma)\sin(\beta)\sin(\alpha)-\sin(\gamma)\cos(\alpha) & \cos(\gamma)\sin(\beta)\cos(\alpha)+\sin(\gamma)\sin(\alpha) \\
            \sin(\gamma)\cos(\beta) & \sin(\gamma)\sin(\beta)\sin(\alpha)+\cos(\gamma)\cos(\alpha) & \sin(\gamma)\sin(\beta)\cos(\alpha)-\cos(\gamma)\sin(\alpha) \\
            -\sin(\beta) & \cos(\beta)\sin(\alpha) & \cos(\beta)\cos(\alpha)
        \end{pmatrix}.
    \end{align*}
\end{lemma}

\begin{proof}
    Left as an exercise :)
\end{proof}
\end{comment}

\begin{lemma}
    (In three dimensions, there is an Euler rotation taking any nonzero vector to any other vector).

    Let $V$ be a $3$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, \huu_2, \huu_3)$ be an orthornormal ordered basis of $V$. If we can find angles $\alpha_1, \beta_1$ that rotate $\vv$ to align with $\huu_3$,

    \begin{align*}
        (\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3,
    \end{align*}
    
    and angles $\alpha_2, \beta_2$ that rotate $\huu_3$ to align with $\ww$, 

    \begin{align*}
        (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww,
    \end{align*}

    then the Euler rotation $\RR := \RR_{2, \beta_2} \circ \RR_{1, \alpha_2} \circ \RR_{2, \beta_1} \circ \RR_{1, \alpha_1}$ satisfies $\RR(\vv) = \ww$, and the lemma is proven.

    \vspace{.5cm}

    First, we find $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    Letting $(v_1, v_2, v_3)^\top := [\vv]_{\hU}$, we have
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_1) & -\sin(\alpha_1) \\
                0 & \sin(\alpha_1) & \cos(\alpha_1)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\ v_2 \\ v_3
            \end{pmatrix}
        }_{[\vv]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \\
            v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) \\
            v_2 \sin(\alpha_1) + v_3 \cos(\alpha_1)
        \end{pmatrix}.
    \end{align*}

    $\alpha_1$ is such that the second component of the above is zero,

    \begin{align*}
        v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) = 0.
    \end{align*}
    
    One value of $\alpha_1$ that solves this equation is $\alpha_1 = \arctan(v_2/v_3)$. With this choice of $\alpha_1$, we have \\ ${\cos(\alpha_1) = v_3/\sqrt{v_2^2 + v_3^2}}$ and ${\sin(\alpha_1) = v_2/\sqrt{v_2^2 + v_3^2}}$. Thus the above is
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \begin{pmatrix}
            v_1 \\
            (v_2 v_3)/\sqrt{v_2^2 + v_3^2} - (v_3 v_2)/\sqrt{v_2^2 + v_3^2} \\
            (v_2^2 + v_3^2)/\sqrt{v_2^2 + v_3^2}
        \end{pmatrix}
        =
        \begin{pmatrix}
            v_1 \\
            0 \\
            \sqrt{v_2^2 + v_3^2}
        \end{pmatrix}.
    \end{align*}

    Now we have

    \begin{align*}
        [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} &=
        [\RR_{2, \beta_1}(\RR_{1, \alpha_1}(\vv))]_{\hU} = [\RR_{2, \beta_1}(\hU)]_{\hU} [\RR_{1, \alpha_1}(\vv)]_{\hU} \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_1) & 0 & -\sin(\beta_1) \\
                0 & 1 & 0 \\
                \sin(\beta_1) & 0 & \cos(\beta_1)
            \end{pmatrix}
        }_{[\RR_{2, \beta_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\
                0 \\
                \sqrt{v_2^2 + v_3^2}
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\vv)]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) \\
            0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1)
        \end{pmatrix}.
    \end{align*}

    $\beta_1$ is such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3 \iff [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} = ||\vv||\see_3$. So $\beta_1$ satisfies the following system of equations:

    \begin{align*}
        \begin{cases}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) = 0 \\
            0 = 0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1) = ||\vv||
        \end{cases}.
    \end{align*}

    One value of $\beta_1$ that solves the first equation is $\beta_1 = \arctan(v_1/ \sqrt{v_2^2 + v_3^2})$. 
    
    Since $\cos(\beta_1) = \sqrt{v_2^2 + v_3^2}/||\vv||$ and $\sin(\beta_1) = v_1/||\vv||$, the third equation is also satisfied:

    \begin{align*}
        \frac{v_1^2}{||\vv||} + \frac{\sqrt{v_2^2 + v_3^2}^2}{||\vv||} = \frac{v_1^2 + v_2^2 + v_3^2}{||\vv||} = \frac{||\vv||^2}{||\vv||} = ||\vv||.
    \end{align*}

    Thus we have found $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    \vspace{.5cm}

    Now, we find $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
    
    We have

    \begin{align*}
        [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU} &=
        [\RR_{2, \beta_2}(\hU)]_{\hU} [\RR_{1, \alpha_2}(\hU)]_{\hU} \huu_3 \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_2) & 0 & -\sin(\beta_2) \\
                0 & 1 & 0 \\
                \sin(\beta_2) & 0 & \cos(\beta_2)
            \end{pmatrix}
        }_{[\RR_{2, \beta_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_2) & -\sin(\alpha_2) \\
                0 & \sin(\alpha_2) & \cos(\alpha_2)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                0 \\ 0 \\ 1
            \end{pmatrix}
        }_{\huu_3} \\
        &=
        \begin{pmatrix}
            \cos(\beta_2) & 0 & -\sin(\beta_2) \\
            0 & 1 & 0 \\
            \sin(\beta_2) & 0 & \cos(\beta_2)
        \end{pmatrix}
        \begin{pmatrix}
            0 \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2)
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\cos(\alpha_2) \sin(\beta_2) \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2) \cos(\beta_2)
        \end{pmatrix}.
    \end{align*}

    \newcommand{\hw}{\hat{w}}

    $\alpha_2$ and $\beta_2$ are such that $\hww = (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) \iff [\hww]_{\hU} = [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU}$. Letting $\hw_i$ denote the $i$th component of $[\hww]_{\hU}$, we see $\alpha_2$ and $\beta_2$ must satisfy the following system of equations:

    \begin{align*}
        \begin{cases}
            \hw_1 = -\cos(\alpha_2) \sin(\beta_2) \\
            \hw_2 = -\sin(\alpha_2) \\
            \hw_3 = \cos(\alpha_2) \cos(\beta_2)
        \end{cases}.
    \end{align*}

    One value of $\alpha_2$ that solves the second equation is $\alpha_2 = -\arcsin(\hw_2)$. Dividing the first equation by the third, we have $\hw_1/\hw_3 = -\tan(\beta_2)$ and thus one value of $\beta_2$ that solves the first equation is $\beta_2 = \arctan(-\hw_1/\hw_3) = -\arctan(\hw_1/\hw_3)$.

    Thus we have found $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
\end{proof}

The previous lemma can easily be generalized so that it holds for all dimensions greater than or equal to $2$.

\begin{lemma}
    (There is an Euler rotation taking any nonzero vector to any other vector).
    
    Let $V$ be an $n$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    \mbox{} \\

    (Case: $n = 2$). The rotation $\RR_{\theta(\vv, \ww)}$ by $\theta(\vv, \ww)$ sends $\vv$ to $\ww$.
    
    (Case: $n = 3$). See the previous lemma.
    
    (Case: $n > 3$).
    
    \indent (Case: $n$ is even). [TO-DO]
    
    \indent (Case: $n$ is odd). [TO-DO]
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
     (Any ordered basis is an Euler rotation of a permuted orthonormal basis).
     
     Let $V$ be an finite-dimensional inner product space. If $\hW = (\hww_1, ..., \hww_k)$ and $\hU = (\huu_1, ..., \huu_k)$ are orthonormal ordered bases of subspaces of $V$, then there is an Euler rotation taking $\hW$ to an ordered basis that is rotationally equivalent to some permutation $\hU^\sigma$ of $\hU$. 
\end{theorem}

\begin{proof}
    By the previous lemma, we know there is an Euler rotation $\RR$ that satisfies $\RR(\hww_1) = \huu_1$. 
    
    We first prove that for all $i$ we have $\RR(\hww_i) = s_i \huu_i$, where $s_i \in \{-1, 1\}$. And we show this by using induction to show that for all $i$, $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    (Base case). We have $R(\hww_1) = \huu_1 = s_1 u_1$, where $s_1 = 1$.

    (Inductive case). Assume that $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    Since $\RR$ is an orthogonal linear function, it preserves orthonormality (\textbf{see Theorem [...]}), and so $(\RR(\hww_1), ..., \RR(\hww_k))$ is orthonormal. Thus, for each $j$, $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \cancel{\RR(\hww_j)}, ..., \RR(\hww_k)$. In particular, when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \RR(\hww_i)$. By the inductive hypothesis, $\RR(\hww_1) = s_1 \huu_1, ..., \RR(\hww_i) = s_i \huu_i$, where $s_1, ..., s_i \in \{-1, 1\}$. So when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $s_1 \huu_1, ..., s_i \huu_i$; it follows that each $\RR(\hww_j)$ with $j \geq i + 1$ is perpendicular to all of $\huu_1, ..., \huu_i$. This means that each $\RR(\hww_j)$ with $j \geq i + 1$ is in the orthogonal complement of $\spann((\huu_1, …, \huu_i))$.
    
    This orthogonal complement is equal to $\spann((\huu_{i + 1}, …, \huu_k)$. Therefore $\RR(\hww_{i + 1}), ..., \RR(\hww_k) \in \spann(\huu_{i + 1}, ..., \huu_k)$. Since $\RR$ preserves orthonormality (\textbf{again, see Theorem [...]}), then $\RR(\hww_{i + 1}), ..., \RR(\hww_k)$ are also orthonormal.
    
    Since $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1}, ..., \huu_k)$, then $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1} + c_{i + 2} \huu_{i + 2} + ... + c_k \huu_k$ for some scalars $c_{i + 1}, ..., c_k$. We claim that we must have $c_{i + 2} = ... = c_k = 0$; assume for contradiction that $c_{\ell_1}, ..., c_{\ell_p}$ are nonzero scalars with $\ell_1, ..., \ell_p > i + 1$. A quick inner product computation shows that $\RR(\hww_{i + 1})$ is not perpendicular to any of $\huu_{\ell_1}, ..., \huu_{\ell_p}$. Since $\RR(\hww_{i + 2}), ..., \RR(\hww_k)$ must all be perpendicular to $\RR(\hww_{i + 1})$, it follows that $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \notin \spann((\huu_{\ell_1}, ..., \huu_{\ell_p}))$. Thus $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \in \spann((\huu_{i + 2}, ..., \huu_k) - (\huu_{\ell_1}, ..., \huu_{\ell_p}))$, which is a subspace of dimension $|[i + 2, k] \cap \Z| - p$. On the other hand, since $(\RR(\hww_{i + 2}, ..., \RR(\hww_k))$ is an orthonormal basis, then $(\RR(\hww_{i + 2}), ..., \RR(\hww_k))$ span a $|[i + 2, k]|$-dimensional space. This is a contradiction; vectors cannot be in a space that has a lesser dimension than their span. Therefore, $c_{i + 2} = ... = c_k = 0$, and $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1}$, i.e., $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1})$. Since $\RR(\hww_{i + 1})$ is a unit vector, then $||\RR(\hww_{i + 1})|| = ||s_{i + 1} \huu_{i + 1}|| = |s_{i + 1}| = 1$. Thus $s_{i + 1} \in \{-1, 1\}$, and $\RR(\hww_{i + 1}) = s_{i + 1} \huu_{i + 1}$, as desired.

    \vspace{.25cm}

    Since $\RR(\hww_i) = s_i \huu_i$ for all $i$, where $s_i \in \{-1, 1\}$, then $\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k)$, where $s_1, ..., s_k \in \{-1, 1\}$. 

    If the number of $s_i$ equal to $-1$ is even, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = 1$, and if the number of $s_i$ equal to $-1$ is odd, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = -1$. (Recall Derivation     \ref{ch::lin_alg::thm::antisymmetry_ordered_bases_signs}). We see that in either case, ${\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for some $\sigma \in S_k$. This proves the theorem.
\end{proof}

\subsubsection*{Completing the definition of orientation for finite-dimensional inner product spaces}

\begin{defn}
    (Orientation of arbitrary ordered bases).
    
    Let $V$ be an $n$-dimensional inner product space, and fix an orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ for $V$. We know how to ``orient'' ordered bases for $V$ that happen to be permutations of $\hU$. Now, we generalize the notion of orientation so that it applies to any orthonormal ordered basis of $V$.
    
    We define the \textit{orientation of an orthonormal ordered basis $E$ of $V$} that is not a permutation of $\hU$ to be the orientation of the unique permuted orthonormal basis $\hU^\sigma$, $\sigma \in S^n$, of $\hU$ for which there exists an $n$-rotation taking $E$ to $\hU^\sigma$.
    
    Then, we define the \textit{orientation of an arbitrary orthonormal ordered basis $E$ of $V$} to be the orientation of the unique orthonormal basis $\hU_E$ obtained from performing the Gram-Schmidt algorithm on $E$ (see Theorem \ref{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}).
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::det_tracks_orientation}
    (The determinant tracks orientation). 
    
    Let $V$ be an $n$-dimensional inner product space with an orientation given by an orthonormal ordered basis $\hU$. Let $E = (\ee_1, ..., \ee_n)$ be any ordered basis (not necessarily orthonormal) of $V$. We have $\det([\EE]_{\hU}) > 0$ iff $E$ is positively oriented relative to $\hU$, and $\det([\EE]_{\hU}) < 0$ iff $E$ is negatively oriented relative to $\hU$.
\end{theorem}

\begin{proof}
   This proof has two overarching steps. First, we pass the definition of orientation for arbitrary ordered bases of $V$ to the definition of orthonormal ordered bases of $V$ by obtaining an orthonormal ordered basis $\hU_E$ from $E$. Then we pass the definition of orientation for orthonormal ordered bases of $V$ that are not permutations of $\hU$ to the definition of orientation for orthonormal ordered bases of $V$ that are permutations of $\hU$.
   
   To begin the first step, consider $\det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$, and perform Gram-Schmidt on $([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$. In the $i$th step of Gram-Schmidt, a linear combination of the vectors $[\ee_1]_{\hU}, ..., \cancel{[\ee_i]_{\hU}}, ..., [\ee_n]_{\hU}$ is added to $[\ee_i]_{\hU}$. In Gram-Schmidt, vectors are first linearly combined into each other in various ways. Recall from Theorem \ref{ch::lin_alg::thm::consequent_det_props} that the determinant is invariant under linearly combining input vectors into a different input vector. Therefore, this first part of the Gram-Schmidt algorithm does not change the determinant of the list of vectors. After this first part of Gram-Schmidt, the vectors are normalized. This step may change the determinant but still preserves its sign.

   Thus, if $\hU_E = (\hww_1, ..., \hww_n)$ is the orthonormal basis obtained by performing Gram-Schmidt on $E$, then 
   
   \begin{align*}
       \text{sign}(\det([\EE]_{\hU})) = \text{sign}(\det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})) = \text{sign}(\det([\hww_1]_{\hU}, ..., [\hww_n]_{\hU}))
       =
       \text{sign}(\det([\hU_E]_{\hU})).
   \end{align*}
   
   In performing this first step of the proof, sign of the determinant has stayed the same as we've passed from $E$ to $\hU_E$. We now show that the determinant continues to stay the same as we pass from  $\hU_E$ to some permutation $\hU^\sigma$ of $\hU$.
   
   Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis} says that there is a $n$-rotation $\RR$ taking $\hU_E$ to $\hU^\sigma$, for some $\sigma \in S_n$, and Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1} guarantees that $\det(\RR) = 1$. Thus, since $\hU^\sigma = \RR(\hU_E)$, we have
   
   \begin{align*}
        \det([\hU_E]_{\hU}) 
        = 1 \cdot \det([\hU_E]_{\hU})
        = \det(\RR) \det([\hU_E]_{\hU})
        = \det([\RR(\hU_E)]_{\hU})
        = \det([\hU^\sigma]_{\hU}).
   \end{align*}

   (The second to last equation is true because in general\footnote{Proof: $\det(\gg) \det([F]_H) = \det([\gg(F)]_H) \det([F]_H) = \det([\gg(F)]_H) \det([\II(F)]_H) = \det([(\gg \circ \II)(F)]_H) = \det([\gg(F)]_H)$.}, if $\gg:Y \rightarrow Y$ is linear and $F, H$ are bases for $Y$, then $\det(\gg) \det([F]_H) = \det([\gg(F)]_H)$.)
   
   To conclude the proof, we will show that $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$; once we have shown this, we are done, since $\sgn(\sigma) \det([\hU]_{\hU}) = \sgn(\sigma) \det(\II) = \sgn(\sigma)$. Since any permutation is a composition of transpositions, then $\hU^\sigma$ can be obtained from $\hU$ by repeatedly swapping vectors in $\hU$. Whenever vectors are swapped in the determinant, the sign of the determinant is multiplied by $-1$. This accounts for the $\sgn(\sigma)$ factor in the equation $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$.
\end{proof}

\subsection*{Orientation of finite-dimensional vector spaces}

\label{ch::lin_alg::orientation_finite_dim_vector_space}

The fact that the determinant tracks orientation is the main result of our discussion of orientation. Because determinants do not rely on the existence of an inner product, the determinant can be used to generalize the notion of orientation to any finite-dimensional vector space.

\begin{defn}
\label{ch::lin_alg::defn::orientation_finite_dim_vector_space}
    (Orientation of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space (not necessarily an inner product space). An \textit{orientation on $V$} is a choice of ordered basis $E$ for $V$. (Notice here that $E$ is not necessarily orthonormal, because $V$ might not have an inner product!). If we have given $V$ the orientation $E$, then we say that an ordered basis $F$ of $V$ is \textit{positively oriented (relative to $E$)} iff $\det([\FF]_E) > 0$, and that $F$ is \textit{negatively oriented (relative to $E$)} iff $\det([\FF]_E) < 0$.
    
    A finite-dimensional vector space that has an orientation is called an \textit{oriented (finite-dimensional) vector space}.
\end{defn}
 
\begin{remark}
    (Antisymmetry of orthonormal ordered bases).
    
    Notice that we still have the previous antisymmetry of orthonormal ordered bases due to the antisymmetry of the determinant.
\end{remark}

As a last sidenote, the following theorem gives some justification as to why our definition of $n$-rotation was a good definition. (Strictly speaking, though, the justification is somewhat circular, since we used the notion of $n$-rotations to explain how the determinant- which is involved in the justification- tracks orientation).

\begin{theorem}
    If $V$ is an $n$-dimensional inner product space, then 
    
    \begin{align*}
        \{\text{$n$-rotations on $V$}\} = \{\text{orthogonal linear functions $V \rightarrow V$ with determinant 1}\}
    \end{align*}
\end{theorem}

\begin{proof}
    \scriptsize Proof idea is from jagr2808. 
    \fontsize{10pt}{12pt}\selectfont \\
    \indent ($\subseteq$). This is just Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1}.
    
    \indent ($\supseteq$). If $\ff:V \rightarrow V$ is an orthogonal linear function, then $\ff$ sends an arbitrary orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ of $V$ to another orthonormal basis $\ff(\hU) = (\ff(\huu_1), ..., \ff(\huu_n))$.

    Let $\RR_1$ be the $2$-dimensional rotation sending $\ff(\huu_1)$ to $\huu_1$. Then $\RR_2 := \RR_1 \circ \ff:V \rightarrow V$ fixes $\huu_1$, so we can think of it as a function from $\spann(\huu_2, ..., \huu_n)$ to $\spann(\huu_2, ..., \huu_n)$. 
    
    We now use the above idea finitely many times. Define $\RR_{i + 1}$ to be the $2$-dimensional rotation sending ${(\RR_i \circ \RR_{i - 1})(\huu_i)}$ to $\huu_i$. We know that such a $2$-dimensional rotation always exists because $\det(\ff) = 1$ implies that $(\ff(\huu_1), ..., \ff(\huu_n))$ has the same orientation as $(\huu_1, ..., \huu_n)$. (Recall, having the same orientation involves ``rotational equivalence'' via $n$-rotations, which are compositions of $2$-dimensional rotations). By induction, $\RR_n \circ \ff$ is a composition of $2$-dimensional rotations.
    
    Thus, since $\ff = \RR_n^{-1} \circ (\RR_n \circ \ff)$, we see $\ff$ is a composition of the $2$-dimensional rotations $\RR_n^{-1}$ and $\RR_n^{-1} \circ \ff$.
    
    %If T is an orthogonal endomorphism then T maps the standard basis e_1, ..., e_n to an orthonormal basis u_1, ..., u_n.

    %Let R be the 2-rotation sending u_1 to e_1. Then RT is an endomorphism in SO(n) that fixes e_1, so we can think of it as an endomorphism in SO(n-1) on the orthogonal complement. By induction RT is the composition of 2-rotations. And T = R-1RT, so T is as well.
\end{proof}
