\chapter{Linear algebra}
\label{ch::lin_alg}

\begin{comment}
\textbf{This chapter uses some unconventional notation.} The unconventional notation will be introduced in a natural way, so there is no need to know any of it now. This is just a ``heads up'' for those who have experience with linear algebra. Here is the list of unconventional notation used in this chapter:

\begin{itemize}
    \item Unconventional combinations of symbols are used, such as:
    \begin{itemize}
        \item ``$\ff(E)$'' (this denotes a function acting on a list of vectors)
        \item ``$[\ff(E)]_F$'' (this is explicit notation for a matrix relative to bases) \item ``$\ff_{E,F}$'' (you'll have to find out!)
    \end{itemize}
    \item To notate the $i$th component of a vector $\vv$ relative to a basis $E$, we write $([\vv]_E)_i$ instead of $v_i$. This notation may seem unnecessarily verbose, but it has the advantage of making any involvement of bases apparent.
\end{itemize}


\vspace{.25cm}

\textbf{Notation for covariance and contravariance is not used in this chapter.} The use of both upper and lower indices to distinguish between ``covariant'' and ``contravariant'' will not be used in the following chapter of linear algebra review to prevent confusion. Only lower indices will be used. (If you don't know what ``covariant'' or ``contravariant'' means, that is 100\% expected. Covariance and contravariance are explained later).
\end{comment}

Linear algebra is the study of \textit{linear elements}- which are more commonly known as \textit{vectors}- and of functions that ``respect'' the algebraic structure of linear elements.

[TO-DO]

\newpage

\section{Vectors}

\begin{defn}
    (Intuitive definition of point).
    
    Intuitively, a \textit{point} is a location in space.
\end{defn}

\begin{defn}
    (Intuitive definition of vector).
    
    Intuitively, a \textit{vector} is a locationless directed line segment. 
    
    (By ``locationless'', we mean that two directed line segments are considered equal iff one of them can be moved- without changing the distance between its start and end point- so that it coincides with the other).
\end{defn}

It is tempting to try to define either \textit{point} in terms of \textit{vector} or \textit{vector} in terms of \textit{point}, but this is actually impossible.

\begin{deriv}
    (Circularity in intuitive definitions of point and directed line segment).
        
    \begin{itemize}
        \item (Attempted definition of \textit{vector} in terms of \textit{point}). 
        
        Suppose that we try to define \textit{vector} in terms of \textit{point} as follows: a \textit{vector} is a difference of the form $\pp_2 - \pp_1$, where $\pp_1$ and $\pp_2$ are points. This definition doesn't work because any intuitively interpretable definition of ``difference of points'' is ``difference of the corresponding\footnote{At this point it is unclear if a correspondence between vectors and points even exists. If we assume there is such a correspondence, though, we see the argument is circular.} vectors''; which is clearly a circular definition because it involves \textit{vector}.
        
        \item (Attempted definition of \textit{point} in terms of \textit{vector}). 
        
        Suppose that we use the above definition of \textit{vector} and attempt to use it to give a definition of \textit{point} that is equivalent to the above definition of \textit{point}. The attempted definition is: a \textit{point} is a sum of the form $(((\qq + \vv_1) + \vv_2) + ...) + \vv_n$, where $\qq$ is any point, each $\vv_i$ is a vector, and where the sum of a point and a vector is the point obtained by starting on the point and ``traveling'' according to the vector. This definition is circular because it involves \textit{point}.
    \end{itemize}
    
    In addition to the logical circularities, also notice that these attempted definitions suggest that points and vectors are inextricably intertwined: the first attempted definition requires a correspondence between vectors and points, and the second attempted definition requires a notion of adding a point with a vector.
\end{deriv}

The above result forces us to define \textit{point} and \textit{vector} independently of each other; we have to define \textit{point} in terms of some more primitive mathematical object, and \textit{vector} in terms of another more primitive mathematical object. (We will actually use the same primitive mathematical object for both \textit{point} and \textit{vector}). Notice that even after we have done this, the above detailed intertwinedness and logical circularities will always persist on the intuitive level, for better or for worse.

\newpage

\subsection*{$\R^n$}

We will use elements of $\R^n$ as the primitive mathematical objects that underlie the notion of \textit{point} and \textit{vector}.

\begin{defn}
    ($\R^n$).
    
    Let $n$ be a positive integer. Recall that if $S$ is any set, then $S^n$ denotes the set of length-$n$ tuples with entries from $S$:
    
    \begin{align*}
        S^n := \underbrace{S \times ... \times S}_{\text{$n$ times}} = \{(x_1, ..., x_n) \mid x_1, ..., x_n \in S\}.
    \end{align*}
    
    In particular, $\R^n$ is the set of length-$n$ tuples whose entries are real numbers: 
    
    \begin{align*}
        \R^n = \underbrace{\R \times ... \times \R}_{\text{$n$ times}} = \{(x_1, ..., x_n) \mid x_1, ..., x_n \in \R\}.
    \end{align*}
\end{defn}

\begin{defn}
    (Column notation for elements of $\R^n$).
    
    To save horizontal writing space, we will use the following \textit{column notation} to denote elements of $\R^n$, and write $\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$ rather than $(x_1, ..., x_n) \in \R^n$. Thus we have
    
    \begin{align*}
        \R^n = \underbrace{\R \times ... \times \R}_{\text{$n$ times}} = \Bigg\{ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \mid x_1, ..., x_n \in \R \Bigg\}.
    \end{align*}
    
    When we wish to refer to an element of $\R^n$ without explicitly specifying what its entries are, we use a bold letter, and say something like ``let $\xx \in \R^n$''.
\end{defn}

Now that we have defined $\R^n$ and introduced notation to represent elements of $\R^n$, we define an \textit{addition} operation $+:\R^n \times \R^n \rightarrow \R^n$ and a \textit{scalar multiplication} operation $\cdot: \R \times \R^n \rightarrow \R^n$, and investigate the algebraic properties of these operations.

\begin{defn}
    (Addition and scalar multiplication in $\R^n$).
    
    We define \textit{addition} $+:\R^n \times \R^n \rightarrow \R^n$ and \textit{scalar multiplication} $\cdot:\R \times \R^n \rightarrow \R^n$ operations as follows.
    
    As a preliminary, we note that for $\xx_1, \xx_2 \in \R^n$ we use the notation $\xx_1 + \xx_2 := +(\xx_1, \xx_2)$, and that for $c \in \R$ and $\xx \in \R^n$ we use the notation $c \xx := \cdot(c, \xx)$.
    
    We define
    
    \begin{align*}
        \underbrace
        {
            \begin{pmatrix}
                x_1 \\ \vdots \\ x_n
            \end{pmatrix}
        }_\xx
        +
        \underbrace
        {
            \begin{pmatrix}
                y_1 \\ \vdots \\ y_n
            \end{pmatrix}
        }_\yy
        &:=
        \underbrace
        {
            \begin{pmatrix}
                x_1 + y_1 \\ \vdots \\ x_n + y_n
            \end{pmatrix}
        }_{\xx + \yy}
        \\
        c
        \underbrace
        {
            \begin{pmatrix}
                x_1 \\ \vdots \\ x_n
            \end{pmatrix}
        }_\xx
        &:=
        \underbrace
        {
            \begin{pmatrix}
                c x_1 \\ \vdots \\ c x_n
            \end{pmatrix}
        }_{c \xx}.
    \end{align*}
\end{defn}

The above addition operation $+$ and scalar multiplication operation $\cdot$ are algebraically sensible because they obey properties one would expect of an ``addition operation'' and a ``multiplication operation''. The following theorem shows this.

\newpage

\begin{theorem}
\label{ch::lin_alg::thm::prop_operations_Rn}
    (Properties of $+$ and $\cdot$ in $\R^n$).
        \begin{enumerate}
            \item (Properties of $+$).
            \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in \R^n$ such that for all $\xx \in \R^n$, $\mathbf{0} + \xx = \xx$. Specifically, we have
        
                \begin{align*}
                    \mathbf{0} = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}.
                \end{align*}
                
                \item[1.2.] (Closure under additive inverses). For all $\xx \in \R^n$ there exists $-\xx \in \R^n$ such that $\xx + (-\xx) = \mathbf{0}$. Specifically, we have
        
                \begin{align*}
                    -\begin{pmatrix}
                        x_1 \\ \vdots \\ x_n
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        -x_1 \\ \vdots \\ -x_n
                    \end{pmatrix}.
                \end{align*}
        
                \item[1.3.] (Associativity of $+$). For all $\xx_1, \xx_2, \xx_3 \in \R^n$ we have $(\xx_1 + \xx_2) + \xx_3 = \xx_1 + (\xx_2 + \xx_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\xx_1, \xx_2 \in \R^n$ we have $\xx_1 + \xx_2 = \xx_2 + \xx_1$.
            \end{enumerate}
            \item (Properties of $\cdot$).
            \begin{enumerate}
                \item[2.1.] (Normalization for $\cdot$). For all $\xx \in \R^n$ we have $1 \xx = \xx$.
                \item[2.2.] (Left-associativity of $\cdot$). For all $\xx \in \R^n$ and $c_1, c_2 \in \R$ we have $c_2 (c_1 \xx) = c_2 c_1 \xx$.
            \end{enumerate}
            \item (Compatibility of $+$ and $\cdot$).
            \begin{enumerate}
                \item[3.1.] For all $\xx_1, \xx_2 \in \R^n$ and $c \in \R$ we have $c(\xx_1 + \xx_2) = c \xx_1 + c \xx_2$.
                \item[3.2.] For all $\xx \in \R^n$ and $c_1, c_2 \in \R$ we have $(c_1 + c_2)\xx = c_1 \xx + c_2 \xx$.
            \end{enumerate}
        \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
        \item The claims about the additive identity and additive inverses follow from the fact that $0$ is the additive identity in $\R$ and that the additive inverse of $x \in \R$ is denoted by $-x$. The associativity and commutativity of $+$ follow from the associativity and commutativity of addition of real numbers.
        \item The normalization property of $\cdot$ follows from the fact that $1$ is the multiplicative identity in $\R$. The left-associativity of $\cdot$ follows from the associativity of real numbers.
        \item Both compatibility properties follow from the facts that we have $c (x_1 + x_2) = c x_1 + x_2$ for all $c, x_1, x_2 \in \R$ and $(c_1 + c_2) x = c_1 x + c_2 x$ for all $c_1, c_2, x \in \R$. In other words, they follow from the fact that $\cdot$ distributes over $+$ in $\R$ and from the fact that $\cdot$ is commutative in $\R$.
    \end{enumerate} 
\end{proof}

\newpage

\subsection*{Points and vectors in $\R^n$}

Now we define \textit{point} and \textit{vector} in terms of elements of $\R^n$.

\begin{defn}
    (Points and vectors in $\R^n$).
    
    A \textit{point} in $\R^n$ is an element of $\R^n$. A \textit{vector} in $\R^n$ is also an element of $\R^n$. (That was pretty simple, wasn't it?)
    
    When we have $\xx = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$, we can interpret $\xx$ to be either a point or vector. The numbers $x_1, ..., x_n \in \R$ are called the \textit{entries}, or \textit{components}, of $\xx$.
    
    If we interpret $\xx$ to be a point, then each $x_i$ is the point's displacement on the $x_i$ axis. If we interpret $\xx$ to be a vector, then each $x_i$ is the displacement, on the $x_i$ axis, of the end of $\xx$ relative to the start of $\xx$, where- since vectors are locationless- we additionally allow the start of $\xx$ to be any point.
\end{defn}

\begin{remark}
    (Intertwinedness of points and vectors).
    
    The above definition easily implements the intertwinedness of points and vectors; since every point in $\R^n$ can also be interpreted as a vector in $\R^n$, the correspondence between points and vectors is given by the identity function.
\end{remark}

Even though there is always some circularity on an intuitive level between the concepts of \textit{point} and \textit{vector}; the setting of $\R^n$ grounds the following geometric intuitions.

\begin{remark}
\label{ch::lin_alg::rmk::geometric_intuition_points_vectors}
    (Geometric intuition about points and vectors).
        
    (Geometric intuition about $+$).
    \begin{itemize}
        \item (Point plus vector). Let $\pp \in \R^n$ be a point and $\vv \in \R^n$ be a vector. We interpret $\pp + \vv$ to be the point that is obtained by placing the start of $\vv$ on $\pp$ and then ``traveling'' according to $\vv$.
        \item (Vector plus vector). Let $\vv_1, \vv_2 \in \R^n$ be vectors. We interpret $\vv_1 + \vv_2$ to be the result of starting at the start point of $\vv_1$ (which of course can be any point), then traveling according to the vector $\vv_1$ to reach the end point of $\vv_1$, then applying ``locationlessness'' to place the start point of $\vv_2$ on the end point of $\vv_1$, and then traveling to the end point of $\vv_2$. That is, the directed line segment that reaches from the ``overall'' beginning to the ``overall'' end is $\vv_1 + \vv_2$.
        \begin{itemize}
            \item If we use locationlessness of to make the end point of $\vv_1$ coincide with the start point of $\vv_2$ before we start the above process, then $\vv_1 + \vv_2$ is represented by the directed line segment that connects the start point of $\vv_1$ to the end point of $\vv_2$. This interpretation is often called the ``tail-to-tip method'', since one computes $\vv_1 + \vv_2$ by forming the vector by drawing a straight line from the “tail” of $\vv_1$ to the “tip” of $\vv_2$.
        \end{itemize}
        \item ($\mathbf{0}$ as a point). When $\mathbf{0}$ is considered to be a point, we interpret it to be the ``central reference point'' of the coordinate system, i.e., the origin.
        \item ($\mathbf{0}$ as a vector). When $\mathbf{0}$ is interpreted to be a vector, we interpret it to be the locationless directed line segment that has a length of zero.
        \item (Additive inverses as points). When $\pp \in \R^n$ is a point, there is no satisfactory interpretation of $-\pp$.
        \item (Additive inverses as vectors). When $\vv \in \R^n$ is a vector, the fact $\vv + (-\vv) = \mathbf{0}$ forces us to interpret $-\vv$ to be the unique vector whose end is the start of $\vv$ if its start is made to coeinide with the end of $\vv$.
    \end{itemize}

    (Geometric intuition about $\cdot$).
    \begin{itemize}
        \item (Scalar times point). Let $c \in \R$. When $\pp \in \R^n$ is a point, there is no satisfactory interpretation of $c \pp$.
        \item (Scalar times vector). Let $c > 0$. When $\vv \in \R^n$ is a vector, we interpret $c \vv$ to be the vector that is obtained by stretching $\vv$ by a factor of $c$ (when $c > 1$) or that is obtained by shrinking $\vv$ by a factor of $c$ (when $c \in (0, 1)$). When $d < 0$, we interpret $d\vv$ to be $-(|d|\vv)$; that is, we interpret $d\vv$ to be the additive inverse of $|d|\vv$.
    \end{itemize}
\end{remark}

\begin{defn}
    (``Vector'' means ``vector'' or ``point'' depending on context).
    
    In practice, we say ``vector in $\R^n$'' to mean ``element of $\R^n$'', and determine whether $\vv \in \R^n$ is interpreted to be a point or a locationless directed line segment from context.
\end{defn}

\newpage

\subsection*{Vector-containing sets}

At this point, we know that we can make the intuitive notions of ``vector'' and ``point'' precise by grounding them in the concrete setting of $\R^n$. This works, of course, but thinking about of vectors in $\R^n$ has a good deal of unnecessary ``implementation detail''. When thinking about a single vector in $\R^n$, one is secretly thinking about the several numbers that are the vector's components!

Thankfully, the previous theorem (Theorem \ref{ch::lin_alg::thm::prop_operations_Rn}) provides an opportunity to escape dealing with components. The theorem states key properties of vectors in $\R^n$ (such as ``there exists $\mathbf{0} \in \R^n$ such that for all $\xx \in \R^n$ we have $\xx + \mathbf{0} = \xx$'', and ``for all $\xx_1, \xx_2 \in \R^n$ and $c \in \R$ we have $c(\xx_1 + \xx_2) = c\xx_1 + c\xx_2$'') in terms of the vectors themselves, and \textit{not} in terms of their components!

The properties of this helpful theorem are in general the properties that we expect of vectors. Thus, we can leave behind the baggage of $\R^n$ by effectively defining vectors to be objects that have the properties of the theorem. We do this now.

\begin{defn}
\label{ch::lin_alg::defn::vector_containing_set_R}
    (Vector-containing set over $\R$).
    
    Suppose that $T$ is a set for which there exist functions $+:T \times T \rightarrow T$ and $\cdot:\R \times T \rightarrow T$. We say that a set $S \subseteq T$ is a \textit{vector-containing set over $\R$} iff the operations $+$ and $\cdot$ satisfy the typical\footnote{When you try to remember these properties, there is no need to be incredibly specific. You don't need to list out ``existence of additive identity'', ``closure under additive inverses'', and so on \textit{every} time you remind yourself of what a vector-containing set over $\R$ is!} properties one would expect of ``vector addition'' and ``vector scaling''. That is, $S$ is a vector-containing set over $\R$ iff $+$ and $\cdot$ satisfy the properties\footnote{You may be confused as to why $T$, rather than $S$, is involved in all of the above conditions when we are defining what it means for $S$ to be a vector-containing set. This is because we want $S$ to have \textit{most} of the properties that $T$ does, but not all of them. Specifically: even though the above conditions require $\mathbf{0} \in T$ and $(\forall \vv \spc \vv \in T \iff -\vv \in T)$, they do not require $\mathbf{0} \in S$ or $(\forall \vv \spc \vv \in S \iff -\vv \in S)$.} listed in Theorem \ref{ch::lin_alg::thm::prop_operations_Rn}:
    
    \begin{enumerate}
        \item (Properties of $+$).
        \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in T$ such that for all $\vv \in T$ we have $\vv + \mathbf{0} = \vv$.
                \item[1.2.] (Closure under additive inverses). For all $\vv \in T$ there exists $-\vv \in T$ such that $\vv + (-\vv) = \mathbf{0}$.
                \item[1.3.] (Associativity of $+$). For all $\vv_1, \vv_2, \vv_3 \in T$ we have $(\vv_1 + \vv_2) + \vv_3 = \vv_1 + (\vv_2 + \vv_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\vv_1, \vv_2 \in T$ we have $\vv_1 + \vv_2 = \vv_2 + \vv_1$.
            \end{enumerate}
        \item (Properties of $\cdot$).
        \begin{enumerate}
            \item[2.2.] (Normalization for $\cdot$). For all $\vv \in T$ we have $1 \vv = \vv$.
            \item[2.1.] (Left-associativity of $\cdot$). For all $\vv \in T$ and $c_1, c_2 \in \R$ we have $c_2 (c_1 \vv) = c_2 c_1 \vv$.
        \end{enumerate}
        \item (Properties of $+$ and $\cdot$).
        \begin{enumerate}
            \item[3.1.] For all $\vv_1, \vv_2 \in T$ and $c \in \R$ we have $c(\vv_1 + \vv_2) = c \vv_1 + c \vv_2$.
            \item[3.2.] For all $\vv \in T$ and $c_1, c_2 \in \R$ we have $(c_1 + c_2)\vv = c_1 \vv + c_2 \vv$.
        \end{enumerate}
    \end{enumerate} 
    
    When $S$ is a vector-containing set over $\R$, we refer to elements of $S$ as \textit{vectors}. As before, elements of $\R$ are sometimes called \textit{scalars}.
\end{defn}

As we discussed before, this abstract characterization of a vector-containing set is useful because it decouples our notion of ``vector'' from the setting of $\R^n$. It allows us to see that many types of objects- which we might have overlooked before- can be interpreted to be vectors.

\begin{remark}
    (Examples of vector-containing sets over $\R$).
    
    Here are some examples of vector-containing sets over $\R$:
    
    \begin{itemize}
        \item $\R^n$, for any positive integer $n$, where $+$ and $\cdot$ are as usual (this one isn't intended to be surprising)
        \item $\Big\{\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} -3 \\ 0 \end{pmatrix}\Big\} \subseteq \R^2$, where $+$ and $\cdot$ are the same as the functions $+$ and $\cdot$ on $\R^2$
        \item $\{\vv_1, ..., \vv_k\} \subseteq S$, where $S$ is another vector-containing set, and $+$ and $\cdot$ are the same as the functions $+$ and $\cdot$ on $S$
        \item The set of polynomials with real coefficients of degree less than $n$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number
        \item The set of infinitely differentiable functions $\R \rightarrow \R$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number
        \item The set of infinite sequences of real numbers, where $+$ and $\cdot$ are defined in the ways you would expect
    \end{itemize}
\end{remark}

Now that we have defined what a ``vector-containing set over $\R$'' is, it is reasonable to introduce a little more abstraction. (Don't worry, though. This abstraction is much easier to grasp than the leap from ``vectors in $\R^n$'' to ``elements of a vector-containing set''). In place of ``vector-containing sets over $\R$'', we will now consider ``vector-containing sets over $K$'', where $K$ is the set-with-structure that contains the scalars.

In other words, in the same way that a vector is ``something that lives in a vector-containing set'', we will define a scalar to be ``something that lives in a `scalar space' ''.  Unfortunately, the terminology ``scalar space'' is nonstandard;  ``scalar spaces'' are actually called \textit{fields}.

\begin{defn}
    (Field). 
    
    Suppose that $K$ is a set for which there exist functions $+:K \times K \rightarrow K$ and $\cdot:K \times K \rightarrow K$. We say that the tuple $(K, +, \cdot)$ is a \textit{field} iff it satisfies\footnote{In the terminology of abstract algebra, a field can be defined to be (1) an ``integral domain that is closed under multiplicative inverses'' or (2) as a ``commutative division ring''.} the following:

    \begin{enumerate}
        \item $K$ is a ``commutative group under addition''. This means that conditions 1.1 through 1.5 must hold.
        \begin{enumerate}
            \item[1.1.] (Closure under $+$). For all $c_1, c_2 \in K$ we have $c_1 + c_2 \in K$.
            \item[1.2.] (Existence of additive identity). There exists $0 \in K$ such that for all $c \in K$ we have $0 + c = c$.
            \item[1.3.] (Associativity of $+$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 + c_2) + c_3 = c_1 + (c_2 + c_3)$.
            \item[1.4.] (Closure under additive inverses). For all $c \in K$ there exists $-c \in K$ such that $(-c) + c = 0$.
            \item[1.5.] (Commutativity of $+$). For all $c_1, c_2 \in K$ we have $c_1 + c_2 = c_2 + c_1$.
        \end{enumerate}
        \item $K$ is a ``commutative group under multiplication''. This means that conditions 2.1 through 2.5 must hold.
        \begin{enumerate}
            \item[2.1.] (Closure under $\cdot$). For all $c_1, c_2 \in K$ we have $c_1 \cdot c_2 \in K$.
            \item[2.2.] (Existence of multiplicative identity). There exists $1 \in K$ such that for all $c \in K$ we have $1 \cdot c = c$.
            \item[2.3.] (Associativity of $\cdot$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 \cdot c_2) \cdot c_3 = c_1 \cdot (c_2 \cdot c_3)$.
            \item[2.4.] (Closure under multiplicative inverses). For all $k \in K$ with $k \neq 0$, there exists $\frac{1}{c} \in K$ such that $\frac{1}{c} \cdot c = 1$.
            \item[2.5.] (Commutativity of $\cdot$). For all $c_1, c_2 \in K$ we have $c_1 \cdot c_2 = c_2 \cdot c_1$.
        \end{enumerate}
        \item ($\cdot$ distributes over $+$). For all $c_1, c_2, c_3 \in K$ we have $(c_1 + c_2) \cdot c_3 = c_1 \cdot c_3 + c_2 \cdot c_3$.
    \end{enumerate} 

    Colloquially, we often say ``let $K$ be a field'' instead of ``let $(K, +, \cdot)$ be a field''.
    
    Recall that the whole point of defining a field is to formalize the notion of what a ``scalar'' is. For this reason, elements of a field are called \textit{scalars}.
\end{defn}

\begin{remark}
    (Examples and non-examples of fields).
    
    \begin{itemize}
        \item The integers $\Z$ are \textit{not} a field because the multiplicative inverse of any $x \neq 1$ is not an integer.
        \item The rational numbers $\Q = \{\frac{a}{b} \mid a, b \in \Z\}$ is a field.
        \item $\R$ is a field.
        \item The complex numbers $\C = \{ a + b\sqrt{-1} \mid a, b \in \R\}$ are a field.
    \end{itemize}
\end{remark}

\begin{remark}
    (Don't memorize the definition of a field!).
    
    It's not necessary to memorize all the conditions for a field. Just remember that a field is ``a set that contains elements which one can add, subtract, multiply, and divide''. In this book, you can imagine an arbitrary field $K$ as being $\R$ almost all of the time\footnote{It's true that there is more to it when the field is a \textit{finite field}, but this is not a major concern in this book.}.
\end{remark}

Without further ado, we define ``vector-containing set over a field''.

\begin{defn}
\label{ch::lin_alg::defn::vector_space}
    (Vector-containing set over a field).
    
    Suppose that $K$ is a field, that $T$ is a set, and that there exist functions $+:T \times T \rightarrow T$ and $\cdot:K \times T \rightarrow T$. If $S \subseteq T$ is a set, we say that the tuple $(S, K, +, \cdot)$ is a \textit{vector-containing set}, or, more colloquially, that ``$S$ is a vector-containing set over $K$'', iff:
    
    \begin{enumerate}
        \item (Properties of $+$).
        \begin{enumerate}
                \item[1.1.] (Existence of additive identity). There exists $\mathbf{0} \in T$ such that for all $\vv \in T$ we have $\vv + \mathbf{0} = \vv$.
                \item[1.2.] (Closure under additive inverses). For all $\vv \in T$ there exists $-\vv \in T$ such that $\vv + (-\vv) = \mathbf{0}$.
                \item[1.3.] (Associativity of $+$). For all $\vv_1, \vv_2, \vv_3 \in T$ we have $(\vv_1 + \vv_2) + \vv_3 = \vv_1 + (\vv_2 + \vv_3)$.
                \item[1.4.] (Commutativity of $+$). For all $\vv_1, \vv_2 \in T$ we have $\vv_1 + \vv_2 = \vv_2 + \vv_1$.
            \end{enumerate}
        \item (Properties of $\cdot$).
        \begin{enumerate}
            \item[2.2.] (Normalization for $\cdot$). For all $\vv \in T$ we have $1 \vv = \vv$.
            \item[2.1.] (Left-associativity of $\cdot$). For all $\vv \in T$ and $c_1, c_2 \in K$ we have $c_2 (c_1 \vv) = c_2 c_1 \vv$.
        \end{enumerate}
        \item (Properties of $+$ and $\cdot$).
        \begin{enumerate}
            \item[3.1.] For all $\vv_1, \vv_2 \in T$ and $c \in K$ we have $c(\vv_1 + \vv_2) = c \vv_1 + c \vv_2$.
            \item[3.2.] For all $\vv \in T$ and $c_1, c_2 \in K$ we have $(c_1 + c_2)\vv = c_1 \vv + c_2 \vv$.
        \end{enumerate}
    \end{enumerate}

    (If you are confused as to why $T$, rather than $S$, is involved in all of the above conditions, see the footnote above the conditions in Definition \ref{ch::lin_alg::defn::vector_containing_set_R}).
    
    Elements of vector-containing sets are called ``vectors''.
    
    In practice, we often don't explicitly mention a field, and say ``let $S$ be a vector-containing sets'' instead of ``let $S$ be a vector-containing set over a field $K$''.
\end{defn}

\subsection*{Vector spaces}

Now that our notion of vector-containing set is complete, we can make use of vector-containing sets. 

Most often, we interpret vectors to be points. In the setting of the vector-containing set $\R^n$, it is easy to describe a point: you simply list the displacements $x_1, ..., x_n \in \R$ of the point from each of the mutually perpendicular coordinate axes (we discussed this in Remark \ref{ch::lin_alg::rmk::geometric_intuition_points_vectors}). We can't describe a vector from a general vector space this way, though, because the vectors are not necessarily elements of $\R^n$! We can do something similar, though. Just as $\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in \R^n$ is a weighted sum of the vectors $\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, ..., \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} \in \R^n$, we can consider weighted sums of vectors from vector-containing sets. The following definition gives a name for vectors that are the result of such weighted sums.

\begin{defn}
    (Linear combination).
    
    Let $S = \{\vv_1, ..., \vv_k\}$ be a finite vector-containing set over a field $K$. We define a \textit{linear combination of the vectors in $S$}, or, more colloquially, a \textit{linear combination of $\vv_1, ..., \vv_k$}, to be a vector of the form
        
    \begin{align*}
        c_1 \vv_1 + ... + c_k \vv_k,
    \end{align*}
    
    where $c_1, ..., c_k$ are some scalars in $K$. So, a linear combination of $\vv_1, ..., \vv_k$ is a ``weighted sum'' involving $\vv_1, ..., \vv_k$.
\end{defn}

As we continue to investigate vectors, we will primarily be interested in vector-containing sets that are closed under the operation of taking linear combinations. The next two definitions formalize this idea. In the second definition, we see that such vector-containing sets are called \textit{vector spaces}.

\begin{defn}
    (Span).
    
    Let $S = \{\vv_1, ..., \vv_k\}$ be a finite vector-containing set over a field $K$. We define the \textit{span} of $S$, or, more colloquially, the \textit{span of $\vv_1, ..., \vv_k$}, to be the set of all linear combinations of $\vv_1, ..., \vv_k$:
    
     \begin{align*}
        \spann(S) = \spann(\{\vv_1, ..., \vv_k\}) := \Big\{ c_1 \vv_1 + ... + c_k \vv_k \mid c_1, ..., c_k \in K \Big\}.
    \end{align*}
    
    For geometric intuition, notice that if $\vv_1, ..., \vv_k \in \R^n$, then $\spann(\{\vv_1, ..., \vv_k\})$ is the $k$-dimensional plane spanned by $\vv_1, ..., \vv_k$ that is embedded in $\R^n$. For example, if $\vv, \ww \in \R^3$, then ${\spann(\{\vv, \ww\}) = \{ c \vv + d \ww \mid c, d \in \R \}}$ is the $2$-dimensional plane spanned by $\vv$ and $\ww$ in $\R^3$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::vector_space}
    (Vector space over a field).
    
    Suppose that $K$ is a field, that $V$ is a set, and that there exist functions $+:V \times V \rightarrow V$ and $\cdot:K \times V \rightarrow V$. We say that the tuple $(V, K, +, \cdot)$ is a \textit{vector space}, or, more colloquially, that ``$V$ is a vector space over $K$'', iff 

    \begin{enumerate}
        \item $(V, K, +, \cdot)$ is a vector-containing set.
        \item $V$ is spanned by some vector-containing set, i.e., there is a vector-containing set $S$ for which $V = \spann(S)$. 
    \end{enumerate}
    
    Elements of vector spaces are called ``vectors''.
    
    In practice, we often don't explicitly mention a field, and say ``let $V$ be a vector space'' instead of ``let $V$ be a vector space over a field $K$''.
\end{defn}

\begin{remark}
    (Vector-containing sets are subsets of vector spaces).

    Vector-containing sets and subsets of vector spaces are one and the same: vector-containing sets are subsets of vector spaces, and subsets of vector spaces are vector-containing sets. From this point on, we will favor ``subset of a vector space'' over ``vector-containing set'', because the terminology ``vector-containing set'' is nonstandard.
\end{remark}

Most of the examples of vector-containing sets over $\R$ we gave before are actually vector spaces.

\begin{remark}
    (Examples of vector spaces).
    
    Here are some examples of vector spaces:
    
    \begin{itemize}
        \item $\R^n$ over $\R$
        \item $K^n$ over $K$, where $K$ is a field
        \item The set of polynomials with coefficients in a field $K$ of degree less than $n$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number
        \item The set of infinitely differentiable functions $\R \rightarrow \R$, where $+$ is function addition and $\cdot$ is the scaling of a function by a real number.
        \item The set of infinite sequences of real numbers, where $+$ and $\cdot$ are defined in the ways you would expect.
    \end{itemize}
    
    Here are some examples of vector-containing sets that are \textit{not} vector spaces:
    
    \begin{itemize}
        \item $\Big\{\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} -3 \\ 0 \end{pmatrix}\Big\} \subseteq \R^2$.
        \item $\{\vv_1, ..., \vv_k\} \subseteq S$, where $S$ is a vector-containing set
        \item $\{ z \in \R \mid z \geq ax + by \} \subseteq \R^3$, where $a, b \in \R$.
        \item $\{ x_n \in \R \mid x_n \geq c_1 x_1 + ... + c_k x_k \} \subseteq \R^n$, where $c_1, ..., c_k \in \R$, $k \leq n - 1$.
    \end{itemize}
\end{remark}

\begin{remark}
    ($\emptyset$ is not a vector space).
    
    The empty set $\emptyset$ is not a vector space over any field because it is not equal to the span of any set. (A set is either empty or not. The span of the empty set is $\{0\}$, and the span of a nonempty set is nonempty. Thus no set has span equal to the empty set).
\end{remark}

\begin{remark}
    (Why is the generality of ``vector space'' useful?)
    
    In this book, we will often consider an arbitrary vector space and use it as a ``base'' from which to construct more vector spaces, while secretly imagining the ``base'' vector space as being $\R^n$. Dealing in the abstraction of vector spaces makes it easier to see precisely how vector spaces produced from $\R^n$ are related to $\R^n$ by removing any doubt as to whether coordinates, which are easy to run into when one deals with $\R^n$ explicitly, are fundamentally at play.
\end{remark}

Now that we have defined what a vector space is in full generality, we present an alternative characterization of vector spaces that is easier in practice to check than the above definition. 

\begin{deriv}
    (Vector spaces are closed under addition and scalar multiplication).
    
    Suppose $V$ is a vector space over a field $K$. Working heuristically, we can discover that the spanning condition satisfied by vector spaces is equivalent to another condition:
    
    \begin{align*}
        &\text{There is a vector-containing set $S$ for which $V = \spann(S)$.} \\
        &\quad \quad \quad \quad \quad \quad \iff \\
        &\text{For all $\vv_1, \vv_2 \in V$ we have $\vv_1 + \vv_2 \in V$ (``$V$ is closed under vector addition'')} \\
        &\quad \quad \quad \quad \quad \quad \spc \spc \text{and} \\
        &\text{for all $c \in K$ and $\vv \in V$ we have $c \vv \in V$ (``$V$ is closed under vector scaling'')}.
    \end{align*}
    
    The idea for showing that the hypothesis implies the first part of the conclusion (the part before the ``and'') is this: if we assume that $V$ is spanned by some vector-containing set $S$, then we can compute $\vv_1 + \vv_2$ by writing $\vv_1$ and $\vv_2$ as ``weighted sums'' of elements from $S$, and the result, being a (longer) weighted sum of elements from $S$, is thus in $V$. Similar reasoning shows the second part of the conclusion. The reverse implication is simple: there is always a vector-containing set spanning $V$, since every vector-containing set spans itself.
\end{deriv}

When determining whether a set $V$ is a vector space or not, it is almost always easier to check the above closure conditions than to check if there is a set that spans $V$. Thus, the following characterization of vector spaces is almost always what one should use when testing if a set is a vector space.

\begin{theorem}
\label{ch::lin_alg::thm::vector_space_alt_characterization}
    (Alternate characterization of vector spaces).
    
    Suppose that $K$ is a field, that $V$ is a set, and that there exist functions $+:V \times V \rightarrow V$ and $\cdot:K \times V \rightarrow V$. The tuple $(V, K, +, \cdot)$ is a vector space iff
    
    \begin{enumerate}
        \item $(V, K, +, \cdot)$ is a vector-containing set.
        \item ``$V$ is closed under vector addition and vector scaling''.
        \begin{enumerate}
            \item[2.1.] (Closure under $+$). For all $\vv_1, \vv_2 \in V$, $\vv_1 + \vv_2 \in V$.
            \item[2.2.] (Closure under $\cdot$). For all $c \in \R$ and $\vv \in V$, $c \vv \in V$.
        \end{enumerate}
    \end{enumerate}
\end{theorem}

We know that if $V$ is a vector space, then $V$ is a vector-containing set, and thus that there is a set $T \supseteq V$ that contains zero and the additive inverse of every vector in $V$. We now see that the fact that vector spaces are spanned by vector-containing sets implies these important vectors are members of all vector spaces.

\begin{theorem}
    (Vector spaces have zero and additive inverses).

    Let $V$ be a vector space. Then for any $\vv \in V$,
    
    \begin{enumerate}
        \item The zero vector is $\mathbf{0} = 0\vv$.
        \item The additive inverse of $\vv$ is $-\vv = (-1)\vv$.
    \end{enumerate}

    Because $V$ is closed under vector scaling, both of these vectors are in $V$.
\end{theorem}

\begin{proof}
    We've already explained that if the given formulas for the zero vector and additive inverses are true, then $V$ contains the zero vector and all additive inverses. We now show that the formulas are true.

    \begin{enumerate}
        \item Let $\vv, \ww \in V$. We want to show that $0\vv + \ww = \ww$. We have $0\vv + \ww = 0\vv + \vv + (\ww - \vv) = (0 + 1)\vv + \ww - \vv = \vv + \ww - \vv = \ww$, as desired.
        \item For all $\vv \in V$ we have $\vv + (-1)\vv = 1\vv + (-1)\vv = (1 - 1)\vv = 0\vv = \mathbf{0}$.
    \end{enumerate}
\end{proof}

\subsection*{Vector subspaces}

Some vector spaces are subsets of vector spaces. We quickly discuss such spaces.

\begin{defn}
\label{ch::lin_alg::defn::vector_subspace}
    (Vector subspace). 
    
    If $V$ and $W$ are vector spaces and $W \subseteq V$, then $W$ is a \textit{vector subspace} of $V$.
\end{defn}

Since every vector subspace is a subset of a vector space, then every vector subspace is a vector-containing set. This fact yields a simple characterization of vector subspaces.

\begin{theorem}
\label{ch::lin_alg::defn::vector_subspace_characterization}
    (Vector subspace characterization).

    If $W$ is a subset of a vector space $V$, then $W$ is a vector subspace of $V$ iff $W$ is closed under vector addition and vector scaling. (See Theorem \ref{ch::lin_alg::thm::vector_space_alt_characterization} for the precise meaning of this).
\end{theorem}

\begin{remark}
    (Examples of vector subspaces).
    
    \begin{itemize}
        \item $\R^2$ is a vector subspace of $\R^3$.
        \item $\R^n$ is a vector subspace of $\R^m$ whenever $n < m$.
        \item Any line in the plane is a vector subspace of $\R^2$.
        \item A line in three-dimensional space is not a vector subspace of $\R^2$ but is a vector subspace of $\R^3$.
        \item Two-dimensional planes of the form $\spann(\{\vv, \ww\})$, for nonzero $\vv, \ww \in \R^3$, are not vector subspaces of $\R^2$ but are vector subspaces of $\R^3$.
        \item Two-dimensional planes of the form $\spann(\{\vv, \ww\})$, for nonzero $\vv, \ww \in \R^4$, are not vector subspaces of $\R^3$ but are vector subspaces of $\R^4$.
        \item ``$k$-dimensional planes'' of the form $\spann(\{\vv_1, ..., \vv_k\})$, for nonzero $\vv_1, ..., \vv_k \in \R^n$, are not vector subspaces of $\R^k$ for $k < n$, but are vector subspaces of $\R^n$.
    \end{itemize}
\end{remark}

\newpage

\subsection*{Linear independence}

In this subsection, we introduce the concept of \textit{linearly independent vectors}, which is a generalization of the notion of ``non-parallel vectors'' to higher dimensions\footnote{In fact, we will see that linear independence helps us define what \textit{dimension} even means!}.

\begin{defn}
    (Linear independence).
    
    Let $V$ be a vector space. If $S = \{\vv_1, ..., \vv_k\}$ is a finite subset of $V$, then we say the vectors in $S$ are \textit{linearly independent} from each other iff for all $\vv \in S$ we have $\vv \notin \spann(S - \{\vv\})$.
    
    Intuitively, the vectors in $S$ are linearly independent if every vector inside $S$ is required to produce $\spann(S)$. Indeed, we prove in Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets} that removing a vector from a linearly independent set produces a set whose span is a proper subset of what it was before.
    
    Vectors are said to be \textit{linearly dependent} on each other iff they are not linearly independent from each other.
\end{defn}

\begin{remark}
    ($\emptyset$ is linearly independent). The empty set $\emptyset$ is linearly independent by false hypothesis.
\end{remark}

\begin{theorem}
    Any subset of a vector space that contains $\mathbf{0}$ is linearly dependent.
\end{theorem}

\begin{proof}
    If a subset $S$ of a vector space contains $\mathbf{0}$, then the condition $(\forall \vv \in S \spc \vv \notin \spann(S - \{\vv\}))$ is violated by $\vv = \mathbf{0}$, since $\mathbf{0}$ is in the span of every set.
\end{proof}

\begin{theorem}
    (Condition for linear independence).
    
    Let $V$ be a vector space over a field $K$. A subset $\{\vv_1, ..., \vv_k\} \subseteq V$ is linearly dependent iff there are scalars $c_1, ..., c_k \in K$ not all $0$ such that
    
    \begin{align*}
        c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}.
    \end{align*}
\end{theorem}

\begin{proof}
    Set $S := \{\vv_1, ..., \vv_k\}$.
    
   ($\implies$). Assume $S$ is linearly dependent. Then $S$ is not linearly independent, so there is some $i$ for which ${\vv_i \in \spann(S - \{\vv_i\})} = \spann(\{\vv_1, ..., \cancel{\vv_i}, ..., \vv_k\})$. This makes $\vv_i$ a linear combination of $\vv_1, ..., \cancel{\vv_i}, ..., \vv_k$, ${\vv_i = c_1 \vv_1 + ... + \cancel{c_i \vv_i} + ... + c_k \vv_k}$, where $c_j \in K$ for $j \in [1, k]$. Subtracting $\vv_i$ from both sides of the previous equation, we have ${(c_1 \vv_1 + ... + c_{i - 1} \vv_{i - 1}) - \vv_i + (c_{i + 1} \vv_{i + 1} + ... + c_k \vv_k) = \mathbf{0}}$, i.e., we have ${d_1 \vv_1 + ... + d_k \vv_k = \mathbf{0}}$, where $d_1 = c_1, ..., d_{i - 1} = c_{i - 1}, d_i = -1, d_{i + 1} = c_{i + 1}, ..., d_k = c_k$. Thus, as desired, there are $d_1, ..., d_k \in K$ not all zero such that $d_1 \vv_1 + ... + d_k \vv_k = \mathbf{0}$.
   
   ($\impliedby$). Assume there are $c_1, ..., c_k \in K$ not all zero such that $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$. Then $c_i \vv_i = c_1 \vv_1 + ... + \cancel{c_i \vv_i} + ... + c_k \vv_k$. Since $c_i \neq 0$, we can divide by $c_i$ to obtain $\vv_i = -\frac{c_1}{c_i} \vv_1 - ... - \cancel{\frac{c_i}{c_i}\vv_i} - ... - \frac{c_k}{c_i}\vv_k$. Thus $\vv_i \in \spann(\{\vv_1, ..., \cancel{\vv_i}, ..., \vv_k\}) = \spann(S - \{\vv_i\})$; $\vv_i$ violates the linear independence condition, so $S$ is linearly dependent.
\end{proof}

\begin{lemma}
    \label{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}
    (Adding and removing vectors from linearly independent and dependent sets).

    \begin{enumerate}
        \item Appending a vector to a linearly independent set produces...
        \begin{enumerate}
            \item[1.1.] another linearly independent set, if the vector isn't in the span of the original set.
            \item[1.2.] a linearly dependent set, if the vector is in the span of the original set.
        \end{enumerate}
        \item Removing a vector from a linearly independent set produces another linearly independent set.
        \item Appending a vector to a linearly dependent set produces another linearly dependent set.
        \item Removing a vector from a linearly dependent set does not change the span of that set.
    \end{enumerate}
\end{lemma}

\begin{proof}
   Left as exercise.
\end{proof}


\newpage

\subsection*{Basis and dimension}

In this subsection, we define the concepts of \textit{basis} and \textit{dimension}. A \textit{basis} for a vector space will be defined to be a spanning set of that vector space with as few vectors in it as possible; a basis is a ``minimal spanning set''. (We will come to see that this minimality is equivalent to linear independence). The \textit{dimension} of a \textit{finite-dimensional} vector space will be defined to be the number of vectors in any basis for it.

\begin{defn}
    (Finite- and infinite- dimensionality).
    
    A vector space is said to be \textit{finite-dimensional} iff it is spanned by a finite set of vectors and is said to be \textit{infinite-dimensional} iff this is not the case.
\end{defn}

\begin{defn}
    (Basis for a finite-dimensional vector space). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces. We say that a subset $E \subseteq W$ is a \textit{basis} for $V$ iff
    
    \begin{enumerate}
        \item $E$ spans $V$.
        \item $E$ is minimal: if $F$ is another set of vectors spanning $V$, then $|F| \geq |E|$.
    \end{enumerate}
\end{defn}

\begin{remark}
    (Bases are automatically subsets of the vector spaces they span).

    Notice that if $E$ spans $V$, then it is automatically true that $E \subseteq V$, since for any $\vv \in E$ we have ${\vv \in \spann(E) = V}$.
\end{remark}

\begin{defn}
    (Dimension of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space. The \textit{dimension} $\dim(V)$ of $V$ is the number of basis vectors in a basis for $V$.
\end{defn}

\begin{theorem}
    Every finite-dimensional vector space has a basis.
\end{theorem}

\begin{proof}
    Let $V$ be a finite-dimensional vector space. Let $C = \{S \mid \spann(S) = V \text{ and $S$ is finite} \}$. By the well-ordering principle, we can select $E$ to be the element of minimal size from $C$. This $E$ is a minimal spanning set for $V$ and thus a basis for $V$.
\end{proof}

\begin{remark}
\label{ch::lin_alg::rmk::zero_dim_vector_space}
    (The $0$-dimensional vector space). 
    
    The empty set $\emptyset$ spans $\{\mathbf{0}\}$ because [...]. It is also a minimal spanning set because there are no sets ``smaller'' than $\emptyset$. Therefore $\emptyset$ is a basis for $\{\mathbf{0}\}$, and so $\{\mathbf{0}\}$ is a zero-dimensional vector space. Since $\emptyset$ is the only set with cardinality zero, it is the only basis with cardinality zero; thus, $\{\mathbf{0}\}$ is the only $0$-dimensional vector space.
\end{remark}

The following two lemmas enable us to obtain a more useful characterization of bases.

\begin{lemma}
\label{ch::lin_alg::lemma::n_lin_indep_vectors_span_n_dim_space}
    ($n$ linearly independent vectors taken from the same $n$-dimensional vector space span that vector space).

    Let $V$ be a finite-dimensional vector space. If a finite set $S \subseteq V$ of vectors is linearly independent and $|S| = \dim(V)$, then $S$ spans $V$.
\end{lemma}

\begin{proof}
     % From p. 30 of Alan MacDonald's Geometric and Linear Algebra

   Suppose $S = \{\vv_1, ..., \vv_n\}$ is linearly independent and that $n = \dim(V)$. We immediately see that $V = \{\mathbf{0}\}$ and that $\spann(S) = \spann(\emptyset) = \{\mathbf{0}\} = V$ if $n = \dim(V) = 0$, so assume $n > 0$.
   
   Let $K$ be the field over which $V$ is taken to be a vector space. Since $V$ is $n$-dimensional, we can take a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$. Since $\spann(E) = V$, then there are $c_1, ..., c_n \in K$ such that ${\vv_1 = c_1 \ee_1 + ... + c_n \ee_n}$. Since $S$ is linearly independent, then $\vv_1 \neq \mathbf{0}$, and some $c_i$ must be nonzero. Since we can divide by $c_i$, we can solve for $\ee_1$ in terms of $c_1, ..., c_n, \vv_1, \ee_2, ..., \ee_n$, and conclude $\ee_1 \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$. 
   
   Now let $\vv \in V$. Again because $\spann(E) = V$, there are $d_1, ..., d_n \in K$ such that $\vv = d_1 \ee_1 + ... + d_n \ee_n$. Since $\ee_1 \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$, we can express $\ee_1$ as a linear combination of $\vv_1, \ee_2, ..., \ee_n$; doing this, we see $\vv$ is a linear combination of $\vv_1, \ee_2, ..., \ee_n$. Thus $\vv \in \spann(\{\vv_1, \ee_2, ..., \ee_n\})$. Since $\vv$ is arbitrary, then ${V \subseteq \spann(\{\vv_1, \ee_1, ..., \ee_n\})}$, which implies $V = \spann(\{\vv_1, \ee_1, ..., \ee_n\})$.

    Repeating this process inductively, we can conclude that $V = \spann(\{\vv_1, \vv_2, \ee_3, ..., \ee_n\})$, and ultimately that $V = \spann(\{\vv_1, ..., \vv_n\})$. 
\end{proof}

\begin{lemma}
\label{ch::lin_alg::lemma::dimension}
    (Linearly independent spanning set lemma). 
    
    For any $n$-dimensional vector space $V$ with $n \neq 0$, we have the following equivalent facts:
    
    \begin{enumerate}
        \item A set of more than $n$ vectors from $V$ is linearly dependent.
        \item A set of less than $n$ vectors from $V$ does not span $V$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    First we show the logical equivalence.
    
    ($(1) \implies (2)$). Suppose for contradiction that a set $S$ of vectors with $|S| < n$ spans $V$. Remove vectors from $S$ until a linearly independent set $T$ is obtained. By item 4 of Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}, removing vectors from linearly dependent sets does not change their span, so $T$ spans $V$. Thus, $T$ is a linearly independent spanning set with $|T| < n$. Since $n > |T|$, it follows from $(1)$ that $T$ is linearly dependent; contradiction.
    
    ($(2) \implies (1)$). Use a similar argument that invokes item 1.1 of Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}.

    Now we show (1) is true. Consider a set $S = \{\vv_1, ..., \vv_n, \vv_{n + 1}, ..., \vv_k\} \subseteq V$ of more than $n$ vectors from $V$. By the previous lemma, we know $\{\vv_1, ..., \vv_n\}$ spans $V$. Thus for each $i \in \{n + 1, ..., k\}$, we have ${\vv_i \in \spann(\{\vv_1, ..., \vv_n\}) \subseteq \spann(S - \{\vv_i\})}$. Since there is a $\vv \in S$ such that $\vv \in \spann(S - \{\vv\})$, then $S$ is linearly dependent.   
\end{proof}

\begin{theorem}
    (Bases for finite-dimensional vector spaces are linearly independent spanning sets).
    
    Let $V$ be a finite-dimensional vector space. A set $E$ of vectors is a basis for $V$ iff $E$ spans $V$ and $E$ is linearly independent.
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    We need to show $(\text{$E$ is a minimal spanning set of $V$}) \iff (\text{$E$ is linearly independent})$.
    
    (Case $E \neq \emptyset$).
    
    \indent ($\implies$). Suppose that $E$ is a minimal spanning set of $V$. We need to show that $E$ is linearly independent, so suppose for contradiction that $E$ is linearly dependent. If $E$ is linearly dependent, then by item 4 from Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets} we can remove a vector from $E$ without changing $\spann(E) = V$, so, $E$ is not minimal; contradiction.
    
    \indent ($\impliedby$). Suppose that $E$ spans $V$ and that $E$ is linearly independent. Since $E$ spans $V$, item 2 from the previous lemma
    implies $|E| \geq \dim(V)$; since $E$ is linearly independent, item 1 from previous lemma implies $|E| \leq \dim(V)$. Therefore $|E| = \dim(V)$. Now we can see the assertion ``if $F$ is a spanning set of $V$ then $|F| \geq |E|$'' is true. If $F$ is a spanning set of $V$, then item $2$ of the previous lemma implies $|F| \geq \dim(V) = |E|$.
    
    (Case $E = \emptyset$). Since there is only one $E$ satisfying $E = \emptyset$, the if-and-only-if we need to show simplifies to an \textit{and} statement: $(\text{$\emptyset$ is a minimal spanning set of $V$}) \text{ and } (\text{$\emptyset$ is linearly independent})$. We discussed how the first statement is true in Remark \ref{ch::lin_alg::rmk::zero_dim_vector_space}. The second statement, which is equivalent to $(\forall \vv \in \emptyset \spc \vv \notin \spann(\emptyset - \{\vv\}))$, is true by false hypothesis, since\footnote{$(\forall \vv \in \emptyset \spc \vv \notin \spann(\emptyset - \{\vv\}))$ is equivalent to $(\forall \vv \spc \vv \in \emptyset \implies \vv \notin \spann(\emptyset - \{\vv\}))$. Because $\vv \in \emptyset$ is false for every $\vv \in \emptyset$, the overall implication $\vv \in \emptyset \implies \vv \notin \spann(\emptyset - \{\vv\})$ is true for every $\vv \in \emptyset$.} there is no $\vv$ for which ``$\vv \in \emptyset$'' is a true statement.
    
    
\end{proof}

Because the minimality condition in the definition we gave for ``basis'' boils down to a comparison of cardinalities of spanning sets, it is only applicable to finite-dimensional vector spaces. The above theorem suggests a more general definition that applies to all vector spaces, however.

\begin{defn}
    (Basis).
    
    Let $V$ be a vector space. We say that a set $E$ of vectors is a \textit{basis} for $V$ iff $E$ spans $V$ and $E$ is linearly independent.
\end{defn}

\begin{remark}
    (No need for the well-ordering principle!).

    With this new definition of ``basis'', it is now possible to prove that every finite-dimensional vector space has a basis without using the well-ordering principle, and thus without the axiom of choice\footnote{The axiom of choice is equivalent to the well-ordering principle.}. This is because linear independence is in some sense a ``local'' property (to test if a set $S$ is linearly independent, one only needs to know $S$), while the minimal spanning condition of before is a ``global'' property (to test if a set $S$ is a minimal spanning set, one needs to compare against all other spanning sets). Here's an outline of the proof: given a spanning set $S$ of a vector space, a linearly independent spanning set (a basis) can be obtained by removing vectors from $S$.
\end{remark}

\begin{defn}
    (Standard basis for $K^n$).
    
    Let $K$ be a field, and consider $K^n$ as a vector space over $K$. We define the \textit{standard basis} of $K^n$ to be the basis $\sE = \{\see_1, ..., \see_n\}$, where the $j$th entry of $\see_i$ is $1$ when $j = i$ and $0$ otherwise.
\end{defn}

\begin{proof}
   We should check that $\sE$ is indeed a basis. Checking that $\sE$ spans $K^n$ is easy and is left as an exercise.
   
   For linear independence, consider the equation $c_1 \see_1 + ... + c_n \see_n = \mathbf{0}$, where $c_1, ..., c_n \in K$. The equation can be rewritten as $\begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} = \mathbf{0}$, which is true only when $c_i = 0$ for all $i$. In all we have that $c_1 \see_1 + ... + c_n \see_n = \mathbf{0}$ only when $c_1, ..., c_n = 0$, so $\see_1, ..., \see_n$ are linearly independent.
\end{proof}

\begin{remark}
    (Why not define dimensionality in terms of bases?). 
    
    It is tempting to define a finite-dimensional vector spaces to be those that have finite bases. This definition would be equivalent to the one we've put in place as far as finite-dimensional vector spaces are concerned, but it becomes problematic for infinite-dimensional vector spaces. If we take the Axiom of Choice to be false, then not all vector spaces spanned by an infinite number of vectors have a basis. Thus, if we defined ``infinite-dimensional'' to mean ``has an infinite basis'', then, assuming the Axiom of Choice is false, not all vector spaces spanned by an infinite number of vectors would be classified as ``infinite-dimensional''! For this reason, it is best to define an infinite-dimensional vector space to be one spanned by an infinite number of vectors rather than one that has as an infinite basis.
\end{remark}

\begin{remark}
    (Vector spaces that \textit{don't} have bases?).
    
    The statement ``every vector space, including infinite-dimensional vector spaces, has a basis'' is equivalent to the Axiom of Choice.
\end{remark}

\newpage

\section{Linear functions}

At the beginning of the chapter, it was said that the two fundamental ideas underlying linear algebra are those of ``objects that behave like vectors'' and of ``functions that preserve the decomposition of their input vectors''. We learned that ``objects that behave like vectors'' are elements of vector spaces. Now, we will investigate ``functions that preserve the decomposition of their input vectors'', which are more formally referred to as \textit{linear functions}.

\begin{defn}
\label{ch::lin_alg::defn::linear_function_intuitive}
    (Linear function). 
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is said to be \textit{linear} iff we have $\ff(c_1 \vv_1 + c_2 \vv_2) = c_1 \ff(\vv_1) + c_2 \ff(\vv_2)$ for all $c_1, c_2 \in K$ and $\vv_1, \vv_2 \in V$.
    
    What does this definition really mean, though? It may help understanding to introduce the notation $\ww_1 := \ff(\vv_1)$ and $\ww_2 := \ff(\vv_2)$. With this notation, we see that $\ff$ is linear iff we have the following for every $c_1, c_2 \in K$ and $\vv_1, \vv_2 \in V$:
    
    \begin{align*}
        \vv_1 \overset{\ff}{\mapsto} \ww_1 &\text{ and } \vv_2 \overset{\ff}{\mapsto} \ww_2 \\
        &\implies \\
        c_1 \vv_1 + c_2 \vv_2 &\overset{\ff}{\mapsto} c_1 \ww_1 + c_2 \ww_2
    \end{align*}
    
    So, roughly speaking, $\ff$ is linear iff elements in $V$ ``interact'' in the same way as do their corresponding elements in the image $\ff(V)$.
    
    Linear functions are also referred to as \textit{linear transformations}, \textit{linear operators}, or \textit{linear maps}. We will stick with the terminology ``linear function''.
\end{defn}

\begin{remark}
\label{ch::lin_alg::rmk::linear_means_vector}
    (``Linear'' in ``linear function'' means ``vector'').
    
    Initially, one might be confused as to what is actually ``linear'' about a linear function, and ask something like, ``What do linear functions have to do with lines?'' The answer is that the ``linear'' in ``linear function'' is meant to connote ``linear element''. (Recall that elements of vector spaces are also sometimes called ``linear elements''). Linear functions are called such because they are the functions that play nicely with linear elements. A better name for ``linear function'' would be ``vector-respecting function''.
\end{remark}

We now quickly present an slightly alternative characterization of linear functions. This is the characterization that is most often used for the definition of a linear function in other texts, and is often easier to check than the above definition in practice.

\begin{theorem}
    (The most common characterization of linear functions).
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is linear iff
    
    \begin{enumerate}
        \item $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ for all $\vv, \ww \in V$.
        \item $\ff(c\vv) = c\ff(\vv)$ for all $c \in K$ and $\vv \in V$.
    \end{enumerate}
\end{theorem}

\begin{proof}
   Left as exercise.
\end{proof}

The definition of a linear functions also easily generalizes to the following fact.

\begin{theorem}
    (Linear function).
    
    Let $V$ and $W$ be vector spaces over a field $K$. A function $\ff:V \rightarrow W$ is linear iff
    
    \begin{align*}
        \ff \Big(\sum_{i = 1}^n c_i \vv_i \Big) = \sum_{i = 1}^n c_i \ff(\vv_i) \text{ for all } c_1, ..., c_n \in K \text{ and } \vv_1, ..., \vv_n \in V.
    \end{align*}
\end{theorem}

\begin{proof}
    Left as exercise.
\end{proof}

\newpage

Now that we are familiar with theoretical characterizations of linear functions, we will investigate linear functions from a geometric perspective.

\begin{theorem}
    (Linear functions send $\mathbf{0}$ to $\mathbf{0}$).

    If $V$ and $W$ are vector spaces and $\ff:V \rightarrow W$ is a linear function, then $\ff(\mathbf{0}) = \mathbf{0}$.
\end{theorem}

\begin{proof}
    We have $\ff(\mathbf{0}) = \ff(0 \cdot \mathbf{0}) = 0 \cdot \ff(\mathbf{0}) = 0 \cdot \mathbf{0} = \mathbf{0}$.
\end{proof}

\begin{theorem}
    Linear functions $\R^n \rightarrow \R^m$ send $\mathbf{0}$ to $\mathbf{0}$ and send parallel lines to parallel lines.
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    We already know that all linear functions send $\mathbf{0}$ to $\mathbf{0}$, so we only need show that linear functions $\R^n \rightarrow \R^m$ send parallel lines to parallel lines. So, consider two parallel lines $\eell_1$ and $\eell_2$ in $\R^n$ described by $\eell_1(t) = \vv_0 + t\vv$ and $\eell_2(t) = \ww_0 + t\vv$, where $\vv_0, \ww_0, \vv \in \R^n$. If $\ff:\R^n \rightarrow \R^m$ is linear, then we have $\ff(\eell_1(t)) = \ff(\vv_0) + t \ff(\vv)$ and $\ff(\eell_2(t)) = \ff(\ww_0) + t \ff(\vv)$. These transformed lines are parallel because they have the same direction vector, $\ff(\vv) \in \R^m$. Thus $\ff$ sends parallel lines to parallel lines, as desired.
\end{proof}

\begin{remark}
    (Examples of linear functions $\R^2 \rightarrow \R^2$).
    
    The following are examples of linear functions from $\R^2$ to $\R^2$:
    
    \begin{itemize}
        \item rotations about the origin
        \item reflection across a line through the origin
        \item projection onto a line through the origin
    \end{itemize}
    
    In order to convince yourself that the above functions are linear, convince yourself that each above function $\ff$ satisfies $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ and $\ff(c \vv) = c \vv$. (For example, if $\ff$ is a rotation about the origin, then $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ is intuitively true because rotating the sum $\vv + \ww$ intuitively produces the same result as summing the rotated vectors $\ff(\vv)$ and $\ff(\ww)$).
\end{remark}

Now that we are somewhat familiar with linear functions, we present a more abstract result, which formalizes the way in which we can think of linear functions as themselves being vectors.

\begin{theorem}
\label{ch::lin_alg::thm::vector_space_linear_functions}
    (Vector space of linear functions).

    Let $V$ and $W$ be vector spaces over a field $K$. The set $\LLLL(V \rightarrow W)$ of linear functions $V \rightarrow W$, taken together with $+:V \rightarrow V \rightarrow V$ being function addition and $\cdot:V \rightarrow K \rightarrow V$ being function scalar multiplication, is a vector space over $K$.
\end{theorem}

\begin{proof}
    According to Theorem \ref{ch::lin_alg::thm::vector_space_alt_characterization}, we need to show that $\LLLL(V \rightarrow W)$ is a vector-containing set and that $\LLLL(V \rightarrow W)$ is closed under vector addition and vector scaling. We leave showing that $\LLLL(V \rightarrow W)$ is a vector-containing set as an exercise, and show the other two conditions.

    \begin{itemize}
        \item (Closure under vector addition). Assume $\ff, \gg \in \LLLL(V \rightarrow W)$, and consider $\ff + \gg$. We need to show $(\ff + \gg) \in \LLLL(V \rightarrow W)$; that is, we need to show (1) that $(\ff + \gg)(\vv + \ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww)$ for all $\vv, \ww \in V$ and (2) that $(\ff + \gg)(c\vv) = c(\ff + \gg)(\vv)$ for all $\vv \in V$ and $c \in K$.
        \begin{itemize}
            \item[1.] We have $(\ff + \gg)(\vv + \ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww) = \ff(\vv) + \gg(\vv) + \ff(\ww) + \gg(\ww) = \ff(\vv) + \gg(\vv) + \ff(\ww) + \gg(\ww) = (\ff + \gg)(\vv) + (\ff + \gg)(\ww)$.
            \item[2.] We have $(\ff + \gg)(c\vv) = \ff(c\vv) + \gg(c\vv) = c\ff(\vv) + c\gg(\vv) = c(\ff(\vv) + \gg(\vv)) = c(\ff + \gg)(\vv)$.
        \end{itemize}
        \item (Closure under vector scaling). Assume $\ff \in \LLLL(V \rightarrow W)$ and $c \in K$. We need to show $c\ff \in \LLLL(V \rightarrow W)$; that is, we need to show (1) that $(c\ff)(\vv + \ww) = (c\ff)(\vv) + (c\ff)(\ww)$ for all $\vv, \ww \in V$ and (2) that $(c\ff)(d\vv) = d(c\ff)(\vv)$ for all $d \in K$ and $\vv \in V$.
        \begin{enumerate}
            \item[1.] We have $(c\ff)(\vv + \ww) = c \ff(\vv + \ww) = c (\ff(\vv) + \ff(\ww)) = c \ff(\vv) + c \ff(\ww) = (c\ff)(\vv) + (c\ff)(\ww)$.
            \item[2.] We have $(c\ff)(d\vv) = c\ff(d\vv) = cd\ff(\vv) = c(d\ff(\vv)) = c(d\ff)(\vv)$.
        \end{enumerate}
    \end{itemize}
\end{proof}

The following formalizes a result that one would intuitively suspect to be true. We will need this result later.

\begin{theorem}
\label{ch::lin_alg::thm::basis_sent_to_any_ordered_list}
    (Any basis can get sent to any list of vectors by some linear function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces and let $E = \{\ee_1, ..., \ee_n\}$ be a basis of $V$. For all ${\ww_1, ..., \ww_n \in W}$, there exists a linear function sending $\ee_i \mapsto \ww_i$.
\end{theorem}

\begin{proof} \mbox{} \\ \indent
    For any linear function $\ff$, we have $\ff(\vv) = \sum_{i = 1}^n ([\vv]_E)_i \ff(\ee_i)$. We want to construct a linear function $\gg$ with $\gg(\ee_i) = \ww_i$. Thinking about replacing the $\ff(\ee_i)$ in the previous sum with $\ww_i$ gives us the idea to define $\gg(\vv) := \sum_{i = 1}^n ([\vv]_E)_i \ww_i$. 
    
    We now check that $\gg(\ee_i) = \ww_i$. We have $\gg(\ee_i) = \sum_{j = 1}^n ([\ee_i]_E)_j \ww_j = \sum_{j = 1}^n \delta_{ij} \ww_j = \ww_i$, as desired. 
    
    Lastly, we have to check that $\gg$ is linear. This is straightforward and left as an exercise.
\end{proof}

\newpage

\subsection*{Kernel and image of linear functions}

This section focuses on two important vector subspaces that come with every linear function.

\begin{defn}
    (Kernel, image of a linear function).
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. The \textit{kernel} of $\ff$ is the set of all vectors that get sent to $\mathbf{0}$ by $\ff$:
    
    \begin{align*}
        \ker(\ff) := \ff^{-1}(\{\mathbf{0}\}) = \{ \vv \in V \mid \ff(\vv) = \mathbf{0} \}.
    \end{align*}
    
    The \textit{image} of $\ff$ is the set of all vectors that are mapped to by $\ff$:
    
    \begin{align*}
        \im(\ff) := \ff(V) = \{ \ww \in W \mid \exists \vv \in V \spc \ww = \ff(\vv) \}.
    \end{align*}
\end{defn}

\begin{theorem}
    (Kernel and image are vector subspaces). 
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. The kernel of $\ff$ is a vector subspace of $V$ and the image of $\ff$ is a vector subspace of $W$.
\end{theorem}

\begin{proof}
    According to Theorem \ref{ch::lin_alg::defn::vector_subspace_characterization}, we need to show that each claimed subspace is closed under vector addition and vector scaling.

    We first do so for the kernel, $\ff^{-1}(\{\mathbf{0}\})$.
    \begin{itemize}
        \item (Closure under vector addition). Let $\vv_1, \vv_2 \in \ff^{-1}(\{\mathbf{0}\})$. Then $\ff(\vv_1) = \mathbf{0}$ and $\ff(\vv_2) = \mathbf{0}$. We have $\ff(\vv_1 + \vv_2) =\ff(\vv_1) + \ff(\vv_2) = \mathbf{0} + \mathbf{0} = \mathbf{0}$, so $(\vv_1 + \vv_2) \in \ff^{-1}(\{\mathbf{0}\})$.        
        \item (Closure under vector scaling). Let $\vv \in \ff^{-1}(\{\mathbf{0}\})$. Then $\ff(\vv) = \mathbf{0}$. We have $\ff(c\vv) = c\ff(\vv) = c\cdot \mathbf{0} = \mathbf{0}$, so $c\vv \in \ff^{-1}(\{\mathbf{0}\})$.
    \end{itemize}

    Now we do so for the image, $\ff(V)$.
    \begin{itemize}
        \item (Closure under vector addition). Let $\ww_1, \ww_2 \in \ff(V)$. Then there exist $\vv_1, \vv_2 \in V$ such that $\ww_1 = \ff(\vv_1)$ and $\ww_2 = \ff(\vv_2)$. We have $\ww_1 + \ww_2 = \ff(\vv_1) + \ff(\vv_2) = \ff(\vv_1 + \vv_2)$, where $(\vv_1 + \vv_2) \in V$, so $(\ww_2 + \ww_2) \in \ff(V)$.
        \item (Closure under vector scaling). Let $\ww \in \ff(V)$. Then there exists $\vv \in V$ such that $\ff(\vv) = \ww$. We have $c\ww = c\ff(\vv) = \ff(c\vv)$, where $c\vv \in V$, so $c\ww \in \ff(V)$.
    \end{itemize}
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::recursive_preimage_char}
    (Recursive preimage characterization).

    Let $V$ and $W$ be vector spaces and consider a linear function $\ff:V \rightarrow W$ and $\ww \in W$. If $\ff^{-1}(\{\ww\})$ is nonempty, then 

    \begin{align*}
        \ff^{-1}(\{\ww\}) = \{\vv + \vv_0 \mid \vv_0 \in \ff^{-1}(\{\mathbf{0}\})\} \text{ for all $\vv \in \ff^{-1}(\{\ww\})$}.
    \end{align*}
\end{theorem}

\begin{proof} \mbox{} \\ \indent
   ($\subseteq$). Let $\uu \in \ff^{-1}(\{\ww\})$. For all $\vv \in \ff^{-1}(\{\ww\})$, we have $\ff(\uu - \vv) = \ff(\uu) - \ff(\vv) = \ww - \ww = \mathbf{0}$, so $(\uu - \vv) \in \ff^{-1}(\{\mathbf{0}\})$. That is, for all $\vv \in \ff^{-1}(\{\ww\})$ there is a $\vv_0 \in \ff^{-1}(\{\mathbf{0}\})$ such that $\uu - \vv = \vv_0$. We conclude that for all $\vv \in \ff^{-1}(\{\ww\})$ there is a $\vv_0 \in \ff^{-1}(\{\mathbf{0}\})$ such that $\uu = \vv + \vv_0$; for all $\vv \in \ff^{-1}(\{\ww\})$, we have $\uu \in \{\vv + \vv_0 \mid \vv_0 \in \ff^{-1}(\{\mathbf{0}\})\}$. 
   
   ($\supseteq$). Suppose $\uu = \vv + \vv_0$ for some $\vv_0 \in \ff^{-1}(\{\mathbf{0}\})$. We have $\ff(\uu) = \ff(\vv + \vv_0) = \ff(\vv) + \ff(\vv_0) = \ff(\vv) + \mathbf{0} = \ff(\vv) = \ww$, so $\uu \in \ff^{-1}(\{\ww\})$.
\end{proof}

\begin{defn}
    (Minimal kernel).
    
    Since $\{\mathbf{0}\}$ is the smallest (in the sense of set-containment) kernel possible for a linear function, we say that the kernel of a linear function is \textit{minimal} iff it is equal to $\{\mathbf{0}\}$.
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::linear_fn_1-1_minimal_kernel}
    Let $V$ and $W$ be vector spaces. A linear function $\ff:V \rightarrow W$ is one-to-one iff it has a minimal kernel.
\end{theorem}

\begin{proof}
    $\ff$ is one-to-one iff for all $\ww \in W$ the preimage $\ff^{-1}(\{\ww\})$ is either empty or a singleton. It is easy to see via the previous theorem that the nonempty preimages are singletons iff $\ff^{-1}(\{\mathbf{0}\}) = \{\mathbf{0}\}$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence}

    (One-to-one linear functions are the linear functions that preserve linear independence). 
    
    Let $V$ and $W$ be finite-dimensional vector spaces. A linear function $\ff:V \rightarrow W$ preserves the linear independence of vectors iff it is one-to-one. That is, 
    
    \begin{align*}
       (\vv_1, ..., \vv_k \text{ are linearly independent}) &\implies (\ff(\vv_1), ..., \ff(\vv_k) \text{ are linearly independent})\\
        \text{if} \text{ } &\text{and only if} \\
        \ff \text{ } &\text{is one-to-one}
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \indent ($\impliedby$). Suppose that $\ff$ is one-to-one and that $\vv_1, ..., \vv_k$ are linearly independent. Since $\ff$ is one-to-one, it has a minimal kernel, and thus for any $c_1, ..., c_k \in K$ we have that $\ff(c_1 \vv_1 + ... + c_k \vv_k) = \mathbf{0}$ implies $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$. Since $\vv_1, ..., \vv_k$ are linearly independent, $c_1 \vv_1 + ... + c_k \vv_k = \mathbf{0}$ implies that the $c_i$'s are all $0$. In all, we have that ``$\ff(c_1 \vv_1 + ... + c_k \vv_k) = \mathbf{0}$ implies the $c_i$'s are all $0$''. Since $\ff$ is linear, this statement becomes  ``$c_1 \ff(\vv_1) + ... + c_k \ff(\vv_k) = \mathbf{0}$ implies the $c_i$'s are all $0$''. Thus $\ff(\vv_1), ..., \ff(\vv_k)$ are linearly independent, as claimed.
    
    ($\implies$). Suppose that if $\vv_1, ..., \vv_k$ are linearly independent, then $\ff(\vv_1), ..., \ff(\vv_k)$ are linearly independent. We need to show $\ff$ is one-to-one; it suffices to show that $\ff$ has a minimal kernel. Let $\vv \in \ff^{-1}(\{\mathbf{0}\}) \iff \ff(\vv) = \mathbf{0}$. We want to show $\vv = \mathbf{0}$. 
    
    Since $V$ is finite-dimensional, there is a basis $E = \{\ee_1, ..., \ee_n\}$ for $V$. Expressing $\vv$ relative to $E$, we have $\ff(\vv) = \ff\Big(\sum_{i = 1}^k ([\vv]_E)_i \ee_i\Big) = \sum_{i = 1}^k ([\vv]_E)_i \ff(\ee_i) = \mathbf{0}$. The hypothesis implies that $\ff(\ee_1), ..., \ff(\ee_n)$ are linearly independent, so $([\vv]_E)_i = 0$ for all $i$ is the only solution to $\sum_{i = 1}^k ([\vv]_E)_i \ff(\ee_i) = \mathbf{0}$. Thus $([\vv]_E)_i = 0$ for all $i$, i.e., $\vv = \mathbf{0}$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::main_dim}
    (Main dimension theorem).
    
    Let $V$ and $W$ be vector spaces, and let $\ff:V \rightarrow W$ be a linear function. If $V$ is finite-dimensional, then $\ff^{-1}(\{\mathbf{0}\})$ and $\ff(V)$ are also finite-dimensional, and we have

    \begin{align*}
        \dim(\ff(V)) = \dim(V) - \dim(\ff^{-1}(\{\mathbf{0}\})).
    \end{align*}
    
    Also, if $\ff^{-1}(\{\mathbf{0}\})$ and $\ff(V)$ are finite-dimensional, then $V$ must be finite-dimensional, and the same relationship with dimensions holds.
    
    This result is commonly called the \textit{rank-nullity theorem}.
\end{theorem}

\begin{proof}
    We prove the first part of the theorem (before ``Also'').
    
    If $V$ is finite-dimensional, then $\ff^{-1}(\{\mathbf{0}\})$ is also finite-dimensional since $\ff^{-1}(\{\mathbf{0}\}) \subseteq V$, so we can choose a basis $\{\ee_1, ..., \ee_k\}$ for $\ff^{-1}(\{\mathbf{0}\})$. Using the uniqueness of dimension and Lemma \ref{ch::lin_alg::lemma::dimension}, one can show \textbf{add footnote} that it is possible to add vectors $\ee_{k + 1}, ..., \ee_n$ to this basis so that it becomes $\{\ee_1, ..., \ee_k, \ee_{k + 1}, ..., \ee_n\}$, a basis for $V$. Since $\dim(\ff^{-1}(\{\mathbf{0}\})) = k$ and $\dim(V) = n$, we want to show $\dim(\ff(V)) = \dim(V) - \dim(\ff^{-1}(\{\mathbf{0}\})) = n - k$; we want to show $\dim(\ff(V)) = n - k$.

    Since $\ff:V \rightarrow W$ is linear, we have ${\ff(\vv) = ([\vv]_E)_1 \ff(\ee_1) + ... + ([\vv]_E)_k \ff(\ee_k) + ([\vv]_E)_{k + 1} \ff(\ee_{k + 1}) + ... + ([\vv]_E)_n \ff(\ee_n)}$. Because $\ee_1, ..., \ee_k \in \ff^{-1}(\{\mathbf{0}\})$, this simplifies to ${\ff(\vv) = ([\vv]_E)_{k + 1} \ff(\ee_{k + 1}) + ... + ([\vv]_E)_n \ff(\ee_n)}$. 
    
    Therefore, any $\ww \in \ff(V)$ is in the span of $\{\ee_{k + 1}, ..., \ee_n\}$. We will show that $\{\ee_{k + 1}, ..., \ee_n\}$ is a basis for $\ff(V)$. Once know this, then, since there are $n - k$ of these vectors, we have shown $\dim(\ff(V)) = n - k$, which is what we want.
    
    It remains to show $\{\ee_{k + 1}, ..., \ee_n\}$ is a linearly independent set. Suppose for the sake of contradiction it's linearly dependent, i.e., that there are $c_{k + 1}, ..., c_n$ not all zero such that $c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = \mathbf{0}$. By the linearity of $\ff$, this is equivalent with $\ff(c_{k + 1} \ee_{k + 1} + ... + c_n \ee_n) = \mathbf{0}$ for some $c_i$'s not all zero. Thus $c_{k + 1} \ee_{k + 1} + ... + c_n \ee_n \in \ff^{-1}(\{\mathbf{0}\}) = \spann(\{\ee_1, ..., \ee_k\})$, which means $c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = d_1 \ee_1 + ... + d_k \ee_k$ for some $c_i$'s and $d_i$'s not all zero. Then $-(d_1 \ee_1 + ... + d_k \ee_k) + c_{k + 1} \ff(\ee_{k + 1}) + ... + c_n \ff(\ee_n) = 0$ for some $c_i$'s and $d_i$'s not all zero. But $\{\ee_1, ..., \ee_n\}$ is a basis for $V$, so this cannot happen. Thus $\ff(\ee_{k + 1}), ..., \ff(\ee_n)$ are linearly independent.
\end{proof}

\newpage

\subsection*{Linear isomorphisms}

\begin{defn}
\label{ch::lin_alg::defn::linear_iso}
    (Linear isomorphism).
    
    Let $V$ and $W$ be vector spaces over a field $K$. Iff $\ff:V \rightarrow W$ is an invertible linear function, i.e. iff it is a bijection\footnote{Recall from Theorem \ref{ch::logic_pf_fns::thm::invertible_iff_bijection} that any (not necessarily linear) function is invertible iff it is a bijection.}, then it is called a \textit{linear isomorphism}, or an \textit{isomorphism (of vector spaces)}.
    
    Let's quickly explain this terminology. Recall that when we have a linear function $\ff:V \rightarrow W$, then elements in $V$ ``interact'' in the same way as do their corresponding elements in $\ff(V)$. When $\ff$ is also invertible, then $\ff(V)$ is \textit{all} of $W$, so, not only are all interactions in $V$ ``mirrored'' in $W$, but all interactions in $W$ are also mirrored in $V$! Thus, when $\ff$ is linear and invertible, $V$ and $W$ are in some sense the ``same'' vector space. For this reason, when $V$ and $W$ are isomorphic, we often say that an element $\vv \in V$ can be \textit{identified} with an element $\ww \in W$.
    
    We write $V \cong W$ to denote that the vector spaces $V$ and $W$ are isomorphic. Note that $\cong$ is an equivalence relation.
\end{defn}

The following theorem tells us that the inverse of a linear isomorphism is also a linear isomorphism.

\begin{theorem}
    (The inverse of a linear function is also a linear function).
    
    If $\ff:V \rightarrow W$ is an invertible linear function, then the inverse $\ff^{-1}$ is also a linear function.
\end{theorem}

\begin{proof}
     Left as exercise.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::linear_fn_1-1_iff_onto}
    
    (A linear function between finite-dimensional vector spaces of the same dimension is one-to-one iff it is onto).
    
    Let $V$ and $W$ be finite dimensional vector spaces with the same dimension, $\dim(V) = \dim(W)$, and let ${\ff:V \rightarrow W}$ be a linear function. Then $\ff$ is one-to-one iff $\ff$ is onto. In other words, $\ff$ is a linear isomorphism iff it is either one-to-one or onto.
\end{theorem}

\begin{proof}
    We use the contrapositive to show that, assuming the hypotheses, $\ff$ is not one-to-one iff it is not onto.
    
    $\ff$ is not one-to-one if and only if it has a nonminimal kernel, i.e., iff $\dim(\ff^{-1}(\{\mathbf{0}\})) > 0$. By the main dimension theorem, this condition is equivalent to $\dim(V) > \dim(\ff(V))$. Since we assumed $\dim(V) = \dim(W)$, this most recent condition is the same as $\dim(W) > \dim(\ff(V))$. One can check that if $Y_1$ and $Y_2$ are subspaces of the same finite-dimensional vector space, then $\dim(Y_1) > \dim(Y_2)$ iff $Y_1 \supsetneq Y_2$. Using $Y_1 = W$ and $Y_2 = \ff(V)$, we obtain the equivalent condition $W \supsetneq \ff(V)$. This is equivalent to $\ff$ not being onto.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::same_dim_iff_isomorphic}
    (Finite-dimensional vector spaces are isomorphic iff they have the same dimension).
    
    Let $V$ and $W$ be finite-dimensional vector spaces. Then there exists a linear isomorphism $V \rightarrow W$ iff $\dim(V) = \dim(W)$.
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    ($\implies$). If $\ff:V \rightarrow W$ is a linear isomorphism, then it has a minimal kernel. The main dimension theorem then implies that $\dim(W) = \dim(V) - \dim(\ff^{-1}(\{\mathbf{0}\})) = \dim(V) - 0 = \dim(V)$.
    
    ($\impliedby$). It suffices to show that every $n$-dimensional vector space is isomorphic to $K^n$. So, let $V$ be an $n$-dimensional vector space, and let $E$ be a basis for $V$. In Definition \ref{ch::lin_alg::defn::coordinates_relative_to_basis} we will define the linear function $[\cdot]_E:V \rightarrow K^n$; Theorem \ref{ch::lin_alg::thm::[]E_invertible} will show that $[\cdot]_E$ is a linear isomorphism.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::iso_bases_to_bases}
    (Linear isomorphisms send bases to bases).

    Let $V$ and $W$ be finite-dimensional vector spaces. If $E$ is a basis of $V$ and $\ff:V \rightarrow W$ is a linear isomorphism, then $\ff(E)$ is a basis for $W$.
\end{theorem}

\begin{proof}
    Linear isomorphisms are one-to-one, so they preserve the linear independence of vectors. Thus $\ff(E)$ is a linearly independent set of $\dim(V)$ many vectors. Since $\ff$ is an isomorphism, then $\dim(V) = \dim(W)$, so $\ff(E)$ is a linearly independent set of $\dim(W)$ many vectors. Recall from Lemma \ref{ch::lin_alg::lemma::n_lin_indep_vectors_span_n_dim_space} that a linearly independent set of $n$ vectors from an $n$-dimensional vector space span that space. This makes $\ff(E)$ a basis for $W$.
\end{proof}

\begin{defn}
\label{ch::lin_alg::defn::natural_iso}
    (Natural linear isomorphism).
    
    Roughly speaking, a linear isomorphism is said to be ``natural'' if it does not depend on a choice of basis. This definition of ``natural'' is not completely technically correct, but it will suffice for our purposes, because the converse (any linear isomorphism which depends on a choice of basis is unnatural) \textit{is} true. To read more about what ``natural'' really means, look up ``natural isomorphism category theory'' online.
\end{defn}

\newpage

\subsection*{Coordinatization of vectors}

\begin{defn}
\label{ch::lin_alg::defn::coordinates_relative_to_basis}
    (Coordinates of a finite-dimensional vector relative to a basis).
    
    Let $V$ be a finite-dimensional vector space over a field $K$, and let $E = \{\ee_1, ..., \ee_n\}$ be a basis for $V$. Given a vector $\vv \in V$, we define $[\vv]_E$ to be the vector in $K^{\dim(V)}$ that stores the \textit{coordinates of $\vv$ relative to the basis $E$}. Formally, $[\vv]_E$ is the tuple of scalars 
    
    \begin{align*}
        [\vv]_E := \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix} \in K^n
    \end{align*}
    
    for which
    
    \begin{align*}
        \vv = ([\vv]_E)_1 \ee_1 + ... + ([\vv]_E)_n \ee_n.
    \end{align*}
    
    We are guaranteed that such scalars exist because $E$, being a basis for $V$, spans $V$.

    We often use the notation $[\cdot]_E$ to denote the function $V \rightarrow K^{\dim(V)}$ sending $\vv \mapsto [\vv]_E$.
\end{defn}

The above definition does not make it obvious that the function $[\cdot]_E$ is well-defined. We show this now.

\begin{theorem}
    ($[\cdot]_E$ is well-defined).

    Let $V$ be a finite-dimensional vector space. The function $[\cdot]_E$ is well-defined for all bases $E$ of $V$.
\end{theorem}

\begin{proof}
    Consider $\vv, \ww \in V$ that are equal, $\vv = \ww$. We need to show that $[\vv]_E = [\ww]_E$.
    
    Set $n := \dim(V)$. We have $\vv = \sum_{i = 1}^n ([\vv]_E)_i \ee_i$ and $\ww = \sum_{i = 1}^n ([\ww]_E)_i \ee_i$. Since $\vv = \ww$, we can subtract one equation from the other to obtain $\mathbf{0} = \sum_{i = 1}^n \Big(([\vv]_E)_i - ([\ww]_E)_i\Big) \ee_i$. Since $E$ is linearly independent, then we must have $([\vv]_E)_i - ([\ww]_E)_i = 0 \iff ([\vv]_E)_i = ([\ww]_E)_i$ for all $i$. Thus $[\vv]_E = [\ww]_E$.
\end{proof}

Of more practical importance than the above is the following fact.

\begin{theorem}
\label{ch::lin_alg::thm::[]E_invertible}
    ($[\cdot]_E$ is a linear isomorphism).
    
    If $V$ is a finite-dimensional vector space with basis $E$, then $[\cdot]_E:V \rightarrow K^{\dim(V)}$ is an invertible linear function.
\end{theorem}

\begin{proof}
    For linearity, we show that $[\vv_1 + \vv_2]_E = [\vv_1]_E + [\vv_2]_E$ and that $[c\vv]_E = c[\vv]_E$.
    
    Let $n := \dim(V)$. We have
        
    \begin{align*}
        [\vv_1 + \vv_2]_E
        &= \Big[\sum_{i = 1}^n ([\vv_1]_E)_i \ee_i + \sum_{i = 1}^n ([\vv_2]_E)_i \ee_i\Big]_E \\
        &= \Big[\sum_{i = 1}^n \Big(([\vv_1]_E)_i + ([\vv_2]_E)_i \Big) \ee_i \Big]_E
        = 
        \begin{pmatrix} ([\vv_1]_E)_1 + ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_1]_E)_m + ([\vv_2]_E)_n \end{pmatrix}
        =
        \begin{pmatrix} ([\vv_1]_E)_1 \\ \vdots \\ ([\vv_1]_E)_n \end{pmatrix}
        +
        \begin{pmatrix} ([\vv_2]_E)_1 \\ \vdots \\ ([\vv_2]_E)_n \end{pmatrix} \\
        &= [\vv_1]_E + [\vv_2]_E.
    \end{align*}
        
    Now we show $[c \vv]_E = c [\vv]_E$. We have

     \begin{align*}
        [c\vv]_E = \Big[ c \Big( \sum_{i = 1 }^n ([\vv]_E)_i \ee_i \Big) \Big] = \Big[\sum_i c([\vv]_E)_i \ee_i\Big]_E = \begin{pmatrix} c([\vv]_E)_1 \\ \vdots \\ c([\vv]_E)_n \end{pmatrix}
        = c \begin{pmatrix} ([\vv]_E)_1 \\ \vdots \\ ([\vv]_E)_n \end{pmatrix}
        = c[\vv]_E.
    \end{align*}
    
    Since $[\cdot]_E$ sends basis vectors to basis vectors, it preserves linear independence, and is therefore one-to-one (recall Theorem \ref{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence}). Since $[\cdot]_E$ is a one-to-one linear function between vector spaces of the same dimension, it must also be onto. Thus $[\cdot]_E$ is a linear isomorphism.

    %Alternatively, one can see that $[\cdot]_E$ is linear by checking that it has a minimal kernel: if $[\cdot]_E(\vv) = \mathbf{0}$, then the coordinates of $\vv$ relative to $E$ are all zero, so $\vv = \mathbf{0}$.
\end{proof}

\newpage

\section{Coordinatization of linear functions with matrices}
\label{ch::lin_alg::section::coordinatization_of_linear functions}

\subsection*{Standard matrices}

In the previous section, we saw that when we have a finite-dimensional vector space $V$ with a basis $E$, we can represent any vector $\vv \in V$ by taking its coordinates $[\vv]_E$ relative to $E$. In this section, we discover that when we have finite-dimensional vector spaces $V$, $W$ with respective bases $E$, $F$, we can also coordinatize a linear function $\ff:V \rightarrow W$ by making use of the bases $E$ and $F$. The first step in doing so is to identify said $\ff$ with a linear function $K^{\dim(V)} \rightarrow K^{\dim(W)}$. The next definition shows us how to produce this linear function.

\begin{defn}
    \label{ch::lin_alg::defn::f_EF}
    (Induced linear function from $K^{\dim(V)}$ to $K^{\dim(W)}$).
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$, and let $E$ and $F$ be the respective bases of $V$ and $W$. Whenever we have a linear function $\ff:V \rightarrow W$, there is also an \textit{induced} linear function $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ for which this diagram commutes:
    
    \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRRkAjFVqMWbAGIA9YAB0FULAFsAFOwCUXbrxAZseAkRHlx9Zq0QgA6nr5HBp0mOqWpNuYuVr1tnW5xGCgAc3giUAAzACcIVSQyEBwIJDMJKzZkJQBjKAgcCgB9AFEAAgBeMpKQagY6ACMYBgAFfmMhEBisUIALHAcQWPikACZqFKQAZndJaxBshTyC4ulKsulB4YTEdMnEGYzPEAAVIuAS0mldOsbmtqcTG26+gZ5ouJ2k-fGj+ZOglwgA
        \begin{tikzcd}
            V \arrow[d, "{[\cdot]_E}"'] \arrow[r, "\ff"] & W \arrow[d, "{[\cdot]_F}"] \\
            K^{\dim(V)} \arrow[r, "{\ff_{E,F}}"']            & K^{\dim(W)}
        \end{tikzcd}
    \end{center}
        
    A diagram like the one above is said to ``commute'' iff the compositions of functions corresponding to different paths through the diagram are the same whenever the paths have the same start and end nodes. So, to say that the above diagram commutes is to say that $[\cdot]_F \circ \ff = \ff_{E,F} \circ [\cdot]_E$. That is,
    
    \begin{empheq}[box = \fbox]{align*}
        \ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}.
    \end{empheq}
    
    Concretely, the commutative diagram tells us that we can think of $\ff_{E,F}$ as accepting an input from $K^{\dim(V)}$ that is expressed relative to the basis $E$ for $V$ and producing an output in $K^{\dim(W)}$ that is expressed relative to the basis $F$ for $W$.
\end{defn}

As we continue our investigations into the coordinatization of a linear function $\ff:V \rightarrow W$, we can restrict ourselves to the case where $V = K^n$ and $W = K^m$, where $K$ is the field and $n$ and $m$ are positive integers. This is because any linear function $\ff$ \textit{not} of this form can immediately be identified\footnote{The map $\ff \mapsto \ff_{E,F}$ is a linear isomorphism.} with the linear function $\ff_{E,F}$ that \textit{is} of this form: just send $\ff \mapsto \ff_{E,F}$!

The following derivation shows us how to coordinatize a linear function $K^n \rightarrow K^m$.

\begin{deriv}
\label{ch::lin_alg::deriv::standard_matrix}
    (Standard matrix of a linear function $K^n \rightarrow K^m$). 
    
    Let $K$ be a field and consider a linear function $\ff:K^n \rightarrow K^m$. For any vector $\vv \in K^n$, we have 
    
    \begin{align*}
        \vv =
        \begin{pmatrix} ([\vv]_\sE)_1 \\ \vdots \\ 0 \end{pmatrix} + ... + \begin{pmatrix} 0 \\ \vdots \\ ([\vv]_\sE)_n \end{pmatrix},
    \end{align*}
    
    and so using the linearity of $\ff$, we find that
    
    \begin{align*}
        \ff(\vv) &= 
        \ff
        \Bigg(
            \begin{pmatrix} ([\vv]_\sE)_1 \\ \vdots \\ 0 \end{pmatrix} + ... + \begin{pmatrix} 0 \\ \vdots \\ ([\vv]_\sE)_n \end{pmatrix}
        \Bigg)
        =
        \ff \Bigg( 
        \begin{pmatrix} 
                ([\vv]_\sE)_1 \\ \vdots \\ 0 
        \end{pmatrix} \Bigg)
        +
        ...
        +
        \ff \Bigg(
        \begin{pmatrix} 
                0 \\ \vdots \\ ([\vv]_\sE)_n 
        \end{pmatrix} \Bigg) \\
        &=
        ([\vv]_\sE)_1
        \ff \Bigg(
        \begin{pmatrix} 
                1 \\ 0 \\ \vdots \\ 0 
        \end{pmatrix} \Bigg)
        +
        ...
        +
        ([\vv]_\sE)_n
        \ff \Bigg(
        \begin{pmatrix} 
                0 \\ 0 \\ \vdots \\ 1 
        \end{pmatrix} \Bigg).
    \end{align*}
    
    The above equalities can be written more compactly as
    
    \begin{align*}
        \ff(\vv) = \ff\Big( \sum_{i = 1}^n ([\vv]_\sE)_i \see_i \Big) = \sum_{i = 1}^n \ff(([\vv]_\sE)_i \see_i) = \sum_{i = 1}^n ([\vv]_\sE)_i \ff(\see_i).
    \end{align*}
    
    (Recall that the $j$th component of $\see_i$ is $1$ when $i = j$ and $0$ otherwise, and that $\sE = \{\see_1, ..., \see_n\}$ is the standard basis of $K^n$).
    
    Thinking more about the above, we see that the action of $\ff$ on an arbitrary $\vv \in K^n$ is determined by $\ff(\see_1), ..., \ff(\see_n)$. That is, if we know $\ff(\see_1), ..., \ff(\see_n)$, then we can figure out what $\ff$ is!
    
    Formally, we have discovered a function $\pp$ that takes as input the ordered list $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$, the vector $\vv \in K^n$, and produces $\ff(\vv) \in K^m$ as output:
    
    \begin{align*}
        \pp\Big( \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv \Big) := ([\vv]_E)_1 \ff(\see_1) + ... + ([\vv]_E)_n \ff(\see_n) = \ff(\vv).
    \end{align*}
    
    We turn our attention to the ordered list of column vectors that is an input to $\pp$:
    
    \begin{align*}
        \begin{pmatrix} 
            \ff(\see_1) & \hdots & \ff(\see_n)
        \end{pmatrix}.
    \end{align*}
    
    This ordered list of $n$ many column vectors from $K^m$ can be interpreted to be a grid of scalars with $m$ rows and $n$ columns. In general, an $m$ by $n$ grid of scalars is called a \textit{$m \times n$ matrix}. ($m \times n$ is read as ``$m$ by $n$'').
    
    Of course, the above matrix isn't just ``any old matrix'': this matrix represents\footnotemark the linear function $\ff$! For this reason, the above matrix is called the \textit{standard matrix of $\ff:K^n \rightarrow K^m$}.
    
    \footnotetext{\label{ch::lin_alg::footnote::standard_matrix_rep} When we say that the standard matrix of $\ff:K^n \rightarrow K^m$ ``represents'' $\ff$, we mean that the function $\FF$ that associates a linear function with its standard matrix is a bijection between the set of linear functions $K^n \rightarrow K^m$ and the set of $m \times n$ matrices. Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list} implies that $\FF$ is onto. Showing that $\FF$ is one-to-one is a simple exercise.}
\end{deriv}

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_vector_product}
    (Matrix-vector product).
    
    Let $K$ be a field and consider a linear function $\ff:K^n \rightarrow K^m$. The previous derivation showed that a linear function $\ff:K^n \rightarrow K^m$ is represented by its standard matrix, $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$. In this derivation, we formalize the notion of using $\ff$'s standard matrix to determine $\ff$ itself.
    
    From the previous derivation, we know that there is a function $\pp$ that returns $\ff(\vv)$ when given the standard matrix of $\ff$ and a vector $\vv$; specifically
    
    \begin{align*}
        \pp\Big( \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv \Big) := ([\vv]_E)_1 \ff(\see_1) + ... + ([\vv]_E)_n \ff(\see_n) = \ff(\vv).
    \end{align*}
    
    Notice that since the columns of the standard matrix of $\ff$ vary over $K^m$, and since $([\vv]_\sE)_1, ..., ([\vv]_\sE)_n$ vary\footnotemark over $K$, we can now restate the action of $\pp$ in terms of an arbitrary $m \times n$ matrix $\AA$ having $i$th column $\aa_i$ and an arbitrary column vector $\vv \in K^n$:
    
    \footnotetext{The $i$th column of $\ff(\sE)$ is $\ff(\ee_i)$. Each $\ff(\ee_i)$ can vary over $K^m$ because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}. The $([\vv]_E)_i$ vary over $K$ because $\vv$ varies over $K^n$.}
    
    \begin{align*}
        \pp\Bigg(
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA,
            \underbrace{
            \begin{pmatrix} 
                v_1 \\ \vdots \\ v_n 
            \end{pmatrix}}_\vv
            \Bigg)
            =
            v_1 \aa_1 + ... + v_n \aa_n.
    \end{align*}
    
    So, $\pp$ is really a function that accepts an $m \times n$ matrix and $n$-dimensional column vector as input and that produces an $m$-dimensional column vector as output. For this reason, we will call $\pp$ the \textit{matrix-vector product}.
    
    No one actually uses the letter $\pp$ when notating matrix-vector products. Instead, we simply establish the convention that writing a column vector to the right of a matrix indicates the evaluation of the corresponding matrix-vector product. That is, given an $m \times n$ matrix $\AA = \begin{pmatrix} \aa_1 \hdots & \aa_n \end{pmatrix}$ and a column vector $\vv \in K^m$, we define
    
    \begin{align*}
        \boxed
        {
            \AA \vv =
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA
            \underbrace{
            \begin{pmatrix} 
                v_1 \\ \vdots \\ v_n 
            \end{pmatrix}}_\vv
            :=
            v_1 \aa_1 + ... + v_n \aa_n
        }
    \end{align*}
    
    Expanding out the columns of $\AA$, here is what the above definition looks like when written out more explicitly:
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix} 
            a_{11} & \hdots & a_{1n} \\
            \vdots & \hdots & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}}_\AA
        \underbrace{
        \begin{pmatrix} 
            v_1 \\ \vdots \\ v_n 
        \end{pmatrix}}_\vv
        :=
        v_1 
        \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}
        + ... + v_n 
        \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}.
    \end{align*}
\end{deriv}

Now that we have the notation of the matrix-vector product, we can restate and expand upon the fact ``$\pp(\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}, \vv) = \ff(\vv)$''.

\begin{theorem}
    (Characterizing property of standard matrices, verbosely stated).
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$ of $\ff$ is the unique matrix satisfying
    
    \begin{align*}
        \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} \vv = \ff(\vv) \text{ for all $\vv \in K^n$}.
    \end{align*}
    
    (The left side of the above equation is a matrix-vector product).
\end{theorem}

\begin{proof}
   Derivation \ref{ch::lin_alg::deriv::standard_matrix} showed that $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$ satisfies the equation. What we have not yet shown is that the standard matrix is the \textit{only} matrix satisfying this equation. To prove this, suppose that some  matrix $\AA$ satisfies $\AA \vv = \ff(\vv)$ for all $\vv \in K^n$. We need to show $\AA$ is in fact equal to the standard matrix of $\ff$.
   
   Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list} guarantees that there is a linear function $\gg$ such that $\begin{pmatrix} \gg(\see_1) & \hdots & \gg(\see_n) \end{pmatrix} = \AA$. Thus $\begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} \vv = \begin{pmatrix} \gg(\see_1) & \hdots & \gg(\see_n) \end{pmatrix} \vv$ for all $\vv \in K^n$. That is, $\ff(\vv) = \gg(\vv)$ for all $\vv \in K^n$, so $\ff = \gg$, and $\AA = \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix}$, which is the standard matrix of $\ff$.
\end{proof}

To make notating standard matrices more compact, we make the following definition.

\begin{defn}
\label{ch::lin_alg::defn::linear_fn_acts_on_vectors}
    (Function acting on a list).
    
    Let $X$ and $Y$ by sets, let $f:X \rightarrow Y$ be a function, and let $L = \begin{pmatrix} x_1 & \hdots & x_n \end{pmatrix}$ be a finite list of elements of $X$. We define the notation
    
    \begin{align*}
        f(L) := \begin{pmatrix} f(x_1) & \hdots & f(x_n) \end{pmatrix}.
    \end{align*}
\end{defn}

The following theorem illustrates the succinctness of this new notation.

\begin{theorem}
    (Compact notation for standard matrix).
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix of $\ff$ is
    
    \begin{align*}
        \begin{pmatrix} \ff(\see_1) & \hdots & \ff(\see_n) \end{pmatrix} = \ff(\begin{pmatrix} \see_1 & ... & \see_n \end{pmatrix}) = \ff(\sE),
    \end{align*}
    
    where, in a slight abuse of notation, we use $\sE$ here to denote the \textit{list} $\begin{pmatrix} \see_1 & \hdots & \see_n \end{pmatrix}$ whose $i$th element is $\see_i$ rather than the \textit{set} $\{\see_1, ..., \see_n\}$ containing $\see_1, ..., \see_n$. (It is necessary to distinguish between lists and sets because sets have no ordering).
\end{theorem}

Now, we can restate the characterizing property of standard matrices concisely.

\begin{theorem}
    (Characterizing property of standard matrices).
    \label{ch::lin_alg::thm::characterizing_property_of_standard_matrix}
    
    Let $K$ be a field and let $\ff:K^n \rightarrow K^m$ be a linear function. The standard matrix $\ff(\sE)$ of $\ff$ is the unique matrix satisfying the following characterizing property:
    
    \begin{align*}
        \boxed{
            \ff(\sE) [\vv]_E = \ff(\vv) \text{ for all $\vv \in K^n$}
        }
    \end{align*}
    
    (The left side of the above equation is a matrix-vector product).
\end{theorem}

\subsection*{Composition of linear functions}

In the previous section, we discovered how the action of a linear function on a vector corresponded to an operation involving the linear function's matrix and the vector. In this section, we discover an operation involving two linear functions' matrices. The operation arises from the fact that a composition of functions represented by matrices produces yet another function represented by a matrix.

\begin{theorem}
    (A composition of linear functions is also linear).
    
    Let $V, W$, and $Y$ be vector spaces over the same field. If $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ are linear functions, then the composition $\gg \circ \ff$ is also a linear function.
\end{theorem}

\begin{proof}
   We have $(\gg \circ \ff)(\vv + \ww) = \gg(\ff(\vv + \ww)) = \gg(\ff(\vv) + \ff(\ww)) = \gg(\ff(\vv)) + \gg(\ff(\ww)) = (\gg \circ \ff)(\vv) + (\gg \circ \ff)(\ww)$ for all $\vv, \ww \in V$ and $(\gg \circ \ff)(c\vv) = \gg(\ff(c\vv)) = \gg(c\ff(\vv)) = c\gg(\ff(\vv)) = c (\gg \circ \ff)(\vv)$ for all $\vv \in V$ and $c \in K$.
\end{proof}

Now, we answer the question: ``how is the standard matrix of $\gg \circ \ff$ related to the standard matrices of $\gg$ and $\ff$?''.

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}
    (Standard matrix of a composition of linear functions, matrix-matrix product). 
    
    Let $K$ be a field, and consider linear functions $\ff:K^n \rightarrow K^m$ and $\gg:K^m \rightarrow K^p$. Additionally, let $\sE = \{\ee_1, ..., \ee_n\}$ be the standard basis for $K^n$, $\sF = \{\see_1, ..., \see_m\}$ be the standard basis for $K^m$, and $\sG = \{\see_1, ..., \see_p\}$ be the standard basis for $K^p$.
    
    Since $\gg \circ \ff$ is a linear function $K^n \rightarrow K^p$, it has a standard matrix, $(\gg \circ \ff)(\sE)$:
    
    \begin{align*}
        (\gg \circ \ff)(\sE)
        &=
        \begin{pmatrix}
            (\gg \circ \ff)(\see_1) & \hdots & (\gg \circ \ff)(\see_n)
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            \gg(\ff(\see_1)) & \hdots & \gg(\ff(\see_n))
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            \gg(\sF) \ff(\see_1) & \hdots & \gg(\sF) \ff(\see_n)
        \end{pmatrix}.
    \end{align*}
    
    Recalling that $\ff(\see_i)$ is the $i$th column $(\ff(\sE))_i$ of $\ff(\sE)$, the standard matrix of $\ff$, we see that
    
    \begin{align*}
        (\gg \circ \ff)(\sE) = 
        \begin{pmatrix} 
            \gg(\sF) (\ff(\sE))_1 & \hdots & \gg(\sF) (\ff(\sE))_n
        \end{pmatrix}.
    \end{align*}
    
    Thus, the standard matrix $(\gg \circ \ff)(\sE)$ depends on the standard matrices $\ff(\sE)$ and $\gg(\sE)$. In other words, we have discovered that there is a function $\PP$ that takes the standard matrices $\ff(\sE)$ and $\gg(\sF)$ as input and returns the standard matrix $(\gg \circ \ff)(\sE)$ as output:
    
    \begin{align*}
        \PP(\gg(\sF), \ff(\sE)) := \begin{pmatrix} 
            \gg(\sF) (\ff(\sE))_1 & \hdots & \gg(\sF) (\ff(\sE))_n
        \end{pmatrix}.
    \end{align*}
    
    Since $\ff(\sE)$ varies over $K^{m \times n}$ and $\gg(\sF)$ varies over $K^{m \times p}$, we can restate the action of $\PP$ in terms of arbitrary matrices $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix} \in K^{m \times n}$ and $\BB \in K^{m \times p}$:
    
    \begin{align*}
        \PP \Bigg(\BB, \underbrace{\begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}}_\AA \Bigg) := 
        \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}.
    \end{align*}
    
    Just as was the case with the matrix-vector product $\pp$, no one actually uses the letter $\PP$ when notating matrix-matrix products. Instead, we establish the convention that writing two matrices of compatible sizes next to each other indicates the evaluation of the corresponding matrix-matrix product. That is, given an $m \times n$ matrix $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}$ and a $p \times m$ matrix $\BB$, we define the $p \times n$ matrix $\BB \AA$ as follows:

    \begin{align*}
        \boxed
        {
            \BB \AA 
            =
            \BB
            \underbrace
            {\begin{pmatrix} 
                \aa_1 & \hdots & \aa_n
            \end{pmatrix}}_\AA
            := \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}
        }
    \end{align*}
    
    With the above definition, the standard matrix $(\gg \circ \ff)(\sE)$ of $\gg \circ \ff$ relative to $\sE$ is expressed as the matrix-matrix product $\gg(\sF) \spc \ff(\sE)$:
    
    \begin{align*}
        (\gg \circ \ff)(\sE) = \gg(\sF) \spc \ff(\sE).
    \end{align*}
\end{deriv}

\begin{remark}
    (Compatibility of matrices for matrix-matrix products). 
    
    We now expand on what was meant when we said the matrix-matrix product $\BB \AA$ is only defined when the sizes of $\AA$ and $\BB$ are ``compatible''.
    
    Consider that the composition $\gg \circ \ff$ of linear functions $\ff$ and $\gg$ is only defined when the output space of $\ff$ is the entire input space of $\gg$, i.e., when the dimension of $\ff$'s output is the same as the dimension of $\gg$'s input. Because of this, the matrix-matrix product $\BB \AA$ of an $m \times n$ matrix with an $r \times s$ matrix $\BB$ is only defined when $r = n$, i.e., when $\BB$ has as many columns as $\AA$ has rows.

    Additionally, when $\AA$ is $m \times n$ and $\BB$ is $p \times m$, then $\AA$ represents a linear function $K^n \rightarrow K^m$ and $\BB$ represents a linear function $K^m \rightarrow K^p$, so $\BB \AA$ represents a linear function $K^n \rightarrow K^p$. Thus when $\AA$ is $m \times n$ and $\BB$ is $p \times m$, we know $\BB \AA$ must be $p \times n$.
\end{remark}

\begin{remark}
    Expanding out the columns of $\BB \AA$, here is what the matrix-matrix product $\BB \AA$ looks like when written out more explicitly:
    
    \begin{align*}
        \underbrace
        {\begin{pmatrix} 
            b_{11} & \hdots & b_{1m} \\
            \vdots & \hdots & \vdots \\
            b_{p1} & \hdots & b_{pm}
        \end{pmatrix}}_\BB
        \underbrace
        {\begin{pmatrix} 
            a_{11} & \hdots & a_{1n} \\
            \vdots & \hdots & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}}_\AA
        &:=
        \begin{pmatrix}
            \underbrace
            {\begin{pmatrix} 
                b_{11} & \hdots & b_{1m} \\
                \vdots & \hdots & \vdots \\
                b_{p1} & \hdots & b_{pm}
            \end{pmatrix}}_\BB
            \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}
            &
            \hdots
            &
            \underbrace
            {\begin{pmatrix} 
                b_{11} & \hdots & b_{1m} \\
                \vdots & \hdots & \vdots \\
                b_{p1} & \hdots & b_{pm}
            \end{pmatrix}}_\BB
            \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}
        \end{pmatrix} \\
        &=
        \begin{pmatrix}
            b_{11} a_{11} ... b_{1m} a_{m1} &
            \hdots
            &
            b_{11} a_{1n} + ... + b_{1m} a_{mn}
            \\
            \vdots & & \vdots \\
            \\
            b_{p1} a_{11} ... b_{pm} a_{m1} &
            \hdots & b_{p1} a_{1n} + ... + b_{pm} a_{mn}
        \end{pmatrix}.
    \end{align*}
    
    Don't try to make too much sense of this now. Theorem \ref{ch::lin_alg::thm::coordinates_of_matrix_matrix_product} makes thinking about entries of matrix-matrix products much more tractable.
\end{remark}

\newpage

\subsection*{Matrices}

The previous section showed us that matrices are important because they can be used to represent linear functions. We now provide explanations for how ideas about linear functions translate over to results ideas about matrices.

First, we restate the earlier hasty definition of ``matrix''; in this definition we also define some new notation for matrix entries and for the set of all $m \times n$ matrices over a field.

\begin{defn}
    (Matrix).
    
    Let $K$ be a field. An \textit{$m \times n$ matrix (with entries in $K$)} is a ``grid'' of elements of $K$ with $m$ rows and $n$ columns. For example, the following is a matrix with three rows and two columns that has entries in $\R$:
    
    \begin{align*}
        \begin{pmatrix}
            -1 & \frac{3}{4} \\
            \pi & 0 \\
            5 & -11
        \end{pmatrix}.
    \end{align*}
    
    The entry in the $i$th row and $j$th column of a matrix is called the \textit{$ij$ entry} of that matrix. (For example, the above matrix has a $21$ entry of $\pi$). Specifying matrices by describing their $ij$th entry is relatively common. We write ``$\AA = (a_{ij})$'' iff $\AA$ is the matrix with $ij$ entry $a_{ij}$.
    
    We also define $K^{m \times n}$ to be the set of $m \times n$ matrices with entries in $K$.
\end{defn}

\begin{defn}
    (Identity matrix).
    
    Let $K$ be a field, and consider the identity function on $K^n$, which is the function $\II_{K_n}:K^n \rightarrow K^n$ defined by $\II_{K^n}(\vv) = \vv$. The standard matrix of $\II_{K^n}$ relative to the standard basis $\sE$ is called the \textit{($n \times n$) identity matrix}. Notice that since the $i$th column of the identity matrix is $\II_{K_n}(\see_i) = \see_i$, it follows that the identity matrix has a diagonal of $1$'s, with $0$'s everywhere else; its $ij$ entry is $1$ if $i = j$ and $0$ if $i \neq j$. 
    
    The $3 \times 3$ identity matrix, for example, is
    
    \begin{align*}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{pmatrix}.
    \end{align*}
    
    We denote the identity matrix by $\II$ rather than by the more verbose notation $\II_{K^n}(\sE)$, and infer $n$ from context.
\end{defn}

\subsubsection{Matrix-vector products}

Now we return to the relationship between linear functions and matrices. This relationship hinges on the matrix-vector product, which we revisit. We first restate the definition of the matrix-vector product from Derivation \ref{ch::lin_alg::deriv::matrix_vector_product} for convenience.

\begin{defn}
    (Matrix-vector product).

    Let $K$ be a field. Given $\AA \in K^{m \times n}$ with $i$th column $\aa_i$ and $\vv \in K^n$ with $i$th entry $v_i$, we define the \textit{matrix-vector product} $\AA \vv$ to be the following:

    \begin{align*}
        \AA \vv =
        \underbrace
        {\begin{pmatrix} 
            \aa_1 & \hdots & \aa_n
        \end{pmatrix}}_\AA
        \underbrace{
        \begin{pmatrix} 
            v_1 \\ \vdots \\ v_n 
        \end{pmatrix}}_\vv
        :=
        v_1 \aa_1 + ... + v_n \aa_n.
    \end{align*}
\end{defn}

Now we present several new theorems.

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_vector_product}
    ($i$th entry of matrix-vector product).
    
        Let $\AA = (a_{ij})$ be an $m \times n$ matrix with entries in a field $K$ and let $\vv = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \in K^n$ be a column vector. By the previous definition, the matrix-vector product $\AA \vv$ is equal to the following:
    
    \begin{align*}
            \AA \vv = 
            \begin{pmatrix}
                a_{11} & \hdots & a_{1n} \\
                \vdots & & \vdots \\
                a_{i1} & \hdots & a_{in} \\
                \vdots & & \vdots \\
                a_{m1} & \hdots & a_{mn}
            \end{pmatrix}
            \begin{pmatrix} v_1 \\ \vdots \\ \vdots \\ \vdots \\ v_n \end{pmatrix}
            =
            v_1
            \begin{pmatrix} a_{11} \\ \vdots \\ a_{i1} \\ \vdots \\ a_{m1} \end{pmatrix}
            +
            ...
            +
            v_n
            \begin{pmatrix} a_{1n} \\ \vdots \\ a_{in} \\ \vdots \\ a_{mn} \end{pmatrix}
            =
            \begin{pmatrix} v_1 a_{11} + ... + v_n a_{1n} \\ \vdots \\ v_1 a_{i1} + ... + v_n a_{in} \\ \vdots \\ v_1 a_{m1} + ... + v_n a_{mn} \end{pmatrix}.
    \end{align*}

    Therefore, the $i$th entry $(\AA \vv)_i$ of $\AA \vv$ is $v_i a_{i1} + ... + v_n a_{in} $, which is
    
    \begin{align*}
        \boxed
        {
            (\AA \vv)_i = (\text{$i$th row of $\AA$}) \cdot \vv
        }
    \end{align*}

    
    Here $\cdot:K^n \times K^n \rightarrow K$ denotes the \textit{dot product}\footnote{We discuss the dot product in-depth later in this chapter.} of vectors in $K^n$, defined by
    
    \begin{align*}
        \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}
        \cdot
        \begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix}
        =
        v_1 w_1 + ... + v_n w_n.
    \end{align*}

    Notice how since the dot product must take two column vectors as input, the above boxed formula uses the transposed $i$th row of $\AA$, which is a column vector, instead of the ``unmodified'' $i$th row of $\AA$.
\end{theorem}

\begin{theorem}
    (Linear functions from matrices).
    
    If $\AA$ is an $m \times n$ matrix with entries in a field $K$, then the function $\vv \mapsto \AA \vv$ is linear, and $\AA$ is the standard matrix of this function. We define $\ker(\AA) := \ker(\vv \mapsto \AA \vv)$ and $\im(\AA) := \im(\vv \mapsto \AA \vv)$.
\end{theorem}

\begin{proof}
    Define $\ff$ by $\ff(\vv) = \AA \vv$. We will show that $\ff$ is linear and that $\AA = \ff(\sE)$.

    Because of Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}, we can interpret the $i$th column $\aa_i$ of $\AA$ to be $\gg(\see_i)$ for some linear function $\gg$. Thus $\AA = \gg(\sE)$, and so $\ff(\vv) = \AA \vv = \gg(\sE) \vv = \gg(\sE) [\vv]_\sE$. The characterizing property of standard matrices says that $\gg(\sE) [\vv]_\sE = \gg(\vv)$, so we have $\ff(\vv) = \gg(\vv)$ for all $\vv \in K^n$, i.e. $\ff = \gg$. Since $\gg$ is linear, $\ff$ is also linear.
    
    The standard matrix $\ff(\sE)$ of $\ff$ has $i$th column $\ff(\see_i) = \AA \see_i$. Compute the matrix-vector product $\AA \see_i$ to verify that $\AA \see_i = \aa_i$, where $\aa_i$ is the $i$th column of $\AA$. This tells us that $\ff(\see_i) = \aa_i$, which means $\ff(\sE) = \AA$.
    
    %Since we have $\AA \vv = \ff(\vv)$ and $\ff(\sE) \vv = \ff(\vv)$, the uniqueness of standard matrices implies that $\AA = \ff(\sE)$. \textbf{Now we have to show that $\ff(\sE) = \gg(\sE)$, where $\gg(\vv) = \AA \vv$}.
\end{proof}

\begin{theorem}
    (Properties of the matrix-vector product).
    
    Practically speaking, the linearity of the function $\vv \mapsto \AA \vv$ translates into properties of the matrix-vector product: we have $\AA(\vv + \ww) = \AA \vv + \AA \ww$ and $\AA(c\vv) = c(\AA \vv)$ for any matrix $\AA$, column vectors $\vv$ and $\ww$, and scalar $c$. 
\end{theorem}

\subsubsection{Matrix-matrix products}

We also revisit the matrix-matrix product for convenience. Similarly to as was with matrix-vector products, we restate the definition of matrix-matrix product from Definition \ref{lin_alg::thm::matrix_matrix_product_relative_to_bases_standard} for convenience.

\begin{defn}
    (Matrix-matrix product).

    Let $K$ be a field. Given $\AA \in K^{m \times n}$ with $i$th column $\aa_i$ and $\BB \in K^{p \times m}$, we define the \textit{matrix-matrix product} $\BB \AA \in K^{p \times n}$ to be the following:

    \begin{align*}
        \BB \AA = \BB \underbrace{\begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}}_\AA := 
        \begin{pmatrix} \BB \aa_1 & \hdots & \BB \aa_n \end{pmatrix}.
    \end{align*}
\end{defn}

Now, we present several new concepts regarding matrix-matrix products.

\begin{theorem}
\label{ch::lin_alg::thm::coordinates_of_matrix_matrix_product}

    ($ij$ entry of matrix-matrix product). 
    
    Let $K$ be a field, let $\AA = (a_{ij}) \in K^{m \times n}$, and let $\BB = (b_{ij}) \in K^{m \times p}$. The $ij$ entry of the matrix-matrix product $\BB \AA$ can be computed by using the definition of the matrix-matrix product (Theorem \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}) together with the fact that the $i$th entry of the matrix-vector product $\AA \vv$ is $(\text{$i$th row of $\AA$}) \cdot \vv$, where $\cdot$ is the dot product. We have
    
    \begin{align*}
        \BB \AA
        = 
        \BB
        \begin{pmatrix}
            \aa_1 & \hdots & \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \BB \aa_1 & \hdots & \BB \aa_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \bb_1 \cdot \aa_1 & \hdots & \bb_1 \cdot \aa_n \\
            \vdots & & \vdots \\
            \bb_m \cdot \aa_1 & \hdots & \bb_m \cdot \aa_n
        \end{pmatrix},
    \end{align*}
    
    where $\aa_i$ is the $i$th column of $\AA$ and $\bb_i$ is the $i$th row of $\BB$. So the $ij$ entry $(\BB \AA)_{ij}$ of $\BB \AA$ is $\bb_i \cdot \aa_j$, which is
    
    \begin{align*}
        \boxed
        {
            (\BB \AA)_{ij} = (\text{$i$th row of $\BB$}) \cdot (\text{$j$th column of $\AA$})
        }
    \end{align*}
    
    Similarly to in Theorem \ref{ch::lin_alg::thm::coordinates_of_matrix_vector_product}, we have to transpose the $i$th row of $\BB$ so that it is a column vector.
\end{theorem}

\begin{remark}
    (Matrix-matrix products are associative).
    
    One would expect that $(\BB \AA) \vv = \BB (\AA \vv)$ for all column vectors $\vv$ when $\AA$ and $\BB$ are ``compatible'' matrices. This is indeed true because the corresponding linear functions $\ff$ and $\gg$ satisfy $(\gg \circ \ff)(\vv) = \gg(\ff(\vv))$.
\end{remark}

\begin{defn}
    (Inverse matrix).
    
    Let $\AA$ be an $m \times n$ matrix with entries in a field $K$. We say that $\AA$ is \textit{invertible} iff there exists a matrix $\AA^{-1}$ such that $\AA^{-1} \AA = \II = \AA \AA^{-1}$.
\end{defn}

\begin{theorem}
    (Inverse matrix).

    Let $\AA$ be an invertible $m \times n$ matrix with entries in a field $K$, and consider the linear function $\ff:K^m \rightarrow K^n$ defined by $\ff(\vv) = \AA \vv$. Because $\ff^{-1} \circ \ff = \II = \ff \circ \ff^{-1}$, it follows that $\AA = \ff(\sE)$ is invertible iff $\ff$ is, and that when $\AA^{-1}$ exists, it is equal to the standard matrix of $\ff^{-1}$. In other words, if $\ff:K^n \rightarrow K^n$ is an invertible linear function, then $\ff(\sE)^{-1} := \ff^{-1}(\sE)$.
    
    Since it is only possible for $\ff$ to be invertible when $n = m$, it follows that $\AA$ must be a $n \times n$ matrix, or \textit{square matrix}, in order to be invertible.
\end{theorem}

\begin{remark}
    (Matrix pedagogy). 
    
    Most linear algebra texts present the relationship between linear functions and matrices in the following way: first define matrices in the context of systems of linear equations (we have not seen how matrices are related to systems of linear equations), then define a linear function to be one for which $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ and $\ff(c\vv) = c\ff(\vv)$ for all vectors $\vv, \ww$ and scalars $c$, and then prove that each linear function has a standard matrix. This is bad pedagogy; there should be no need to conjecture and prove that a matrix-vector product corresponds to the action of a linear function, because this fact is apparent from natural investigations (such as Derivation \ref{ch::lin_alg::deriv::standard_matrix}). (Furthermore, while systems of linear equations are an important application of linear algebra, and while their study does enhance our knowledge about the kernels of linear functions, they should not be the starting point).
    
    Oftentimes, linear algebra texts present the formula for the $i$th entry of a matrix-vector product and the formula for the $ij$ entry of a matrix-matrix product as facts that should be memorized rather than understood. Be wary of this! You \textit{should not} memorize these formulas. If you can't quite remember them, try to derive them by starting with the fact that linear functions on finite-dimensional vector spaces are determined by what they do to bases, and by following the derivations given in this book!
\end{remark}

\subsubsection{Matrix transposes}

The topic of matrix tranposes does not become theoretically significant until Chapter \ref{ch::bilinear_forms_metric_tensors}. We present some surface-level details now, though, because they do appear occasionally before Chapter \ref{ch::bilinear_forms_metric_tensors}.

\begin{defn}
    (Transpose of a matrix).

    Let $\AA = (a_{ij})$ be an $m \times n$ matrix with entries in a field $K$. The \textit{transpose $\AA^\top$ of $\AA$} is the $n \times m$ matrix whose $ij$ entry is the $ji$ entry of $\AA$: $\AA^\top := (a_{ji})$. 
    
    We also use the notation $a_{ij}^\top$ to denote the $ij$ entry of $(a_{ij})^\top$.

    For any matrix $\AA$, the columns of $\AA^\top$ are the rows of $\AA$, and the rows of $\AA^\top$ are the columns of $\AA$.
\end{defn}

\begin{remark}
    (Concise column vectors).

    Since

    \begin{align*}
        (v_1, ..., v_n)^\top =
        \begin{pmatrix}
            v_1 \\ \vdots \\ v_n
        \end{pmatrix} 
        \text{ for all $v_1, ..., v_n \in K^n$}
    \end{align*}

    we often find it convenient to use $(v_1, ..., v_n)^\top$ to save vertical writing space.
\end{remark}

\begin{theorem}
    (Properties of the transpose).

    \begin{enumerate}
        \item $\II^\top = \II$.
        \item $(\AA^\top)^\top = \AA$ for all matrices $\AA$.
        \item $\AA \mapsto \AA^\top$ is linear for all matrices $\AA$.
        \item $(\BB \AA)^\top = \AA^\top \BB^\top$ for all matrices $\AA, \BB$ compatible with each other for matrix multiplication.
        \item $(\AA^{\top})^{-1} = (\AA^{-1})^\top$ for all invertible matrices $\AA$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\    
    \begin{enumerate}
        \item This is true because $\delta_{ij} = \delta_{ji}$.
        \item $((a_{ij})^\top)^\top = (a_{ij}^\top)^\top = (a_{ji})^\top = (a_{ji}^\top) = (a_{ij})$.
        \item We must show that $(\AA + \BB)^\top = \AA^\top + \BB^\top$ for all matrices $\AA$ and $\BB$ and that $(c\AA)^\top = c\AA^\top$ for all matrices $\AA$ and scalars $c$.

        We have $((a_{ij}) + (b_{ij}))^\top = (a_{ij} + b_{ij})^\top = (a_{ji} + b_{ji}) = (a_{ji}) + (b_{ji}) = (a_{ij})^\top + (b_{ij})^\top$, so the first property holds.

        We also have $(c(a_{ij}))^\top = (c a_{ij})^\top = (c a_{ji}) = c (a_{ji}) = c (a_{ij})^\top$, so the second property holds as well.
        
        \item $((b_{ij}) (a_{ij}))^\top = ( \sum_k b_{ik}a_{kj})^\top = (\sum_k b_{jk} a_{ki}) = (\sum_k a_{ik}^\top b_{kj}^\top) = (a_{ij}^\top) (b_{ij}^\top) = (a_{ij})^\top (b_{ij})^\top$.
        
        \item We need to show that $\AA^\top (\AA^{-1})^\top = \II$. By property (2), this equation is equivalent to $\AA^{-1} \AA = \II$, which is true.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Inverse transpose).

    Let $\AA$ be a matrix. Because the matrix inversion and transposition operations commute, it is unambiguous to define $\AA^{-\top} := (\AA^{\top})^{-1} = (\AA^{-1})^\top$.
\end{defn}

\newpage

\subsection*{Matrices relative to bases}

We started this section- ``Coordinatization of linear functions with matrices''- by noting that if $V$ and $W$ are $n$- and $m$-dimensional vector spaces, we can effectively study linear functions $V \rightarrow W$ by studying linear functions $K^n \rightarrow K^m$. Then, we showed that because a linear function $K^n \rightarrow K^m$ is determined by how it acts on the standard basis $\sE = \{\see_1, ..., \see_n\}$ of $K^n$, any linear function $\ff:K^n \rightarrow K^m$ is represented by the matrix $\ff(\sE)$. We now extend this result so that it is applicable for a linear function $V \rightarrow W$, where $V$ and $W$ are arbitrary finite-dimensional vector spaces.

\begin{lemma}
    \label{ch::lin_alg::lemma::basis_inverse}
    ($[\cdot]_E^{-1}(\sE) = E$).

    Let $V$ be a vector space and let $E$ be a basis of $V$. We have $[\cdot]_E^{-1}(\sE) = E$. 
\end{lemma}

\begin{proof}
    Since $[\ee_i]_E = \see_i$, then $[\cdot]_E^{-1}(\see_i) = \ee_i$, and so $[\cdot]_E^{-1}(\sE) = E$. 
\end{proof}

Notice that $E$ is not a matrix in the above lemma! It is a list of vectors that are not necessarily column vectors. We investigate the special case when $E$ is indeed a list of column vectors (i.e. a list of vectors from $K^n$) in Theorem \ref{ch::lin_alg::thm::basis_change_from_standard_vectors}.

\begin{deriv}
\label{ch::lin_alg::deriv::matrix_relative_to_bases}
    (Matrix relative to bases).
    
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $K$ with bases $E$ and $F$, and consider a linear function $\ff:V \rightarrow W$. 
    
    The standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ represents $\ff_{E,F}$, and $\ff_{E,F}$ represents $\ff$, so\footnote{Again, when we say that $\hh$ \textit{represents} $\gg$, we mean that the map sending $\gg \mapsto \hh$ is a bijection.} we conclude by transitivity that $\ff_{E,F}(\sE)$ represents $\ff$. For this reason, we define the \textit{matrix of $\ff$ relative to $E$ and $F$} to be the standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F}$.
    
    Now, let's investigate $\ff_{E,F}(\sE)$ itself. We have $\ff_{E,F}(\sE) = ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1})(\sE) = \Big[\ff\Big([\cdot]_E^{-1}(\sE)\Big)\Big]_F$. By the previous lemma, we have $[\cdot]_E^{-1}(\sE) = E$, so
    
    \begin{align*}
        \boxed
        {
            \ff_{E,F}(\sE) = [\ff(E)]_F
        }
    \end{align*}
    
    Thus, the matrix of a linear function $\ff:V \rightarrow W$ relative to the bases $E$ and $F$ for $V$ and $W$ is $[\ff(E)]_F$. Explicitly, $[\ff(E)]_F$ looks like this:
    
    \begin{align*}
        [\ff(E)]_F =
        \begin{pmatrix}
            [\ff(\ee_1)]_F & \hdots & [\ff(\ee_n)]_F
        \end{pmatrix}
    \end{align*}
    
    The characterizing property of standard matrices for linear functions $K^n \rightarrow K^m$ gives us the following equivalent statements:
    
    \begin{align*}
        \ff_{E,F}(\vv) &= \ff_{E,F}(\sE) \spc \vv \text{ for all $\vv \in K^n$} \\
        ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1})(\vv) &= [\ff(E)]_F \spc \vv \text{ for all $\vv \in K^n$} \\
        ([\cdot]_F \circ \ff)(\vv_1) &= [\ff(E)]_F [\vv_1]_E \text{ for all $\vv_1 \in V$} \\
        [\ff(\vv_1)]_F &= [\ff(E)]_F [\vv_1]_E \text{ for all $\vv_1 \in V$}
    \end{align*}
    
    So, we have the following characterizing property for matrices relative to bases:
    
    \begin{align*}
        \boxed
        {
            [\ff(\vv)]_F = [\ff(E)]_F [\vv]_E \text{ for all $\vv \in V$}
        }
    \end{align*}
    
    This characterizing property tells a similar story as does the commutative diagram that describes $\ff_{E,F}$: it says that we can think of the function $\uu \mapsto [\ff(E)]_F \uu$ as accepting an input that is expressed relative to the basis $E$ for $V$ and as producing an output that is expressed relative to the basis $F$ for $W$.
\end{deriv}

\begin{remark}
\label{ch::lin_alg::rmk::standard_matrix_as_matrix_wrt_bases}
    (Standard matrix as special case of matrix relative to bases). 
    
    The standard matrix $\ff(\sE)$ of a linear function $\ff:K^n \rightarrow K^m$ is the matrix $\ff(\sE) = [\ff(\sE)]_\sE$ of $\ff:K^n \rightarrow K^m$ relative to the bases $\sE$ and $\sE$.
\end{remark}

\newpage

\subsection*{Vectors relative to bases}

We will now discover how when we have a finite-dimensional vector space with bases $E$ and $F$ we can relate $[\vv]_E$ to $[\vv]_F$ for any vector $\vv$.

\begin{theorem}
    \label{ch::lin_alg::thm::change_of_basis_for_vectors}
    
    (Change of basis for vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E = \{\ee_1, ..., \ee_n\}$ and $F = \{\ff_1, ..., \ff_n\}$. 
    
    The characterizing property of matrices relative to $E$ and $F$ says that for any linear function $\ff:V \rightarrow V$, we have $[\ff(\vv)]_F = [\ff(E)]_F [\vv]_E$. In particular, when $\ff$ is the identity on $V$, we obtain
    
    \begin{align*}
        \boxed
        {
            [\vv]_F = [\EE]_F [\vv]_E
        }
    \end{align*}
    
    where $\EE$ is the list of vectors whose $i$th entry is $\ee_i$, so that $[\EE]_F$ is the matrix whose $i$th column is $[\ee_i]_F$.
    
    It is a good sanity check that the identity on $V$ is involved in changing bases, since representing a vector with different bases does not change the vector itself.
\end{theorem}

\begin{remark}
    (Alternate proof of change of basis for vectors).
    
    Here is an alternate proof to the above. The matrix of $[\cdot]_F$ relative to $E$ and $\sE$ is $[[\cdot]_F(E)]_\sE = [\cdot]_F(E) = [\EE]_F$. Using the characterizing property of matrices relative to bases, we have $[\vv]_F = [[\cdot]_F(E)]_\sE [\vv]_E = [\EE]_F [\vv]_E$.
\end{remark}

The following is an important special case\footnote{Use the substitutions $E := \sE$ and $F := E$ in the change of basis theorem for vectors to obtain $[\vv]_E = [\sE]_E [\vv]_\sE = \EE^{-1} \vv$.} of the change of basis theorem for vectors.

\begin{theorem}
    (Change of basis from standard basis for vectors).
    \label{ch::lin_alg::thm::basis_change_from_standard_vectors}

    Let $V$ be a vector space and let $E$ be a basis of $V$. Then from the definition of $[\cdot]_E$ (recall Definition \ref{ch::lin_alg::defn::coordinates_relative_to_basis}) it immediately follows that

    \begin{align*}
        \boxed
        {
            \vv = \EE [\vv]_E \text{ for all $\vv \in V$}
        }
    \end{align*}

    When $V = K^n$, then the list $E$ becomes a matrix of column vectors $\EE$, and the standard matrices of $[\cdot]_E$ and $[\cdot]_E^{-1}$ exist. Using the result of Lemma \ref{ch::lin_alg::lemma::basis_inverse}, we see they are:

    \begin{align*}
        [\cdot]_E^{-1}(\sE) = \EE \text{ and } [\cdot]_E(\sE) = \EE^{-1}
    \end{align*}

    Importantly, we see that $\EE^{-1}$ exists. We can now use $\EE^{-1}$ to solve the equation $\vv = \EE[\vv]_E$ for $[\vv]_E$:

    \begin{align*}
        \boxed
        {
            [\vv]_E = \EE^{-1} \vv \text{ for all $\vv \in K^n$}
        }
    \end{align*}
\end{theorem}

\begin{theorem}
\label{ch::lin_alg::thm::I_EF}
    ($\II_{E,F}^{-1} = \II_{F,E}$ when $V = K^n$).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$. The identity function $\II:V \rightarrow V$ on $V$ satisfies $\II_{E,F}^{-1} = \II_{F,E}$. As a corollary, when $V = K^n$, then $E$ and $F$ are both lists of column vectors, so we have $[\EE]_F^{-1} = [\FF]_E$.
\end{theorem}

\begin{proof}
    Since $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$, then $\II_{E,F} = [\cdot]_F \circ [\cdot]_E^{-1}$ and $\II_{F,E} = [\cdot]_E \circ [\cdot]_F^{-1}$. We clearly have $\II_{E,F}^{-1} = \II_{F,E}$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::change_of_basis_with_basis_vectors}
    (Change of basis in terms of basis vectors).
    
    Let $V$ be a finite-dimensional vector space with bases $E$ and $F$. By the definition of $[\cdot]_F$, we have
    
    \begin{align*}
        \ee_i = \sum_{j = 1}^n ([\ee_i]_F)_j \ff_j = \sum_{j = 1}^n ([\EE]_F)_{ji} \ff_j,
    \end{align*}
    
    where $\EE$ is the matrix whose $i$th column is $\ee_i$.
    
    In the last equality, we have used that $[\ee_i]_F$ is the $i$th column of $[\EE]_F$.
\end{theorem}

\begin{remark}
    (On the order of proving change of basis theorems). 
    
    Most linear algebra texts first prove the previous theorem and use it to show a version of the first equation in the box of Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors}. This approach for proving Theorem \ref{ch::lin_alg::thm::change_of_basis_for_vectors} was not used because it involves quite a bit of unmemorable matrix algebra. It's good to know that these theorems are equivalent, though.
\end{remark}

\newpage

\subsection*{Matrix similarity}

Not all linear algebra resources use the above notion of matrices relative to bases and instead investigate matrices of the form $\EE^{-1} \AA \EE$. The following theorem explains that such matrices arise as a special case of matrices relative to bases.

\begin{theorem}
    (Matrices of linear functions $K^n \rightarrow K^n$ relative to bases).
    
    Let $K$ be a field, consider the vector space $K^n$, let $E$ and $F$ be bases for $K^n$, and consider a linear function $\ff:K^n \rightarrow K^n$. The matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$ is
    
    \begin{align*}
        [\ff(E)]_F = \FF^{-1} \ff(\sE) \EE.
    \end{align*}

    An important special case of this theorem is that the matrix of $\ff$ relative to $E$ and $E$ is $\EE^{-1} \ff(\sE) \EE$.
\end{theorem}

\begin{proof}
   The matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$ is the standard matrix $\ff_{E,F}(\sE)$ of $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$, which is $\ff_{E,F}(\sE) = [\cdot]_F(\sE) \spc \ff(\sE) \spc [\cdot]_E^{-1}(\sE)$. Since $F$ is a list of column vectors, then the corollary of Theorem \ref{ch::lin_alg::thm::I_EF} applies, and $[\cdot]_F(\sE) = [\sE]_F$ is equal to $[\FF]_\sE^{-1} = \FF^{-1}$. Also, because $E$ is a list of column vectors, we can apply Theorem \ref{ch::lin_alg::thm::basis_change_from_standard_vectors}, which says $[\cdot]_E^{-1}(\sE) = \EE$. So the most recent expression is equal to $\FF^{-1} \ff(\sE) \EE$, as claimed.
\end{proof}

\begin{remark}
    (Matrix similarity).

    Most linear algebra texts do not define the matrix of a linear function $V \rightarrow W$ relative to bases $E$ and $F$, and only define the ``matrix of a linear function $\ff:K^n \rightarrow K^n$ relative to the basis $E$ of $K^n$'' to be $\EE^{-1} \ff(\sE) \EE$.

    In such texts, one of two definitions of \textit{matrix similarity} is given:

    \begin{enumerate}
        \item Matrices $\AA$ and $\BB$ are \textit{similar} iff there exists a matrix $\EE$ such that $\BB = \EE^{-1} \AA \EE$.
        \item Matrices $\AA$ and $\BB$ are \textit{similar} iff there exists a matrix $\EE$ such that $\BB = \EE \AA \EE^{-1}$.
    \end{enumerate}
    
    The definitions are equivalent because matrices of the form $\EE^{-1} \AA \EE$ are also of the form $\EE \AA \EE^{-1}$.
    
    It is much better to use the first definition than the second one, though, since the placement of the matrix inverse in the second definition is different than it is in the formula $[\ff(E)]_E = \EE^{-1} \ff(\sE) \EE$. Using the second definition could lead one to think that ``$[\ff(E)]_E = \EE \ff(E) \EE^{-1}$'', which is not true!
\end{remark}

\newpage

\subsection*{Linear isomorphism between linear functions and matrices}

We've alluded to the correspondence between linear functions and matrices several times. Now that we know of matrices relative to bases, we can formalize our knowledge of this relationship.

\begin{theorem}
    \label{ch::lin_alg::thm::linear_functions_matrices_isomorphism}
    (Linear isomorphism between linear functions and matrices).
    
    Let $V, W$, and $Y$ be finite-dimensional vector spaces over a field $K$, with bases $E$, $F$, and $G$.

    The function $\FF:\LLLL(V \rightarrow Y) \rightarrow K^{\dim(Y) \times \dim(V)}$ defined by $\FF(\ff) := [\ff(E)]_G$ is a linear isomorphism, and also satisfies $\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$ for all linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$.

    The inverse $\FF^{-1}:K^{\dim(Y) \times \dim(V)} \rightarrow \LLLL(V \rightarrow Y)$ is defined by $\FF^{-1}(\AA) = [\cdot]_G^{-1} \circ (\vv \mapsto \AA \vv) \circ [\cdot]_E$, and satisfies $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ are linear functions with $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$.
\end{theorem}

\begin{proof}   
    Notice that $\FF = \QQ \circ \PP$, where ${\PP:\LLLL(V \rightarrow Y) \rightarrow \LLLL(K^{\dim(V)} \rightarrow K^{\dim(Y)})}$ is defined by $\PP(\ff) := \ff_{E,G}$ and where ${\QQ:\LLLL(K^{\dim(V)} \rightarrow K^{\dim(Y)}) \rightarrow K^{\dim(Y) \times \dim(V)}}$ is defined by $\QQ(\gg) := \gg(\sE)$. (We have $(\QQ \circ \PP)(\ff) = \QQ(\PP(\ff)) = \QQ(\ff_{E,G}) = \ff_{E,G}(\sE) = [\ff(E)]_G$). 

    \begin{itemize}    
        \item ($\FF$ is linear). It's easy to check that $\ff \mapsto \ff(E)$ is a linear function. Once we know this, then since $\ff \mapsto \ff(E)$ and $[\cdot]_G$ are both linear functions, we see $\ff \mapsto \FF(\ff) = [\ff(E)]_G$ is a composition of linear functions, and is thus linear.
        
        \item ($\FF^{-1} = [\cdot]_G^{-1} \circ (\vv \mapsto \AA \vv) \circ [\cdot]_E$). To prove this, we first find $\QQ^{-1}$ and $\PP^{-1}$ and then use the fact that $\FF^{-1} = \PP^{-1} \circ \QQ^{-1}$.
                
        Since $\QQ(\gg)$ is the standard matrix of $\gg$, we claim that $\QQ^{-1}(\AA)$ is the linear function obtained from the matrix $\AA$: we claim $\QQ^{-1}(\AA) = (\vv \mapsto \AA \vv)$. We have $\QQ^{-1}(\QQ(\gg)) = \QQ^{-1}(\gg(\sE)) = (\vv \mapsto \gg(\sE) \vv) = \gg$ and $\QQ(\QQ^{-1}(\AA)) = \QQ(\vv \mapsto \AA \vv) = (\vv \mapsto \AA \vv)(\sE) = \AA$, so $\QQ^{-1}$ is indeed defined by $\QQ^{-1}(\AA) = (\vv \mapsto \AA \vv)$.

        Since we have $\PP(\ff) = \ff_{E,G} = [\cdot]_G \circ \ff \circ [\cdot]_E^{-1}$, then $\PP^{-1}(\gg) = [\cdot]_G^{-1} \circ \gg \circ [\cdot]_E$.

        Thus $\FF^{-1} = (\PP^{-1} \circ \QQ^{-1})(\AA) = [\cdot]_G^{-1} \circ (\vv \mapsto \AA \vv) \circ [\cdot]_E$.
        
        \item ($\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F$). We have $\FF(\gg \circ \ff) = (\QQ \circ \PP)(\gg \circ \ff) = \QQ(\PP(\gg \circ \ff)) = \QQ((\gg \circ \ff)_{E,G})$. Notice that $(\gg \circ \ff)_{E,G} = [\cdot]_G \circ \gg \circ \ff \circ [\cdot]_E^{-1} = ([\cdot]_G \circ \gg \circ [\cdot]_F^{-1}) \circ ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1}) = \gg_{F,G} \circ \ff_{E,F}$. Thus $\QQ((\gg \circ \ff)_{E,G}) = \QQ(\gg_{F,G} \circ \ff_{E,F})$. Since $\ff_{E,F}:K^{\dim(V)} \rightarrow K^{\dim(W)}$ and $\gg:K^{\dim(W)} \rightarrow K^{\dim(Y)}$, we can apply the result from the end of Derivation \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard} to conclude that the standard matrix of the composition is equal to the matrix-matrix product of the corresponding standard matrices; that is, $\QQ(\gg_{F,G} \circ \ff_{E,F}) = \gg_{F,G}(\sE) \ff_{E,F}(\sE) = [\gg(F)]_G [\ff(E)]_F$.

        \item ($\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$). We start with the proven fact ${\FF(\gg \circ \ff) = [\gg(F)]_G [\ff(E)]_F}$. Applying $\FF^{-1}$ to both sides, we obtain ${\gg \circ \ff = [\cdot]_G^{-1} \circ (\vv \mapsto [\gg(F)]_G [\ff(E)]_F \vv) \circ [\cdot]_E}$. Consider the matrix-matrix product that appears in the expression on the right side, $\gg_{F,G}(\sE) \ff_{E,F}(\sE)$. Again applying the result from the end of Derivation \ref{ch::lin_alg::deriv::matrix_matrix_product_relative_to_bases_standard}, we see that the matrix-matrix product of standard matrices $\gg_{F,G}(\sE) \ff_{E,F}(\sE)$ is equal to the standard matrix of the composition $\gg_{F,G} \circ \ff_{E,F}$; that is, \\ ${\gg_{F,G}(\sE) \ff_{E,F}(\sE) = (\gg_{F,G} \circ \ff_{E,F})(\sE)}$. Furthermore, we have
        ${\vv \mapsto (\gg_{F,G} \circ \ff_{E,F})(\sE) \vv = \gg_{F,G} \circ \ff_{E,F}}$. Thus $\gg \circ \ff = [\cdot]_G^{-1} \circ (\gg_{F,G} \circ \ff_{E,F}) \circ [\cdot]_E = [\cdot]_G^{-1} \circ ([\cdot]_G \circ \gg \circ [\cdot]_F^{-1}) \circ ([\cdot]_F \circ \ff \circ [\cdot]_E^{-1}) \circ [\cdot]_E^{-1} = \gg \circ \ff$.
        
        In all, we've shown that $\FF^{-1}([\gg(F)]_G [\ff(E)]_F) = \gg \circ \ff$ for all linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$. Recall from Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}that for any set of vectors, there exists a linear function sending basis vectors to those vectors. It follows that for any matrices $\AA$ and $\BB$ there exist linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Y$ such that $\AA = [\ff(E)]_F$ and $\BB = [\gg(F)]_G$. We can apply the equation $\FF^{-1}([\gg(F)]_G [\ff(E)]_F) = \gg \circ \ff$ to these $\ff$ and $\gg$ to obtain that $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $[\ff(E)]_F = \AA$ and $[\gg(F)]_G = \BB$.
    \end{itemize}
\end{proof}

The following special case of the above theorem follows immediately.

\begin{theorem}
    \label{ch::lin_alg::thm::linear_functions_matrices_isomorphism_special_case}

    (Special case of linear isomorphism between linear functions and matrices).
    
    Let $K$ be a field, and let $\sE$ be a basis for $K^n$ and $\sF$ be a basis for $K^m$.
    
    The function $\FF:\LLLL(K^n \rightarrow K^p) \rightarrow K^{p \times n}$ defined by $\FF(\ff) := \ff(\sE)$ is a linear isomorphism, and also satisfies $\FF(\gg \circ \ff) = \gg(\sF) \ff(\sE)$ for all linear functions $\ff:K^n \rightarrow K^m$ and $\gg:K^m \rightarrow K^p$.

    The inverse $\FF^{-1}:K^{p \times n} \rightarrow \LLLL(K^n \rightarrow K^p)$ is defined by $\FF^{-1}(\AA) = (\vv \mapsto \AA \vv)$, and satisfies $\FF^{-1}(\BB \AA) = \gg \circ \ff$, where $\ff(\sE) = \AA$ and $\gg(\sE) = \BB$.
\end{theorem}

\newpage

\section{Geometry in $\R^2$}

\subsection*{Length}

\begin{defn}
\label{ch::lin_alg::defn::length_in_R2}
    (Length of a vector in $\R^2$).

    Let $\vv \in \R^2$. If we interpret $([\vv]_\sE)_1$ and $([\vv]_\sE)_2$ as the lengths of the base and height, respectively, of a right triangle, then the length of the hypotenuse of this right triangle is $\sqrt{([\vv]_\sE)_1^2 + ([\vv]_\sE)_2^2}$. Since $\vv = ([\vv]_\sE)_1 \see_1 + ([\vv]_\sE)_2 \see_2$, it makes sense to think of the hypotenuse as being $\vv$, and thus, to define the \textit{length} (or \textit{magnitude}) $||\vv||$ of $\vv$ to be the length of the hypotenuse: $||\vv|| := \sqrt{([\vv]_\sE)_1^2 + ([\vv]_\sE)_2^2}$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::unit_vector_hat_notation}
    (Unit vector hat notation). 
    
    For $\vv \in \R^2$, we define the notation $\hat{\vv} := \frac{\vv}{||\vv||}$. We have $||\hat{\vv}|| = 1$ for all $\vv \in \R^2$.
\end{defn}

\begin{lemma}
    $\vv = \mathbf{0} \iff ||\vv|| = 0$ for all $\vv \in \R^2$.
\end{lemma}

\subsection*{Angle}

You may be familiar with how the \textit{angle} between two vectors $\vv, \ww \in \R^2$ of the same length $r$ is defined by placing the tails of the vectors at the center of a circle of radius $r$, and then computing the ratio of arclength on the circle between the tips to the radius of the circle; $\text{angle} = \frac{\text{arclength}}{\text{radius}}$. Less commonly discussed, though still crucial for understanding, is the distinction between the two types of angle, which are \textit{signed} and \textit{unsigned}. We will see that the \textit{signed counterclockwise angle} $\theta_s(\vv, \ww)$ between vectors $\vv, \ww \in \R^2$ accounts for whether the shorter of the two arcs between the vectors is achieved by counterclockwise motion (in this case, the \textit{sign} is positive) or by clockwise motion (in this case, the \textit{sign} is negative). (There is a similar definition for \textit{signed clockwise angle} $\tttheta_s(\vv, \ww)$ that uses the opposite signed convention, in which clockwise motion is positive and counterclockwise motion is negative). The \textit{unsigned angle} $\theta_u(\vv, \ww)$ between vectors $\vv, \ww \in \R^2$ is simply the length of the shortest arc between the two vectors; it has no sign attached describing whether that shortest arc happens to be counterclockwise or clockwise.

Signed angle is useful for computing the vector that makes a specified signed angle with another vector, since for every vector $\vv \in \R^2$ and angle $t$ there is a unique vector that makes a signed counterclockwise angle of $t$ with $\vv$. (Unsigned angle does not work for this purpose, since for every vector $\vv$ and angle $t$, there are \textit{two} vectors that make an unsigned angle of $t$ with $\vv$).

Unsigned angle is the natural notion of ``the angle between two vectors'', since $\theta_u(\vv, \ww) = \theta_u(\ww, \vv)$, which is in accordance to the fact that ``the angle between two vectors'' shouldn't have any antisymmetry that distinguishes the two vectors from each other. (Signed angle does not work for this purpose, since $\theta_s(\vv, \ww) \neq \theta_s(\ww, \vv)$).

\subsubsection*{Counterclockwise and clockwise signed angle}

\begin{defn}
    (Counterclockwise and clockwise parameterizations of the unit circle).

    There are at least two unit-speed parameterizations of the unit circle:
    
    \begin{itemize}
        \item a parameterization $\cc:\R \rightarrow \R^2$ in which the motion is ``counterclockwise'' ($Q_1 = \{(x, y) \mid x, y > 0\}$ is the first quadrant visited and $Q_4 = \{(x, y) \mid x, y < 0\}$ is the last quadrant visited)
        \item a parameterization $\widetilde{\cc}:\R \rightarrow \R^2$ in which the motion is ``clockwise'' ($Q_4$ is the first quadrant visited and $Q_1$ is the last quadrant visited
    \end{itemize}

    One example of a counterclockwise parameterization is given by $\cc(t) := (t, \sqrt{1 - t^2})$ when $t \in [-1, 1]$ and $\cc(t) := (t - 1, -\sqrt{1 - (t - 1)^2})$ when $t \in [1, 3]$. For any counterclockwise parameterization $\cc$ with domain $[t_1, t_2]$, a clockwise parameterization $\widetilde{\cc}$ is given by $\widetilde{\cc}(t) := \cc(t_1 + t_2 - t)$.
\end{defn}

\begin{defn}
    (Arclength on a curve).

    Let $\xx:\R \rightarrow \R^2$ be a curve in $\R^2$, and suppose $\vv = \xx(t_1)$ and $\ww = \xx(t_2)$ are points on $\xx$ with $t_1 \leq t_2$. We define the \textit{arclength $\ss(\xx, \vv, \ww)$ traversed on $\xx$ between $\vv$ and $\ww$} to be
    
    \begin{align*}
        s(\xx, \vv, \ww) :=  \int_{t_1}^{t_2} \Big|\Big|\frac{d\xx(t)}{dt}\Big|\Big| dt.
    \end{align*}
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::angle_in_R2}
    (Signed angle between vectors in $\R^2$).
    
    Let vectors $\vv, \ww \in \R^2$ have the same length $r = ||\vv|| = ||\ww||$, suppose the initial points of $\vv$ and $\ww$ coincide, and consider the circle of radius $r$ that has the coinciding initial points of $\vv$ and $\ww$ as its center. We define the \textit{signed counterclockwise angle $\theta_s(\vv, \ww)$ between $\vv$ and $\ww$} to be $\theta_s(\vv, \ww) := \frac{s(\cc, \vv, \ww)}{r}$, and the \textit{signed clockwise angle
    $\tttheta_s(\vv, \ww)$ between $\vv$ and $\ww$} to be $\tttheta_s(\vv, \ww) := \frac{s(\widetilde{\cc}, \vv, \ww)}{r}$. 
    
    Notice that $\theta_s(\vv, \ww) = \theta_s(\hvv, \hww)$ and $\tttheta_s(\vv, \ww) = \tttheta_s(\hvv, \hww)$. This allows us to generalize: when $\vv$ and $\ww$ don't have the same length, we define $\theta_s(\vv, \ww) := \theta_s(\hvv, \hww) = s(\cc, \hvv, \hww)$ and $\tttheta_s(\vv, \ww) := \tttheta_s(\hvv, \hww) = s(\widetilde{\cc}, \hvv, \hww)$. Therefore, we have

    \begin{align*}
        \theta_s(\vv, \ww) = s(\cc, \hvv, \hww) \text{ and }
        \tttheta_s(\vv, \ww) = s(\widetilde{\cc}, \hvv, \hww) \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}
\end{defn}

\begin{theorem}
    (Domain of angle).

    We have $\theta_s(\vv, \ww), \tttheta_s(\vv, \ww) \in [0, 2\pi)$ for all $\vv, \ww \in \R^2$. Furthermore, $\theta_s$ and $\tttheta_s$ are onto when considered to be functions mapping into $[0, 2\pi)$.
\end{theorem}

\begin{proof}
    We give the proof for $\theta_s$; the proof for $\tttheta_s$ is similar.

    We first show $\theta_s(\vv, \ww) \in [0, 2\pi)$ for all $\vv, \ww \in \R^2$.

    \fullindent
    {
        (Case: $\vv = \ww$). Let $t \in [0, 2\pi)$ be such that $\cc(t) = \hvv$. We have $\theta_s(\vv, \ww) = \theta_s(\vv, \vv) = s(\cc, \hvv, \hvv) = \int_t^t ||\frac{d\cc(t)}{dt}|| dt = 0 \in [0, 2\pi]$.
    }

    \fullindent
    {
        (Case: $\vv \neq \ww$). Since $\hvv \neq \hww$, the path from $\hvv$ to $\hww$ is a proper subset of the unit circle, and the arclength traversed on the path is strictly less than the circumference of the unit circle. By the definition of $\pi$, the circumference of the unit circle is $2\pi$, so we have $s(\cc, \hvv, \hww) = \theta_s(\vv, \ww) < 2\pi$.
    }

    Now we show $\theta_s:\R^2 \times \R^2 \rightarrow [0, 2\pi)$ is onto.

    \fullindent
    {
        Let $\vv \in \R^2$ and suppose for contradiction that there exists $t_0 \in [0, 2\pi)$ for which there does not exist $\ww \in \R^2$ such that $\theta_s(\vv, \ww) = t_0$. Equivalently, there does not exist $\hww \in \R^2$ such that $s(\cc, \hvv, \hww) = t_0$. Since arclength is continuous, then there also does not exist $\hww \in \R^2$ such that $s(\cc, \hvv, \hww) = t$ for all $t > t_0$. Thus $\max_{\hww \in \R^2} s(\cc, \hvv, \hww) = t_0$. But the circumference of the unit circle is also $\max_{\hww \in \R^2} s(\cc, \hvv, \hww)$, so the circumference of the unit circle is $t_0$. This is a contradiction, since $t_0 < 2\pi$, and as we know that $2\pi$ is the actual circumfrence of the unit circle.
    }
\end{proof}

\begin{theorem}
    (Clockwise and counterclockwise signed angles sum to $2\pi$).

    We have $\theta_s(\vv, \ww) + \tttheta_s(\vv, \ww) = 2\pi$ for all $\vv, \ww \in \R^2$.
\end{theorem}

\begin{proof}
    For all $\vv, \ww \in \R^2$, the unit circle is partitioned into the counterclockwise path from $\hvv$ to $\hww$ (which is $\cc([t_1, t_2])$, where $\cc(t_1) = \hvv$ and $\cc(t_2) = \hww$) and the clockwise path from $\hvv$ to $\hww$ (which is $\widetilde{\cc}([\widetilde{t}_1, \widetilde{t}_2])$, where $\widetilde{\cc}(\widetilde{t}_1) = \hvv$ and $\widetilde{\cc}(\widetilde{t}_2) = \hww$). The arclength of the unit circle, $2\pi$, is therefore the sum of the arclengths of these parameterizations. Thus $s(\cc, \hvv, \hww) + s(\widetilde{\cc}, \hvv, \hww) = 2\pi$ for all $\vv, \ww \in \R^2$. That is, $\theta_s(\vv, \ww) + \tttheta_s(\vv, \ww) = 2\pi$ for all $\vv, \ww \in \R^2$.
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::negation_of_angle}
    (Negation of an angle $\text{mod } 2\pi$).

    We have 

    \begin{align*}
        -\theta_s(\vv, \ww) \bmod 2\pi &= 2\pi - \theta_s(\vv, \ww) = \tttheta_s(\vv, \ww), \\
        -\tttheta_s(\vv, \ww) \bmod 2\pi &= 2\pi - \tttheta_s(\vv, \ww) = \theta_s(\vv, \ww)
    \end{align*}

    for all $\vv, \ww \in \R^2$.
\end{theorem}

\begin{proof}
    We prove the theorem for $\theta_s$; the proof for $\tttheta_s$ is similar.
    
    In general, if $t \in [0, 2\pi)$ then $-t \bmod 2\pi = 2\pi - t$. (Hopefully this is intuitive; if not, see Theorem \ref{ch::appendix::thm::negation_of_angle} of the appendix). Therefore $-\theta_s(\vv, \ww) \bmod 2\pi = 2\pi - \theta_s(\vv, \ww)$ for all $\vv, \ww \in \R^2$. Of course, we have $2\pi - \theta_s(\vv, \ww) = \tttheta_s(\vv, \ww)$ for all $\vv, \ww \in \R^2$ by the previous theorem, so $-\theta_s(\vv, \ww) \bmod 2\pi = \tttheta_s(\vv, \ww)$ for all $\vv, \ww \in \R^2$, as desired.
\end{proof}

\begin{theorem}
    (Signed angle is antisymmetric).
    
    We have 
    
    \begin{align*}
        \theta_s(\vv, \ww) &= -\theta_s(\ww, \vv) \bmod 2\pi, \\
        \tttheta_s(\vv, \ww) &= -\tttheta_s(\ww, \vv) \bmod 2\pi
    \end{align*}

    for all $\vv, \ww \in \R^2$.
\end{theorem}

\begin{proof}
    We show the theorem for $\theta_s$; the proof for $\tttheta_s$ is similar.

    For all $\vv, \ww \in \R^2$, the counterclockwise path on the unit circle from $\hvv$ to $\hww$ (which is $\cc([t_1, t_2])$, where $\cc(t_1) = \hvv$ and $\cc(t_2) = \hww$) is equal to the clockwise path on the unit circle from $\hww$ to $\hvv$ (which is $\widetilde{\cc}([\widetilde{t}_1, \widetilde{t}_2])$, where $\cc(\widetilde{t}_1) = \hww$ and $\cc(\widetilde{t}_2) = \hvv$). Thus $s(\cc, \hww, \hvv) = s(\widetilde{\cc}, \hvv, \hww)$ for all $\vv, \ww \in \R^2$. That is, $\theta_s(\ww, \vv) = \tttheta_s(\vv, \ww)$ for all $\vv, \ww \in \R^2$. We have $\tttheta_s(\vv, \ww) = -\theta_s(\ww, \vv) \bmod 2\pi$ by the previous theorem. This proves the theorem.
\end{proof}

\subsubsection*{Unsigned angle}

\begin{theorem}
    (Signed counterclockwise angle as a minimum).

    \begin{align*}
        \theta_s(\vv, \ww) = \Big(\pm \min(\theta_s(\vv, \ww), \tttheta_s(\vv, \ww))\Big) \bmod 2\pi,
    \end{align*}

    where $\pm$ is $+$ when the minimum is $\theta_s(\vv, \ww)$ and is $-$ when the minimum is $\tttheta_s(\vv, \ww)$.
\end{theorem}

\begin{proof}
    We will show that for every possible value of the minimum, the right side is equal to the left side. 
    
    (Case: the minimum is $\theta_s(\vv, \ww)$). In this case, the expression on the right side is $\theta_s(\vv, \ww) \bmod 2\pi$. Since $\theta_s(\vv, \ww) \in [0, 2\pi)$ then $\theta_s(\vv, \ww) \bmod 2\pi = \theta_s(\vv, \ww)$.

    (Case: the minimum is $\tttheta_s(\vv, \ww)$). In this case, the expression on the right side is $(-\tttheta_s(\vv, \ww)) \bmod 2\pi$. We know from Theorem \ref{ch::lin_alg::thm::negation_of_angle} that this is $\theta_s(\vv, \ww)$.
\end{proof}

The above inspires the following definition of unsigned angle.

\begin{defn}
    (Unsigned angle between vectors in $\R^2$).

    The \textit{unsigned angle $\theta_u(\vv, \ww)$ between $\vv$ and $\ww$} is defined to be

    \begin{align*}
        \theta_u(\vv, \ww) := \min(\theta_s(\vv, \ww), \tttheta_s(\vv, \ww)) \bmod 2\pi
    \end{align*}

    \textbf{so the abs comes before the mod, meaning it isn't the abs of the mod}
\end{defn}

\begin{theorem}
    (Unsigned angle is symmetric).

    We have $\theta_u(\vv, \ww) = \theta_u(\ww, \vv)$ for all $\vv, \ww \in \R^2$.
\end{theorem}

\begin{proof}
    We have

    \begin{align*}
        \theta_u(\vv, \ww) &= \min(\theta_s(\vv, \ww), \tttheta_s(\vv, \ww)) \bmod 2\pi \\
        &= \min(-\theta_s(\ww, \vv) \bmod 2\pi, -\tttheta_s(\ww, \vv) \bmod 2\pi) \bmod 2\pi \\
        &= \min(\tttheta_s(\ww, \vv), \theta_s(\ww, \vv)) \bmod 2\pi \\
        &= \min(\theta_s(\ww, \vv), \tttheta_s(\ww, \vv)) \bmod 2\pi \\
        &= \theta_u(\ww, \vv).
    \end{align*}
\end{proof}

The above fact $\theta_u$ the natural notion of angle between vectors. [expand, copying from intro]

\begin{theorem}
\label{ch::lin_alg::thm::transitivity_of_angle}
    (Transitivity of angle).

    We have

    \begin{align*}
        \theta_s(\vv, \ww) &= \theta_s(\vv, \uu) + \theta_s(\uu, \ww) \text{ for all $\vv, \uu, \ww \in \R^2$}
    \end{align*}

    and

    \begin{align*}
        \theta_u(\vv, \ww) &=
        \begin{cases}
            \theta_u(\vv, \uu) + \theta_u(\uu, \ww) & \theta_u(\vv, \uu) + \theta_u(\uu, \ww) \leq \pi \\
            2\pi - (\theta_u(\vv, \uu) + \theta_u(\uu, \ww)) & \theta_u(\vv, \uu) + \theta_u(\uu, \ww) > \pi 
        \end{cases} \text{ for all $\vv, \uu, \ww \in \R^2$}.
    \end{align*}
\end{theorem}

\begin{proof}
    simple addition of arclength along path for first one.

    for second one:

    (Case: $\theta_u(\vv, \uu) + \theta_u(\uu, \ww) \leq \pi$).

    (Case: $\theta_u(\vv, \uu) + \theta_u(\uu, \ww) > \pi$).
\end{proof}

\subsubsection*{For dot product section}

Make sure this discussion on angle is given in dot product section:
\begin{itemize}
    \item definition: $\vv \cdot \ww := ||\ww|| \sproj(\vv \rightarrow \ww)$
    \item lemma: $\sproj(\vv \rightarrow \ww) = ||\vv|| \cos(\theta_s(\vv, \ww))$
    \item lemma: $\cos(\theta_s(\vv, \ww)) = \cos(\theta_u(\vv, \ww))$
    \item theorem: $\vv \cdot \ww = ||\vv|| ||\ww|| \cos(\theta_s(\vv, \ww)) = ||\vv|| ||\ww|| \cos(\theta_u(\vv, \ww))$
    \item now we derive an expression for angle in terms of lengths and the dot product. [this expression would amount to a tautology if we didn't know the algebraic formula for the dot product].
    \item derivation. recall that $\arccos$ is the inverse of the restriction of cos to $I := [0, \pi]$. The set which $\theta_u(\vv, \ww)$ ranges over is exactly $I$, while the set $\theta_s(\vv, \ww)$ ranges over is a proper superset of $I$. Therefore, we have $\arccos(\cos(\theta_u(\vv, \ww)) = \theta_u(\vv, \ww)$ but do not always have $\arccos(\cos(\theta_s(\vv, \ww)) = \theta_s(\vv, \ww)$. consequently, we have $\theta_u(\vv, \ww) = \arccos(\hvv \cdot \hww)$ but do not have $\theta_s(\vv, \ww) = \arccos(\hvv \cdot \hww)$.
\end{itemize}


\subsection*{Rotations}

\begin{defn}
    (Rotation).
    
    A function $\RR:\R^2 \rightarrow \R^2$ is a \textit{rotation in $\R^2$} iff

    \begin{enumerate}
        \item ($\RR$ preserves length). $||\vv|| = ||\RR(\vv)||$ for all $\vv \in \R^2$.
        \item ($\RR$ preserves signed angle). $\theta_s(\vv, \ww) = \theta_s(\RR(\vv), \RR(\ww))$ for all $\vv, \ww \in \R^2$.
        \item ($\RR$ fixes the origin). $\RR(\mathbf{0}) = \mathbf{0}$.
    \end{enumerate}
\end{defn}

\begin{remark}
    (Common alternative rotation terminology).
    
    Other texts define \textit{general rotations} to be length preserving functions that preserve length, \textit{un}signed angle, and fix the origin. When using this convention, \textit{proper rotations} are general rotations that preserve signed angle and \textit{improper rotations} are general rotations that negate signed angle.
\end{remark}

\begin{lemma}
    The kernel of every length-preserving function is trivial.
\end{lemma}

\begin{proof}
    Suppose for contradiction that $\ff$ preserves length and has a nontrivial kernel. Then there exists a nonzero $\vv \in \R^n$ such that $\ff(\vv) = \mathbf{0}$. We have $||\vv|| \neq 0$ (by the lemma) and $||\ff(\vv)|| = ||\mathbf{0}|| = 0$, so $||\vv|| \neq ||\ff(\vv)||$, contradicting the fact that $\ff$ is an isometry.
\end{proof}
        
\begin{lemma}
    Let $\ff:\R^n \rightarrow \R^n$ be a length-preserving function.
        
    \begin{enumerate}
        \item If $\ff$ fixes the origin then for all $\vv \in \R^n$ we have $\ff(\spann(\{\vv\})) = \spann(\{\ff(\vv)\})$.

        \item $\ff$ maps the midpoint $\frac{1}{2}(\vv + \ww)$ of $\vv, \ww \in \R^n$ to the midpoint $\frac{1}{2}(\ff(\vv) + \ff(\ww))$ of $\ff(\vv), \ff(\ww)$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    TO-DO
    
    \begin{enumerate}
        \item ...

        \item ...
    \end{enumerate}
\end{proof} 

\begin{theorem}
    Every length-preserving function $\R^n \rightarrow \R^n$ that fixes the origin is linear\footnote{From \url{https://www.sfu.ca/~mdevos/notes/geom-sym/18_isometries.pdf}.}.
\end{theorem}

\begin{proof}
    Let $\ff:\R^n \rightarrow \R^n$. We need to show that $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ for all $\vv, \ww \in \R^n$ and $\ff(c\vv) = c\ff(\vv)$ for all $c \in \R$ and $\vv \in \R^n$.
    
    ($\ff(c\vv) = c\ff(\vv)$ for all $c \in \R$ and $\vv \in \R^n$).
    \fullindent
    {
        \fullindent
        {
            (Case: $c = 0$). Then $\ff(c\vv) = \ff(0\vv) = 0\ff(\vv) = \mathbf{0} = 0\ff(\vv) = c\ff(\vv)$.
        }
        \fullindent
        {
            (Case: $c \neq 0$). Define $\text{dist}:\R^n \times \R^n \rightarrow \R$ by $\text{dist}(\vv, \ww) := ||\ww - \vv||$. Notice that $c\vv \in \spann(\{\vv\})$ satisfies

            \begin{align*}
                \begin{cases}
                    \text{dist}(c\vv, \mathbf{0}) = ||c\vv|| \\
                    \text{dist}(c\vv, \vv) = ||(c - 1)\vv||
                \end{cases}
            \end{align*}

            Similarly, $\ff(\vv) \in \spann(\{\ff(\vv)\})$ satisfies

            \begin{align*}
                \begin{cases}
                    \text{dist}(c\ff(\vv), \mathbf{0}) = ||c\ff(\vv)|| = ||c\vv|| \\
                    \text{dist}(c\ff(\vv), \ff(\vv)) = ||(c - 1)\ff(\vv)|| = ||(c - 1)\vv||
                \end{cases}
            \end{align*}

            We know from the lemma that $\spann(\{\vv\}) = \spann(\{\ff(\vv)\})$. Therefore $\ff(c\vv)$ and $c\ff(\vv)$ are two vectors in the same $1$-dimensional vector space satisfying the same distance constraints. To complete the proof of this case, it suffices to show that for all $d_1, d_2 \in \R$ there is a unique $\ww \in \spann(\{\vv\})$ such that $\text{dist}(\ww, \mathbf{0}) = d_1$ and $\text{dist}(\ww, \mathbf{v}) = d_2$.

            TO-DO
        }
    }
    
    ($\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ for all $\vv, \ww \in \R^n$).
    \fullindent
    {
        We know $\ff$ maps midpoints to midpoints: $\ff\left(\frac{1}{2}(\vv + \ww)\right) = \frac{1}{2}(\ff(\vv) + \ff(\ww))$ for all $\vv, \ww \in \R^n$. Since $\ff(c\vv) = c\ff(\vv)$ for all $c \in \R$ and $\vv \in \R^n$ then the left side of the previous equation is equal to $\frac{1}{2}\ff(\vv + \ww)$, and we obtain $\frac{1}{2}\ff(\vv + \ww) = \frac{1}{2}(\ff(\vv) + \ff(\ww))$ for all $\vv, \ww \in \R^n$. Multiply both sides of this equation by $2$ to obtain $\ff(\vv + \ww) = \ff(\vv) + \ff(\ww)$ for all $\vv, \ww \in \R^n$, as desired.
    }
\end{proof}

The next theorem is an immediate consequence of the previous one.

\begin{theorem}
    All rotations on $\R^2$ are linear functions.
\end{theorem}

\subsubsection*{Trigonometry}

\begin{lemma}
\label{ch::lin_alg::lemma::vector_from_vector_and_angle}

    (Specifying a vector with a vector and an angle difference).
    
    For every $\hvv \in \R^2$ and $t \in [0, 2\pi)$ there is a unique $\hww \in \R^2$ such that $\theta_s(\vv, \hww) = t$.
\end{lemma}

\begin{proof}
    The map $t \mapsto s(\cc, \vv, \cc(t))$ measures the arclength accumulated along the counterclockwise parameterization of the unit circle, and is therefore is strictly increasing, i.e., it is one-to-one, i.e., it is invertible. Since every $\hww \in \R^2$ is of the form $\hww = \cc(t)$ for some $t \in \R$ [come back and ensure consistency of angle intervals], then for all $\hvv \in \R^2$ the map $\hww \mapsto s(\cc, \hvv, \hww)$ defined on unit vectors is invertible as well. This means that for every $\hvv \in \R^2$ and $t \in [0, 2\pi)$ there is a unique $\hww \in \R^2$ such that $\theta_s(\vv, \hww) = t$.
\end{proof}

The previous lemma justifies the following definition.

\begin{defn}
\label{ch::lin_alg::deriv::cosine_sine}
    (Cosine and sine).

    For every $t \in [0, 2\pi)$ we define $\cos(t)$ and $\sin(t)$ to be the coordinates of the unique unit vector $\hvv$ such that $\theta_s(\see_1, \hvv) = t$.
\end{defn}

\begin{theorem}
    \label{ch::lin_alg::thm::vector_polar_form}
    (Polar form of vectors).

    We have
    
    \begin{align*}
        \vv =
        ||\vv||
        \begin{pmatrix}
            \cos(\theta_s(\see_1, \vv)) \\
            \sin(\theta_s(\see_1, \vv))
        \end{pmatrix}
        =
        ||\vv||
        \begin{pmatrix}
            \cos(\tttheta_s(\see_1, \vv)) \\
            -\sin(\tttheta_s(\see_1, \vv))
        \end{pmatrix}
        \text{ for all $\vv \in \R^2$}.
    \end{align*}
\end{theorem}

\begin{proof}
    Let $\vv \in \R^2$. We first prove that $\vv$ is equal to the first expression. We have $\theta_s(\see_1, \vv) = t$ for some $t \in [0, 2\pi)$; by the previous definition we have $\hvv = \vv/||\vv|| = (\cos(t), \sin(t))^\top = (\cos(\theta_s(\see_1, \vv)), \sin(\theta_s(\see_1, \vv)))^\top$. Multiply both sides of this equation by $||\vv||$ to obtain the desired result.

    To derive the second expression for $\vv$, use the that $\theta_s(\see_1, \vv) = 2\pi - \tttheta_s(\see_1, \hvv)$.
\end{proof}

\begin{deriv}
    (Cosine and sine as ratios of lengths of sides of right triangles).

    \newcommand{\thh}{\widetilde{\hh}}

    Consider an arbitrary right triangle $T$ with non-hypotenuse sides $\ss_1$ and $\ss_2$. Note, the hypotenuse $\hh$ is $\hh = \ss_1 \pm \ss_2$.

    For $i \in \{1, 2\}$, the $\ss_i$ which appears in the angle $\theta_u(\ss_i, \hh)$ is called the \textit{side adjacent to $\theta_u(\ss_i, \hh)$}; and the side $\ss_j$ which does not appear in that angle is called \textit{side opposite to $\theta_u(\ss_i, \hh)$}. Thus $\ss_1$ is the side adjacent to $\theta_u(\ss_1, \hh)$ and is also the side opposite to $\theta_u(\ss_2, \hh)$; $\ss_2$ is the side adjacent to $\theta_u(\ss_2, \hh)$ and is also the side opposite to $\theta_u(\ss_1, \hh)$.

    let $\ff$ be an length-preserving and signed-angle-preserving function that takes the arbitrary right triangle $T$ to a congruent right triangle $\ff(T) = \tT \subseteq Q_1$ in the first quadrant with hypotenuse $\thh = \ff(\hh)$, a horizontal side $\ss_h = \ff(\ss_1)$ that lies on the $x$-axis, and a vertical side $\ss_v = \ff(\ss_2)$. \textbf{justify why such $\ff$ exists}

    note, since $\ff$ preserves angles, then $\ff$ also preserves the adjacentness and oppositeness of triangle sides\footnote{If $\ss_i$ is the side adjacent to $\theta(\ss_i, \hh)$ then $\ff(\ss_i) = \ss_h$ is the side adjacent to $\theta(\ff(\ss_i), \ff(\hh))$; the analogous statement for opposite sides also holds.}

    \textbf{since $\ff$ is linear (justify why)} then since $\hh = \ss_1 \pm \ss_2$ we have $\ff(\hh) = \ff(\ss_1) \pm \ff(\ss_2) \iff \thh = \ss_h \pm \ss_v$. since $\thh \in Q_1$, the $\pm$ sign must be $+$, and we have $\thh = \ss_h + \ss_v = ||\ss_h||\widehat{\ss}_h + ||\ss_v|| \widehat{\ss}_v = (||\ss_h||, ||\ss_v||)^\top$. also, by the previous theorem we have $\thh = ||\thh||(\cos(t), \sin(t))^\top = (||\thh||\cos(t), ||\thh||\sin(t))^\top$, where $t = \theta_s(\see_1, \thh) = \theta_s(\ss_h, \thh)$. comparing the results of the previous two sentences we see $\ss_h = (||\thh||\cos(t), 0) = ||\thh||\cos(t)\see_1$ and $\ss_v = (0, ||\thh||\sin(t)) = ||\thh||\sin(t)\see_2$.

    with this information, we can obtain expressions for $\cos(t)$ and $\sin(t)$ as functions of the side lengths of $\tT \cong T$:

    \begin{align*}
        \frac{||\ss_1||}{||\hh||} = \frac{||\ff(\ss_1)||}{||\ff(\hh)||} = \frac{||\ss_h||}{||\thh||} = \frac{||\thh||\cos(t)}{||\thh||} = \cos(t), \\
        \frac{||\ss_2||}{||\hh||} = \frac{||\ff(\ss_2)||}{||\ff(\hh)||} = \frac{||\ss_v||}{||\thh||} = \frac{||\thh||\sin(t)}{||\thh||} = \sin(t).
    \end{align*}
    
    This shows that if we have a right triangle and $t$ is one of the non-right angles, then, using $\text{adj}$ to denote the length of the side adjacent to $t$ by $\text{adj}$, $\text{opp}$ to denote the length of the side opposite to $t$, and $\text{hyp}$ to denote the length of the hypotenuse, we have

    \begin{align*}
        \cos(t) = \frac{\text{adj}}{\text{hyp}} \text{ and } \sin(t) = \frac{\text{opp}}{\text{hyp}}.
    \end{align*}
\end{deriv}

\begin{defn}
    (Inverse trigonometric functions).

    $\arccos$ and $\arcsin$ as inverses of one-to-one restrictions of $\cos$ to $[0, \pi]$ and $\sin$ to $[-\frac{\pi}{2}, \frac{\pi}{2}]$
\end{defn}

\begin{theorem}
    (Standard matrix of a rotation).
     
    The standard matrix of a rotation is

    \begin{align*}
        \begin{pmatrix}
            \cos(t) & -\sin(t) \\
            \sin(t) & \cos(t)
        \end{pmatrix}
    \end{align*}

    for some $t \in [0, 2\pi)$. The rotation whose standard matrix is the above is called the \textit{counterclockwise rotation by $t$} and is denoted $\RR_t$. The \textit{clockwise rotation by $t$} is denoted $\widetilde{\RR}_t$.
\end{theorem}

\begin{proof}
    Let $\RR$ be a rotation. We know from Theorem \ref{ch::lin_alg::thm::vector_polar_form} that 

    \begin{align*}
        \RR(\vv) =
        ||\RR(\vv)||
        \begin{pmatrix}
            \cos(\theta_s(\see_1, \RR(\vv)) \\
            \sin(\theta_s(\see_1, \RR(\vv))
        \end{pmatrix}
        =
        ||\vv||
        \begin{pmatrix}
            \cos(\theta_s(\see_1, \RR(\vv)) \\
            \sin(\theta_s(\see_1, \RR(\vv))
        \end{pmatrix}
        \text{ for all $\vv \in \R^2$.}
    \end{align*}
        
    When $\vv = \see_1$, then

    \begin{align*}
        \RR(\see_1) =
        \begin{pmatrix}
            \cos(t) \\
            \sin(t)
        \end{pmatrix},
        \text{ where $t = \theta_s(\see_1, \RR(\see_1))$}.
    \end{align*}

    When $\vv = \see_2$, the transitivity of signed angle (recall Theorem \ref{ch::lin_alg::thm::transitivity_of_angle}) gives us $\theta_s(\see_1, \RR(\see_2)) = \theta_s(\see_1, \RR(\see_1)) + \theta_s(\RR(\see_1), \RR(\see_2)) = t + \theta_s(\RR(\see_1), \RR(\see_2)) = t + \theta_s(\see_1, \see_2) = t + \frac{\pi}{2}$. Thus

    \begin{align*}
        \RR(\see_2) =
        \begin{pmatrix}
            \cos(\theta_s(\see_1, \RR(\see_2)) \\
            \sin(\theta_s(\see_1, \RR(\see_2))
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos(t + \frac{\pi}{2}) \\
            \sin(t + \frac{\pi}{2})
        \end{pmatrix}
        =
        \begin{pmatrix}
            \sin(-t) \\
            \cos(-t)
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\sin(t) \\
            \cos(t)
        \end{pmatrix}.
    \end{align*}

    The standard matrix $\RR(\sE)$ of $\RR$ has $\RR(\see_1)$ as its first column and $\RR(\see_2)$ as its second column, so we have shown the theorem.
\end{proof}

\begin{theorem}
    ().
    
    \begin{enumerate}
        \item $\theta_s(\vv, \RR_t(\vv)) = t$ for all $\vv \in \R^2$.
        \item For all $\hvv, \hww \in \R^2$ there exists a rotation taking $\hvv$ to $\hww$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
        \item Looking at the proof of the previous theorem, we see that the angle $t$ in the definition of $\RR_t$ is $t = \theta_s(\see_1, \RR_t(\vv))$ [might have to do trigonometric computation]
        
        \item By (1), we have $\theta_s(\hvv, \RR_{\theta_s(\hvv, \hww)}(\hvv)) = \theta_s(\hvv, \hww)$. We know from Lemma \ref{ch::lin_alg::lemma::vector_from_vector_and_angle} that the vector $\huu \in \R^2$ for which $\theta_s(\hvv, \huu) = \theta_s(\hvv, \hww)$ is unique. Therefore $\huu = \RR_{\theta_s(\hvv, \hww)}(\vv) = \hww$, so $\RR_{\theta_s(\hvv, \hww)}$ takes $\hvv$ to $\hww$.        
    \end{enumerate}
\end{proof}

\subsection*{Projections}

\begin{defn}
\label{ch::lin_alg::defn::vector_proj}
    (Vector projection in $\R^2$).
    
    Outline:
    \begin{itemize}
        \item Defn. Vectors $\vv, \ww \in \R^2$ are said to be \textit{perpendicular}, or \textit{orthogonal}, iff there exists a rotation taking $\hvv$ to $\see_1$ and $\hww$ to $\pm \see_2$.
        \begin{itemize}
            \item Why? If $\ww$ and $\uu$ are linearly independent then they form a basis of $\R^2$, and we have $\vv = v_{||} \hww + v_u \huu$ for some $v_{||}, v_u \in \R$. Define $\proj_{\uu}(\vv \rightarrow \ww) := v_{||} \hww$. 
            
            Notice that $\proj_{\uu}(\vv \rightarrow \ww)$ is nonzero for $\vv$ such that $\theta_u(\hvv, \uu) \neq \frac{\pi}{2}$. We might as well have this nonzero arc of vectors that sweeps out $\frac{\pi}{2}$ line up with the ``standard'' arc of vectors that sweeps out $\frac{\pi}{2}$, which are the unit vectors in the first quadrant.
            \item Find where orthonormal bases are defined for $n$-dimensional inner product spaces, and change that definition so it is in terms of rotations. Then prove that all orthonormal bases satisfy the usual condition $\langle \huu_i, \huu_j \rangle = \delta_{ij}$.
        \end{itemize}
        \item Thm. For every $\vv \in \R^2$ there exists $\vv_\perp \in \R^2$ that is perpendicular to $\vv$.
        \begin{itemize}
            \item Proof. Let $t$ be such that $\RR_t(\see_1) = \vv$. Then $\RR^{-1}$ is a rotation taking $\see_1$ to $\hvv$ and taking $\RR(\see_2)$ to $\see_2$. Thus $\vv_\perp = \RR(\see_2)$ is perpendicular to $\vv$.
        \end{itemize}
        \item Thm. If $\vv, \vv_\perp \in \R^2$ and $\vv_\perp$ is perpendicular to $\vv$, then $\{\vv, \vv_\perp\}$ is a basis for $\R^2$.
        \begin{itemize}
            \item Proof. See older versions of this book for the proof.
        \end{itemize}
        \item $\hvv_\perp = \widehat{\vv_\perp}$
        \item Let $\vv, \ww, \ww_\perp \in \R^2$, where $\ww_\perp$ is orthogonal to $\ww$. Since $\{\ww, \ww_\perp\}$ is a basis for $\R^2$ we have $\vv = v_{||} \hww + v_\perp \hww_\perp$ for some $v_{||}, v_\perp \in \R$. We define the \textit{(orthogonal) projection} $\proj(\vv \rightarrow \ww)$ \textit{ of $\vv$ onto $\ww$} to be $\proj(\vv \rightarrow \ww) := v_{||} \hww$, and define the the \textit{signed magnitude of the vector projection $\sproj(\vv \rightarrow \ww)$ of $\vv$ onto $\ww$} to be $\sproj(\vv \rightarrow \ww) := v_{||}$.
        \item Thm. $\vv, \ww \in \R^2$ are orthogonal iff $\theta_u(\vv, \ww) = \frac{\pi}{2}$ iff $\proj(\vv \rightarrow \ww) = \mathbf{0}$.
    \end{itemize}
\end{defn}

\begin{remark}
    Note that, for $\vv, \ww \in \R^2$, we have $\proj(\vv \rightarrow \ww) = \proj(\vv \rightarrow \hww)$ because $\hat{\hww} = \hat{\ww}$.
\end{remark}

\begin{theorem}
\label{ch::lin_alg::thm::vector_proj_Signed_magnitude}
    (Vector projection and its signed magnitude in $\R^2$).

    Let $\vv, \ww \in \R^2$. We have the following.
    
    \begin{enumerate}
        \item $\proj(\vv \rightarrow \ww) = \sproj(\vv \rightarrow \ww) \hww$.
        \item $\sproj(\vv \rightarrow \ww) = \pm ||\proj(\vv \rightarrow \ww)||$.
    \end{enumerate}

    This second fact is what should be emphasized when thinking about the signed magnitude of a vector projection.
\end{theorem}

\begin{proof}
    (1) follows from the definition of $\proj(\vv \rightarrow \ww)$. (2) is obtained by taking the magnitude of each side of (1).
\end{proof}

\begin{lemma}
    \label{ch::lin_alg::lemma::rotated_projection}
    (Rotated projection is projection of rotated vectors). 
    
    Let $\vv, \ww \in \R^2$. For any $t \in [0, 2\pi)$ we have $\RR_t(\proj(\vv \rightarrow \ww)) = \proj(\RR_t(\vv) \rightarrow \RR_t(\ww))$. It immediately follows that for any $t \in [0, 2\pi)$ we have $\RR_t(\sproj(\vv \rightarrow \ww)) = \sproj(\RR_t(\vv) \rightarrow \RR_t(\ww))$.
\end{lemma}

\begin{proof}
    Let $\vv = v_{||} \hww + v_\perp \hww_\perp$. Then $\RR_t(\vv) = v_{||} \RR_t(\hww) + v_\perp \RR_t(\hww_\perp)$. Since rotations are length-preserving, we have $\RR_t(\hww) = \widehat{\RR_t(\hww)}$. Because there exists a rotation that sends $\ww$ to $\ww_\perp$, because rotations commute with each other, and because rotations are length-preserving, we have $\RR_t(\hww_\perp) = \widehat{\RR_t(\ww)}_\perp$. Therefore

    \begin{align*}
        \RR_t(\vv) = v_{||} \RR_t(\hww) + v_\perp \RR_t(\hww_\perp) = v_{||} \widehat{\RR_t(\hww)} + v_\perp \widehat{\RR_t(\ww)}_\perp
    \end{align*}

    Of course, $v_{||} \RR_t(\hww) = \RR_t(v_{||} \hww) = \RR_t(\proj(\vv \rightarrow \ww))$. Also, since $v_{||}\widehat{\RR_t(\hww)}$ is the parallel term in the orthogonal decomposition of $\RR_t(\vv)$ onto $\RR_t(\ww)$, it must be $\proj(\RR_t(\vv) \rightarrow \RR_t(\ww))$. Rewriting both sides in terms of these projections, we have

    \begin{align*}
        \RR_t(\proj(\vv \rightarrow \ww)) + v_\perp \RR_t(\hww_\perp) = \proj(\RR_t(\vv) \rightarrow \RR_t(\ww)) + v_\perp \widehat{\RR_t(\ww)}_\perp.
    \end{align*}

    Using again the fact that $\RR_t(\hww_\perp) = \widehat{\RR_t(\hww)}_\perp$, we obtain $\RR_t(\proj(\vv \rightarrow \ww)) = \proj(\RR_t(\vv) \rightarrow \RR_t(\ww))$, as desired. 
\end{proof}

\newpage

\subsection*{The dot product}

Most explanations of the dot product engage in at least one of two pedagogically problematic approaches. The most common of these approaches is: first, define the dot product between $\vv = (v_1, v_2)^\top \in \R^2$ and $\ww = (w_1, w_2)^\top \in \R^2$ via the \textit{algebraic formula} $\vv \cdot \ww := \sum_{i = 1}^2 v_i w_i$;
second, show that this initial definition implies the \textit{geometric formula} $\vv \cdot \ww = ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww))$ by using the law of cosines. This is the wrong way of doing things for two reasons. Firstly, there is much more motivation (such as further investigation of vector projections or the physical concept of work) for starting with the geometric formula and then proving that the algebraic formula follows. Secondly, the law of cosines gives no intuition\footnote{When used as a starting point, the law of cosines is unintuitive because its validity is established via Euclidean geometry, which is unintuitive (to me, at least)}; there is a much better way to prove that algebraic formula implies the geometric formula, which we present.

The second problematic approach arises when an author does decide to start with the geometric formula, $\vv \cdot \ww = ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww))$. Authors will prove that the algebraic formula follows as a result of the law of cosines. Again, using the law of cosines gives no intuition. Instead, the algebraic formula should be proved by showing and then using the bilinearity of the dot product. The law of cosines should never
be used in proving the equivalence of the two dot product formulas. When the equivalence between these two formulas is shown correctly, the law of cosines can be shown as a consequence.

\vspace{.25cm}

The \textit{cross product} also comes with two common pedagogical problems. [section later in this chapter] describes these problems and presents a satisfying explanation of the cross product. (The cross product is addressed in [later in this chapter ] because understanding cross products requires understanding determinants and orientation).

\begin{defn}
    (Dot product).
    
    \begin{align*}
        \vv \cdot \ww := ||\ww|| \spc \sproj(\vv \rightarrow \ww)
    \end{align*}
\end{defn}

\subsubsection*{Geometric $\implies$ algebraic}

\begin{lemma}
    (Signed projection onto a vector in $\R^2$ is a linear function).
    
    Let $\vv_1, \vv_2 \in \R^2$. The map $\vv_1 \mapsto \sproj(\vv_1 \rightarrow \vv_2)$ is linear.
\end{lemma}

\begin{proof}
    Let $\vv_1, \vv_2, \ww \in \R^2$, and define $\ff:\R^2 \rightarrow \R$ by $\ff(\vv) = \sproj(\vv \rightarrow \ww)$. We need to show that $\ff(\vv_1 + \vv_2) = \ff(\vv_1) + \ff(\vv_2)$ and that $\ff(c\vv) = c\ff(\vv)$ for all $c \in \R$. 
    
    We have
    
    \begin{align*}
        \ff(\vv_1 + \vv_2) &= \ff\Big(\Big((v_1)_{||}\hat{\ww} + (v_1)_{\perp}\hat{\ww}_\perp \Big) + \Big((v_2)_{||}\hat{\ww} + (v_2)_\perp \hat{\ww}_\perp \Big) \Big), \text{ where $(v_i)_{||} = \sproj(\vv_i \rightarrow \ww)$ for $i \in \{1, 2\}$} \\
        &= \ff \Big(\Big((v_1)_{||} + (v_2)_{||} \Big) \hat{\ww} + \Big((v_1)_\perp + (v_2)_\perp) \Big) \hat{\ww}_\perp \Big) \\
        &= \sproj\Big(\Big[ \Big((v_1)_{||} + (v_2)_{||} \Big) \hat{\ww} + \Big((v_1)_\perp + (v_2)_\perp \Big) \hat{\ww}_\perp \Big] \rightarrow \ww \Big) \\
        &= (v_1)_{||} + (v_2)_{||} 
        = \sproj(\vv_1 \rightarrow \ww) + \sproj(\vv_2 \rightarrow \ww) = \ff(\vv_1) + \ff(\vv_2).
    \end{align*}

    And we have
    
    \begin{align*}
        \ff(c \vv) &= 
        \ff \Big(c \Big(v_{||}\hat{\ww} + v_{\perp}\hat{\ww}_\perp \Big) \Big), \text{ where $v_{||} = \sproj(\vv \rightarrow \ww)$} \\
        &= \ff\Big(cv_{||}\hat{\ww} + cv_{\perp}\hat{\ww}_\perp \Big)
        = \sproj\Big(\Big(cv_{||}\hat{\ww} + cv_{\perp}\hat{\ww}_\perp \Big) \rightarrow \ww \Big) \\
        &= c v_{||} \hat{\ww} = c \sproj(\vv \rightarrow \ww) = c \ff(\vv).
    \end{align*}
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::geom_dot_product_bilinear}
    (The dot product in $\R^2$ is a bilinear function).
    
    The dot product in $\R^2$ is a bilinear function. That is, both $\vv \mapsto \vv \cdot \ww$ and $\ww \mapsto \vv \cdot \ww$ are linear functions.
\end{theorem}

\begin{proof}
    The dot product in $\R^2$ is symmetric, so it suffices to show that it is a linear function in either argument; it suffices to show that $\ff:\R^2 \rightarrow \R$ defined by $\ff(\vv) = \vv \cdot \ww$ is a linear function. Well, ${\ff(\vv) = ||\ww||\sproj(\vv \rightarrow \ww)}$, and we know that $\vv \mapsto \sproj(\vv \rightarrow \ww)$ is linear. Since $\ff$ is the result of scaling a linear function by $||\ww||$, it too is a linear function.
\end{proof}

\begin{lemma}
    Let $\sE = \{\see_1, \see_2 \}$ be the standard basis for $\R^2$. We have
    
    \begin{align*}
        \see_i \cdot \see_j = 
        \begin{cases}
            1 & i = j \\
            0 & i \neq j
        \end{cases}
        = \delta_{ij}
    \end{align*}

    for all $i, j \in \{1, 2\}$.
\end{lemma}

\begin{proof}
   The lemma is equivalent to the statement $\sproj(\see_i \rightarrow \see_j) = \delta_{ij}$, which is easily proved.
\end{proof}

\begin{deriv}
    (Dot product in $\R^2$). 
    
    We can now derive the algebraic characterization of the dot product by using its bilinearity (Theorem \ref{ch::lin_alg::thm::geom_dot_product_bilinear}) together with the previous lemma.
    
    We will make use of the following general fact about bilinear functions\footnote{If $V_1, V_2$, and $W$ are vector spaces over a field $K$, then $\ff:V_1 \times V_2 \rightarrow W$ is a \textit{bilinear} function iff $\vv \mapsto \ff(\vv, \ww)$ and $\ww \mapsto \ff(\vv, \ww)$ are both linear functions.}. If $V$ is a finite-dimensional vector space over a field $K$ with a basis $E = \{\ee_i\}_{i = 1}^n$, then a bilinear function $B:V \times V \rightarrow K$ satisfies
    
    \begin{align*}
        B(\vv, \ww) &= B\Big(\sum_{i = 1}^n ([\vv]_E)_i \ee_i, \sum_{i = 1}^n ([\vv]_E)_j \ee_j\Big) \\
        &= \sum_{i = 1}^n ([\vv]_E)_i B\Big(\ee_i, \sum_{i = 1}^n ([\vv]_E)_j \ee_j\Big) \\
        &= \sum_{i = 1}^n \sum_{j = 1}^n ([\vv]_E)_i ([\ww]_E)_i B(\ee_i, \ee_j).
    \end{align*}
    
    In the above, we've written $\vv$ and $\ww$ as sums of basis vectors and then used the linearity of $B$ in each argument.
    
    The dot product is a bilinear function $\cdot:\R^2 \times \R^2 \rightarrow \R$, so the above fact can be applied to the dot product. We know from the previous lemma that $B(\see_i, \see_j) = \delta_{ij}$, so we have
    
    \begin{align*}
        \vv \cdot \ww = \sum_{i = 1}^2 \sum_{j = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i \delta_{ij} = \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i.
    \end{align*}
    
    Therefore
    
    \begin{align*}
        \boxed
        {
            \vv \cdot \ww = \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
        }
    \end{align*}
\end{deriv}

\subsubsection*{Algebraic $\implies$ geometric}

\begin{theorem}
\label{ch::lin_alg::lemma::orthogonal_linear_fns_preserve_alg_dot_product}
    (Preserved quantities).

    Let $\ff:\R^2 \rightarrow \R^2$. The following are equivalent.

    \begin{enumerate}
        \item ($\ff$ is length-preserving and fixes the origin). $||\vv|| = ||\ff(\vv)||$ for all $\vv \in \R^2$ and $\ff(\mathbf{0}) = \mathbf{0}$.
        \item ($\ff$ is linear and preserves algebraic dot product). $\ff$ is linear and $\vv \cdot \ww = \ff(\vv) \cdot \ff(\ww)$ for all $\vv, \ww \in \R^2$.
        \item ($\ff$ is linear, length-preserving, and angle-preserving). $\ff$ is linear, $||\vv|| = ||\ff(\vv)||$ for all $\vv \in \R^2$, and $\theta(\vv, \ww) = \theta(\ff(\vv), \ff(\ww))$ for all $\vv, \ww \in \R^2$. 
    \textbf{figure out if $\ff$ should be preserving of signed or unsigned angle}
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{itemize}
        \item ((1) $\implies$ (2)). Suppose $\ff$ is length-preserving and fixes the origin. Then $\ff$ is also linear, so it remains to show that $\ff$ preserves algebraic dot product. If we show that $\vv \cdot \ww$ depends on $||\vv||$ and $||\ww||$ (on lengths), then since $\ff$ preserves lengths, it follows that $\ff$ preserves algebraic dot product. This is what we will do.

        We need to show that $\vv \cdot \ww$ depends on $||\vv||$ and $||\ww||$. Note that $||\vv||^2 = \vv \cdot \vv$ for $\vv \in \R^n$. So \\ $||\vv + \ww||^2 = (\vv + \ww) \cdot (\vv + \ww) = \vv \cdot \vv + 2 \vv \cdot \ww + \ww \cdot \ww = ||\vv||^2 + 2 \vv \cdot \ww + ||\ww||^2$. Solve for $\vv \cdot \ww$ to see that $\vv \cdot \ww$ does indeed depend on $||\vv||$ and $||\ww||$:
        
       \begin{align*}
            \vv \cdot \ww = \frac{1}{2}\Big(||\vv + \ww||^2 - (||\vv||^2 + ||\ww||^2) \Big).
        \end{align*}
        
        ((1) $\impliedby$ (2)). If $\ff$ is linear and preserves dot product, then $||\ff(\vv)|| = \sqrt{\ff(\vv) \cdot \ff(\vv)} = \sqrt{\vv \cdot \vv} = ||\vv||$, so $\ff$ preserves length. Since $\ff$ is linear, it also fixes the origin. Therefore $\ff$ is length-preserving and fixes the origin, as desired.
        
        \item \mbox{} \\
        ((2) $\implies$ (3)). If $\ff$ preserves length, then, since angle is a function of quantities that are preserved by $\ff$ (dot product and length), $\theta(\vv, \ww) = \arccos\Big( \frac{\vv \cdot \ww}{||\vv|| \spc ||\ww||} \Big)$, it too is preserved by $\ff$. \\
        
        ((2) $\impliedby$ (3)). Suppose $\ff$ preserves angle: $\theta(\vv, \ww) = \theta(\ff(\vv), \ff(\ww))$ for all $\vv, \ww \in \R^2$. Then $\ff(\vv) \cdot \ff(\ww) = ||\ff(\vv)|| \spc ||\ff(\ww)|| \cos(\theta(\ff(\vv), \ff(\ww)))$
    \end{itemize}
\end{proof}

\begin{remark}
    (Orthogonal linear functions).

    Linear isometries are more often referred to as ``orthogonal linear functions''. [maybe say more about this]
    
    ``Orthogonal linear function'' is a somewhat misleading name. It's true that certain facts about orthogonal linear functions involve orthogonal \textit{vectors}, but orthogonal linear functions aren't ``perpendicular'' to other orthogonal linear functions in some sense.
\end{remark}

\begin{deriv}
    (Algebraic characterization $\implies$ geometric characterization).
    
    If

    \begin{align*}
        \vv \cdot \ww := \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
    \end{align*}

    then
    
    \begin{align*}
        \vv \cdot \ww := \sproj(\vv \rightarrow \ww) \spc ||\ww|| \text{ for all $\vv, \ww \in \R^2$}.
    \end{align*}
\end{deriv}

\begin{proof}
    Let $\vv, \ww \in \R^2$ and consider $\vv \cdot \ww$. We can\footnote{Use $\theta = \arccos(||\vv||)$.} choose $\theta \in [0, 2\pi)$ so that $\RR_\theta(\hvv) = \see_1$, i.e., so that $\RR_\theta(\vv) = ||\vv||\see_1$. With this choice of $\theta$, we have
    
    \begin{align*}
        \vv \cdot \ww 
        = \RR_\theta(\vv) \cdot \RR_\theta(\ww) 
        = ||\vv|| \see_1 \cdot \RR_\theta(\ww)
        = 
        \begin{pmatrix} ||\vv|| \\ 0 \end{pmatrix}
        \cdot 
        \begin{pmatrix} ([\RR_\theta(\ww)]_\sE)_1 \\ ([\RR_\theta(\ww)]_\sE)_2 \end{pmatrix} 
        = 
        ||\vv|| ([\RR_\theta(\ww)]_\sE)_1.
    \end{align*}
    
    We have
    
    \begin{align*}
        ([\RR_\theta(\ww)]_\sE)_1
        &= \proj(\RR_\theta(\ww) \rightarrow \see_1) 
        = \proj(\RR_\theta(\ww) \rightarrow \RR_\theta(\hvv))
        = \proj(\RR_\theta(\ww) \rightarrow \widehat{\RR_\theta(\vv)}) \\
        &= \proj(\RR_\theta(\ww) \rightarrow \RR_\theta(\vv)) 
        = \proj(\ww \rightarrow \vv),
    \end{align*}
    
    where the last equality is by Lemma \ref{ch::lin_alg::lemma::rotated_projection}. \\ Therefore $\vv \cdot \ww = ||\proj(\ww \rightarrow \vv)|| \spc ||\vv||$. Since the dot product is symmetric, we can swap $\vv$ and $\ww$; this gives the desired result.
\end{proof}

\begin{comment}
    \begin{theorem}
        (The dot product in $\R^2$ is bilinear).
    
        The dot product in $\R^2$ is a bilinear function. That is, both $\vv \mapsto \vv \cdot \ww$ and $\ww \mapsto \vv \cdot \ww$ are linear functions.
    \end{theorem}
    
    \begin{proof}
        The dot product in $\R^2$ is symmetric, so it suffices to show that it is linear in either argument; it suffices to show that $\vv \mapsto \vv \cdot \ww$ is a linear function. That is, it suffices to show that for all $\vv_1, \vv_2, \ww \in \R^2$ we have $(\vv_1 + \vv_2) \cdot \ww = \vv_1 \cdot \ww + \vv_2 \cdot \ww$ and for all $\vv, \ww \in \R^2$ and $c \in \R$ we have $(c\vv) \cdot \ww = c(\vv \cdot \ww)$.
        
        Note that because $[\cdot]_\sE$ is linear we have $([\vv_1 + \vv_2]_\sE)_i = ([\vv_1]_\sE)_i + ([\vv_2]_\sE)_i$ and $([c\vv]_\sE)_i = c([\vv]_\sE)_i$ for all $i$. Using this fact, we see
    
        \begin{align*}
            (\vv_1 + \vv_2) \cdot \ww
            &= \sum_{i = 1}^2 ([\vv_1 + \vv_2]_\sE)_i ([\ww]_\sE)_i
            = \sum_{i = 1}^2 \Big(([\vv_1]_\sE)_i + ([\vv_2]_\sE)_i\Big) ([\ww]_\sE)_i \\
            &= \sum_{i = 1}^2 \Big( ([\vv_1]_\sE)_i ([\ww]_\sE)_i + ([\vv_2]_\sE)_i ([\ww]_\sE)_i \Big)
            = \sum_{i = 1}^2 ([\vv_1]_\sE)_i ([\ww]_\sE)_i + \sum_{i = 1}^2 ([\vv_2]_\sE)_i ([\ww]_\sE)_i \\
            &= \vv_1 \cdot \ww + \vv_2 \cdot \ww
        \end{align*}
    
        and
    
        \begin{align*}
            (c\vv) \cdot \ww
            = \sum_{i = 1}^2 ([c\vv]_\sE)_i ([\ww]_\sE)_i
            = \sum_{i = 1}^2 c([\vv]_\sE)_i ([\ww]_\sE)_i
            = c \sum_{i = 1}^2 ([\vv]_\sE)_i ([\ww]_\sE)_i
            = c(\vv \cdot \ww),
        \end{align*}
    
        as claimed.    
    \end{proof}
\end{comment}

\section{Geometry in $\R^n$}

\subsection*{Length}

\begin{defn}
\label{ch::lin_alg::defn::length_in_Rn}
    (Length of a vector in $\R^n$). 

    When $n > 2$ and we have a vector $\vv_n \in \R^n$, we consider $\vv_n$ in the form $\vv_n = \vv_{n - 1} + ([\vv]_\sE)_n \see_n$, where $\vv_{n - 1} \in \R^{n - 1}$, and define the \textit{length} (or \textit{magnitude}) $||\vv_n||$ of $\vv_n$ to be

    \begin{align*}
        ||\vv_n|| = \sqrt{||\vv_{n - 1}||^2 + ([\vv]_\sE)_n^2} \text{ when $n > 2$}.
    \end{align*}
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::unit_vector_hat_notation}
    (Unit vector hat notation). 
    
    As we did in $\R^2$, we define for $\vv \in \R^n$ notation $\hat{\vv} := \frac{\vv}{||\vv||}$. We have $||\hat{\vv}|| = 1$ for all $\vv \in \R^2$.
\end{defn}

\begin{theorem}
    (Length of a vector in $\R^n$).

    For all $\vv \in \R^n$ and all $n$, the length of $\vv$ is

    \begin{align*}
        ||\vv|| = \sqrt{\sum_{i = 1}^n ([\vv]_\sE)_i^2}\spc.
    \end{align*}
\end{theorem}

\begin{proof}
    We prove the theorem by induction.

    (Base case). The definition of the length of a vector in $\R^2$ establishes the base case.

    (Inductive case). Let $\vv_n \in \R^n$. Assume as the inductive hypothesis that the claim is true for $n$; we need to show the claim is true for $n + 1$.

    By the definition the length of a vector in $\R^{n + 1}$ (Definition \ref{ch::lin_alg::thm::length_in_Rn}), for $\vv_{n + 1} \in \R^{n + 1}$ we have \\ ${||\vv_{n + 1}|| = \sqrt{||\vv_n||^2 + ([\vv]_\sE)_{n + 1}^2}}$. By the inductive hypothesis, $||\vv_n|| = \sqrt{\sum_{i = 1}^n ([\vv]_\sE)_i^2}^2$, so

    \begin{align*}
        ||\vv_{n + 1}|| = \sqrt{\sqrt{\sum_{i = 1}^n \Big(([\vv]_\sE)_i^2\Big)}^2 + ([\vv]_\sE)_{n + 1}^2} = \sqrt{\sum_{i = 1}^n \Big([\vv]_\sE)_i^2\Big) + ([\vv]_\sE)_{n + 1}^2} = \sqrt{\sum_{i = 1}^{n + 1} ([\vv]_\sE)_i^2}.
    \end{align*}

    This proves the claim for $n + 1$.
\end{proof}

\section{Length and angle [relocate the below to new updated sections above]}

\subsection*{Length and angle in $\R^n$}

The algebraic characterization of the dot product in $\R^2$ generalizes straightforwardly to $\R^n$.

\begin{defn}
    (Dot product in $\R^n$).

    We define the \textit{dot product of $\vv, \ww \in \R^n$} to be

    \begin{align*}
        \vv \cdot \ww = \sum_{i = 1}^n ([\vv]_\sE)_i ([\ww]_\sE)_i.
    \end{align*}
\end{defn}

\begin{theorem}
    (Length in $\R^n$).

    For all $\vv \in \R^n$ we have $||\vv|| = \sqrt{\vv \cdot \vv}$.
\end{theorem}

\begin{proof}
    We have $\vv \cdot \vv = \sum_{i = 1}^n ([\vv]_\sE)_i^2$. Take the square root of both sides to obtain the result.
\end{proof}

In $\R^2$, we were able to derive that $\theta(\vv, \ww) = \arccos(\hvv \cdot \hww)$. This result inspires our definition of angle in $\R^n$.

\begin{defn}
    (Angle in $\R^n$). 
    
    We define the \textit{angle between $\vv, \ww \in \R^n$} to be

    \begin{align*}
        \theta(\vv, \ww) := \arccos(\hvv \cdot \hww).
    \end{align*}
\end{defn}

\subsubsection*{Vector projection in terms of dot produt}

In the case of $n = 2$, we could have proved the following theorem immediately after defining $\vv \cdot \ww := \sproj(\vv \rightarrow \ww) \spc ||\ww||$ for $\vv, \ww \in \R^2$ in Definition [...]. The usefulness of this next theorem, however, wouldn't have been apparent until knowing that $\vv \cdot \ww = \sum_{i = 1}^n ([\vv]_\sE)_i ([\ww]_\sE)_i$.

\begin{theorem}
\label{ch::lin_alg::thm::vector_proj_dot_product}
    (Vector projection in $\R^n$ terms of dot product in $\R^n$).
    
    If $\vv, \ww \in \R^n$, then
    
    \begin{align*}
        \boxed
        {
            \proj(\vv \rightarrow \ww) = \frac{\vv \cdot \ww}{||\ww||} \hww = \frac{\vv \cdot \ww}{\ww \cdot \ww} \ww
        }
    \end{align*}
    
    In the case $n = 2$, if we didn't know the algebraic characterization of the dot product, this theorem would be a bit of a tautology. However, knowing the algebraic characterization of the dot product allows for easy computation of the expressions $\vv \cdot \ww$ and $\ww \cdot \ww$ whenever we know the coordinates of $\vv$ and $\ww$ relative to the standard basis\footnote{Actually, we only need to know the coordinates of $\vv$ and $\ww$ relative to some \textit{orthonormal} basis! Orthonormal bases, also known as \textit{self-dual bases}, are discussed in Section \ref{ch::bilinear_forms_metric_tensors::section::self-duality}.} for $\R^2$.
\end{theorem}

\begin{proof}
   Since $\vv \cdot \ww = \sproj(\vv \rightarrow \ww) \spc ||\ww||$, then $\sproj(\vv \rightarrow \ww) = \frac{\vv \cdot \ww}{||\ww||}$, and, by the first fact of Theorem \ref{ch::lin_alg::thm::vector_proj_Signed_magnitude}, $\proj(\vv \rightarrow \ww) = \sproj(\vv \rightarrow \ww) \hww = \frac{\vv \cdot \ww}{||\ww||} \hww$.
\end{proof}

\subsubsection*{The law of cosines}

The next theorem presents the law of cosines as it is best understood, which is as a consequence of the equivalence between the geometric and algebraic dot product formulas.

\begin{theorem}
\label{ch::lin_alg::thm::law_of_cosines}
    (Law of cosines in $\R^n$). 
    
    We have

    \begin{align*}
        ||\vv - \ww||^2 = ||\vv||^2 + ||\ww||^2 - 2 ||\vv|| \spc ||\ww|| \cos(\theta(\vv, \ww)).
    \end{align*}
    
    Note that when $\theta(\vv, \ww) = 0$, we recover the Pythagorean theorem.
\end{theorem}

\begin{proof}
    We have $||\vv - \ww||^2 = (\vv - \ww) \cdot (\vv - \ww) = \vv \cdot \vv - 2 \vv \cdot \ww + \ww \cdot \ww = ||\vv||^2 + ||\ww||^2 - 2 ||\vv|| \spc ||\ww|| \cos(\theta)$.
\end{proof}

\newpage

\subsection*{Inner product spaces}

This section generalizes the notion of the dot product, which we only know to be applicable to $\R^n$, to general vector spaces.

\begin{defn}
    \label{ch::lin_alg::defn::inner_product}
    (Inner product).
    
    Let $V$ be a vector space over a field $K$. An \textit{inner product} on $V$ is a function $\langle \cdot, \cdot \rangle:V \times V \rightarrow K$ with the following properties:

    \begin{enumerate}
        \item (Bilinearity). The functions $\vv \mapsto \langle \vv, \ww \rangle$ and $\ww \mapsto \langle \vv, \ww \rangle$ are linear for all $\vv, \ww \in V$. In other words $\langle \cdot, \cdot \rangle$ is ``linear in each argument''.
        \item (Symmetry). $\langle \vv, \ww \rangle = \langle \ww, \vv \rangle$ for all $\vv, \ww \in V$.
        \item (Nondegeneracy). $\langle \vv, \vv \rangle = 0 \implies \vv = \mathbf{0}$ for all $\vv \in V$.
    \end{enumerate}

    Inner products are called ``inner products'' because they generalize the dot product, which satisfies ${\vv \cdot \ww = \vv^\top \ww}$ for all $\vv, \ww \in K^n$; notice how the position of the transpose $\top$ is ``inside'' $\vv$ and $\ww$, here. Contrastingly, the ``outer product'' of vectors $\ww, \vv \in K^n$ is sometimes defined to be $\ww \vv^\top$, since the position of the transpose $\top$ is on the ``outside'' of $\ww$ and $\vv$. We will learn more about the outer product in Remark \ref{ch::motivated_intro::rmk::outer_products}.
\end{defn}

\begin{remark}
\label{ch::lin_alg::rmk::inner_product_conventions}
    (Inner product conventions).

    Some authors define inner products to satisfy the ``positive-definitness'' criterion rather than the nondegeneracy criterion. A function $V \times V \rightarrow K$ is \textit{positive-definite} iff $\langle \vv, \vv \rangle \geq 0$ for all $\vv \in V$, with $\langle \vv, \vv \rangle = 0$ occurring only when $\vv = \mathbf{0}$. A \textit{negative-definite} inner product is defined similarly and involves the condition $\langle \vv, \vv \rangle \leq 0$ for all $\vv \in V$.
\end{remark}

\begin{defn}
    (Inner product space).
    
    Let $V$ be a vector space over $K$. Iff there is an inner product $\langle \cdot, \cdot \rangle$ on $V$, then $V$ is called a \textit{vector space with inner product}, or an \textit{inner product space}.
\end{defn}

\begin{example}
    \mbox{} \\ \indent
    The dot product in $\R^n$ is an inner product on $\R^n$. (Proof left as exercise). 
    
    The dot product in $K^n$, defined analogously to the dot product in $\R^n$, is in general \textit{not} an inner product because it is not positive-definite. For example, we have $\begin{pmatrix} 3 \\ 3 \end{pmatrix} \cdot \begin{pmatrix} 3 \\ 3 \end{pmatrix} = 0$ when these vectors are elements of $\Z/9\Z$. (In $\Z/9\Z$, we have $3 \cdot 3 = 9 = 0$).
\end{example}

\subsubsection*{Length and orthogonality with respect to an inner product}

\begin{defn}
    (Length of a vector with respect to an inner product). 
    
    Let $V$ be an inner product space. In analogy to the fact that the length of a vector in $\R^n$ can be expressed using the dot product in $\R^n$ (see Theorem \ref{ch::lin_alg::thm::length_in_Rn}), we define the \textit{length of a vector $\vv \in V$ with respect to the inner product on $V$} to be $||\vv|| := \sqrt{\langle \vv, \vv \rangle}$.
\end{defn}

\begin{defn}
    (Angle between vectors with respect to an inner product). 
    
    Let $V$ be an inner product space. In analogy to the the fact that the angle between vectors $\vv_1, \vv_2 \in \R^n$ is $\arccos(\hat{\vv}_1 \cdot \hat{\vv}_2)$, we define the \textit{angle $\theta$ between vectors $\vv_1, \vv_2 \in V$ with respect to the inner product on $V$} to be $\theta := \arccos(\langle \hat{\vv}_1, \hat{\vv}_2 \rangle)$. Note that $\hat{\vv}_i = \frac{\vv_i}{||\vv_i||} = \frac{\vv_i}{\sqrt{\langle \vv_i, \vv_i \rangle}}$.
\end{defn}

\begin{remark}
    (Geometric inner product).
    
    Let $V$ be an inner product space. Then $\langle \vv_1, \vv_2 \rangle = ||\vv_1||\spc||\vv_2|| \cos(\theta)$, where $\theta$ is the angle between $\vv_1$ and $\vv_2$ with respect to the inner product on $V$. (This fact is the generalization of the geometric dot product in $\R^n$, which was discussed in Theorem \ref{ch::lin_alg::thm::dot_prod_and_angle}).
\end{remark}

\begin{theorem}
\label{ch::bilinear_forms_metric_tensors::thm::Cauchy_Schwarz}
     (Cauchy-Schwarz inequality for vector spaces over $\R$).
     
     Let $V$ be a vector space over $\R$ with inner product. Then the \textit{Cauchy-Schwarz inequality} holds: \\ ${\langle \vv_1, \vv_2 \rangle \leq ||\vv_1|| \spc ||\vv_2|| \text{ for all $\vv_1, \vv_2 \in V}}$. 
     
     Note, the Cauchy-Schwarz inequality is equivalent to the statement that the angle $\theta$ in $V$ between $\vv_1$ and $\vv_2$ with respect to the inner product on $V$ satisfies $\theta \in [0, 2 \pi)$.
\end{theorem}

\begin{proof}
    Define $f:\R \rightarrow [0, \infty) \subseteq \R$ by $f(k) = \langle k \vv_1 + \vv_2, k \vv_1 + \vv_2 \rangle = k^2 \langle \vv_1, \vv_1 \rangle + 2 k \langle \vv_1, \vv_2 \rangle + \langle \vv_2, \vv_2 \rangle$. Set $a := \langle \vv_1, \vv_1 \rangle$, $b := 2 \langle \vv_1, \vv_2 \rangle$, and $c := \langle \vv_2, \vv_2 \rangle$, so that $f(k) = ak^2 + bk + c$.
    
    Since $\langle \cdot, \cdot \rangle$ is positive-definite, then $f$ is nonnegative, and therefore must have either one or zero real roots. According to the quadratic formula, one real root occurs when $b^2 - 4ac = 0$, and zero real roots occur when $b^2 - 4ac < 0$. So, we must have $b^2 - 4ac \leq 0$. 
    
    Using our expressions for $a, b$, and $c$, we see that $(4 \langle \vv_1, \vv_2 \rangle)^2 - 4(\langle \vv_1, \vv_2 \rangle)(\langle \vv_2, \vv_2 \rangle) \leq 0$. Thus \\ ${\langle \vv_1, \vv_2 \rangle^2 \leq \langle \vv_1, \vv_1 \rangle \langle \vv_2, \vv_2 \rangle = ||\vv_1||^2 ||\vv_2||^2}$. Take the square root of each side to obtain the result.  
\end{proof}

\begin{defn}
    (Orthogonality of vectors with respect to an inner product). 
    
    Let $V$ be an inner product space. We say vectors $\vv_1, \vv_2 \in V$ are \textit{orthogonal with respect to the inner product on $V$} iff the angle between $\vv_1$ and $\vv_2$ is $\frac{\pi}{2}$. That is, $\vv_1, \vv_2 \in V$ are orthogonal iff $\langle \vv_1, \vv_2 \rangle = 0$.
\end{defn}

\newpage

\begin{defn}
    (Orthonormal basis with respect to an inner product).
    
     Let $V$ be a finite-dimensional vector space with inner product $\langle \cdot, \cdot \rangle$. We say a basis $E = \{\ee_1, ..., \ee_n\}$ of $V$ is \textit{orthonormal (with respect to $\langle \cdot, \cdot \rangle$)} iff
     
     \begin{itemize}
         \item $||\ee_i|| = 1$ for all $i$
         \item $\ee_i$ and $\ee_j$ are orthogonal to each other when $i \neq j$
     \end{itemize}
     
     That is, $E$ is an orthonormal basis iff $\langle \ee_i, \ee_j \rangle = \delta_{ij}$ for all $i, j$.
\end{defn}

\begin{theorem}
\label{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}
    (Gram-Schmidt algorithm).
    
    Let $V$ be a finite-dimensional inner product space. Given any basis $E = \{\ee_1, ..., \ee_n\}$ for $V$, we can use the following \textit{Gram-Schmidt algorithm} to convert $E$ into an orthonormal basis $\hU = \{\huu_1, ..., \huu_n\}$.
    
    First, we ``orthogonalize'' the basis $E$ into a basis $F = \{\ff_1, ..., \ff_n\}$. Set $\ff_1 := \ee_1$, and, for $i \geq 2$, set
    
    \begin{align*}
        \ff_i := \ee_i - \proj(\ff_i \rightarrow \spann(\ee_1, ..., \cancel{\ee_i}, ..., \ee_n)) = \ee_i - \sum_{j \neq i} \proj(\ff_i \rightarrow \ee_j) = \ee_i - \sum_{j \neq i} \frac{\langle \ff_i, \ee_j \rangle}{\langle \ee_j, \ee_j \rangle}, \quad i \geq 2.
    \end{align*}
    
    (In the last equality in the line above, we've used an analogue of Theorem \ref{ch::lin_alg::thm::vector_proj_dot_product} to express vector projections in terms of inner products). 
    To obtain the orthonormal basis $\hU = \{\huu_1, ..., \huu_n\}$, we just normalize the orthogonal basis $F$, and set $\huu_i := \frac{\ff_i}{||\ff_i||}$.
\end{theorem}

\begin{remark}
    The above theorem reveals that the algebraic dot product (in $\R^2$) can also be discovered as an orthogonality condition between vectors. When $\vv_1, \vv_2 \in \R^2$ are orthogonal, they form a right triangle, so Pythagorean theorem gives $||\vv_1||^2 + ||\vv_2||^2 = ||\vv_1 - \vv_2||^2$. Use $||\vv_i||^2 = \sum_{j = 1}^2 (([\vv_i]_\sE)_j)^2$ to discover that we must have $([\vv_1]_\sE)_1 ([\vv_2]_\sE)_1 + ([\vv_1]_\sE)_2 ([\vv_2]_\sE)_2 = 0$.
\end{remark}

\newpage

\section{Systems of linear equations}

This section presents how systems of linear equations can be solved by using concepts from linear algebra.

Reading this section is entirely unnecessary for the remaining content in this book. This treatment of systems of linear equations has only been provided to serve as a superior alternative to the traditional approach of teaching linear algebra (which is: start with systems of linear equations, define the matrix-vector product, and then haphazardly discover the other material we have covered). The approach we take emphasizes that linear algebra should not be introduced as a field of study that is discovered by starting with systems of linear equations\footnote{Linear algebra \textit{did} historically grow out of the study of systems of linear equations. Just remember that the historical approach is not always the most enlightening one!}, and that systems of linear equations should be viewed as an application of linear algebra.

\begin{defn}
    (Linear equation).

    Let $V$ and $W$ be vector spaces, let $\ff:V \rightarrow W$ be a function, and let $\ww \in W$. When $\ff$ is linear, the equation
    
    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in V$}
    \end{align*}
    
    is called a \textit{linear equation}.

    The set of solutions to the equation, $\{\text{solns}\} := \{\vv \in V \mid \ff(\vv) = \ww\}$, is a vector space whenever it is nonempty.
\end{defn}

We first present a recursive characterization of solutions to linear equations. This isn't an algorithm for finding solutions; it's only a characterization.

\begin{theorem}
\label{ch::lin_alg::thm::solutions_to_linear_equations}
    (Solutions to linear equations).

    Let $V$ and $W$ be vector spaces, let $\ff:V \rightarrow W$ be a linear function, and let $\ww \in W$. Consider the linear equation

    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in V$}.
    \end{align*}
    
    The set of solutions $\{\text{solns}\}$ is the preimage $\{\text{solns}\} := \ff^{-1}(\{\ww\})$. Recall from Theorem \ref{ch::lin_alg::thm::recursive_preimage_char} that when it is nonempty, this preimage can be described in recursively:
    
    \begin{align*}
        \{\text{solns}\} = \{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} \text{ for any $\vv \in \{\text{solns}\}$}.
    \end{align*}

    This theorem is very useful when in the case when $\ff$ is not invertible, as it says that we can obtain the entire solution set by finding a \textit{single} solution to the equation, finding the kernel of $\ff$, and then combining these two pieces of information in a simple way.
\end{theorem}

The above characterization implies the following theorem.

\begin{theorem}
    (Number of solutions to a linear equation).

    A linear equation has either zero, one, or infinitely many solutions.
\end{theorem}

\begin{proof}
    Let $V$ and $W$ be finite-dimensional vector spaces, let $\ff:V \rightarrow W$ be a linear function, and let $\ww \in W$. Consider the linear equation $\ff(\vv) = \ww$.

    (Case: there are no solutions). There are zero solutions in this case.

    \fullindent
    {
        (Case: there are solutions).
    
        \fullindent
        {
            (Case: $\ff$ is invertible). We saw in the previous proof that the solution set is $\{\ff^{-1}(\ww)\}$ in this case, so, there is one solution in this case.
        }
    
        \fullindent
        {
            (Case: $\ff$ is not invertible). In general, the solution set is $\{\vv + \vv_0 \mid \vv_0 \in \ker(\ff)\}$. Since $\ff$ is not invertible, then $\ker(\ff)$ has dimension at least $1$, and so the solution set is a vector-space\footnote{Check for yourself that when $\ker(\ff)$ has dimension of at least one, the solution set is a vector space with dimension equal to that of $\ker(\ff)$.} of dimension at least $1$. Vector spaces of dimension at least $1$ contain infinitely many elements, so the solution set is infinite when $\ff$ is not invertible.
        }
    }
\end{proof}

The first theorem in this chapter recursively characterizes the problem of solving linear equations, telling us that in order to solve $(\ff(\vv) = \ww \text{ for all $\vv \in V$})$, we have to solve another linear equation, $(\ff(\vv) = \mathbf{0} \text{ for all $\vv \in V$})$. We haven't yet discussed a method for actually solving either of these linear equations, though!

We address this now. In order to solve a linear equation, we will transform it into an equivalent system of linear equations, and then solve that.

\begin{deriv}
    (System of linear equations corresponding to a linear equation).
    
    Let $V$ and $W$ be $n$- and $m$-dimensional vector spaces over a field $K$, let $\ff:V \rightarrow W$ be a linear function, let $\ww \in W$, and consider the linear equation

    \begin{align*}
        \ff(\vv) = \ww \text{ for all $\vv \in V$}.
    \end{align*}

    Making use of the characterizing property of the matrix $[\ff(E)]_F$ of $\ff$ relative to $E$ and $F$ (recall the result at the end of Derivation \ref{ch::lin_alg::deriv::matrix_relative_to_bases}), the above equation is equivalent to

    \begin{align*}
       [\ff(E)]_F [\vv]_E = [\ww]_F \text{ for all $\vv \in V$}.
    \end{align*}

    If we let $[\ff(E)]_F$ vary\footnote{We can vary $\AA := [\ff(E)]_F$ over $K^{m \times n}$ because for any set of vectors, there exists a linear function sending basis vectors to those vectors (recall Theorem \ref{ch::lin_alg::thm::basis_sent_to_any_ordered_list}). We can let $[\vv]_E$ vary over $K^n$ and $[\ww]_F$ vary over $K^m$ because $[\cdot]_E$ and $[\cdot]_F$ are invertible.} over $K^{m \times n}$, $\xx := [\vv]_E$ vary over $K^n$, and $\bb:= [\ww]_F$ vary over $K^m$, we see that the above equation is of the form

    \begin{align*}
        \AA \xx = \bb \text{ for all $\xx \in K^n$}.
    \end{align*}
    
    Equations of the above form are called \textit{(linear) matrix equations}\footnote{Unfortunately, in practice, \textit{matrix equation} usually means \textit{linear matrix equation}.}.
    
    Notice that if $\xx \in K^n$ has $i$th component $x_i$ and $\bb \in K^m$ has $i$th component $b_i$, then the above linear matrix equation is the same as

    \begin{align*}
        \begin{pmatrix}
            a_{11} & \hdots & a_{1n} \\
            \vdots & & \vdots \\
            a_{m1} & \hdots & a_{mn}
        \end{pmatrix}
        \begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix}
        &=
        \begin{pmatrix}
            b_1 \\ \vdots \\ b_m
        \end{pmatrix}
        \text{ for all $x_1, ..., x_n \in K$}
        \\
        \begin{pmatrix}
            x_1 a_{11} + ... + x_n a_{1n} \\
            \vdots \\
            x_1 a_{m1} + ... + x_n a_{mn}
        \end{pmatrix}
        &=
        \begin{pmatrix}
            b_1 \\ \vdots \\ b_m
        \end{pmatrix}
        \text{ for all $x_1, ..., x_n \in K$},
    \end{align*}

    which is equivalent to the system of linear equations

    \begin{align*}
        \begin{cases}
            x_1 a_{11} + ... + x_n a_{1n} = b_1 \\
            \vdots \\
            x_1 a_{m1} + ... + x_n a_{mn} = b_m
        \end{cases}
        \text{ for all $x_1, ..., x_n \in K$}.
    \end{align*}

    Each equation in this system is itself a linear equation because the function $(x_1, ..., x_n) \mapsto x_1 a_{i1} + ... + x_n a_{in}$ is linear for all $i$.
\end{deriv}

\subsection*{Finding bases for the kernel and image of a matrix}

% \begin{proof}
%     Proof. Let $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}$ and $\rref(\AA) = \begin{pmatrix} \rr_1 & \hdots & \rr_n \end{pmatrix}$.
    
%     (Case: $\rref(\AA)$ has no pivot columns). Then $\rref(\AA) = \mathbf{0}$.
    
%     (Case: $\rref(\AA)$ has pivot columns). 
%     Let $x_{j_1}, ..., x_{j_p}$ be the pivot variables of $\rref(\AA)$ and let $x_{k_1}, ..., x_{k_q}$ be the free variables of $\rref(\AA)$. Now consider $\SS := \begin{pmatrix} \rr_{j_1} & \hdots & \rr_{j_p} & \rr_{k_1} & \hdots & \rr_{k_q} \end{pmatrix}$. Since for each $\alpha$, the vector $\rr_{j_\alpha}$ is the pivot column of index $\alpha$ from $\rref(\AA)$, then $\rr_{j_\alpha} = \see_{j_\alpha}$. Thus $\SS = \begin{pmatrix} \see_{j_1} & \hdots & \see_{j_p} & \rr_{k_1} & \hdots & \rr_{k_q} \end{pmatrix}$. Now, since row-reduction is an invertible linear operation, there is an invertible matrix $\RR$ for which $\rref(\AA) = \RR \AA$. We thus have $\AA = \RR^{-1} \rref(\AA)$, or equivalently, $\aa_i = \RR^{-1} \rr_i$.
    
%     ================= 
    
%     $= (\RR^{-1}\see_1, ..., \RR^{-1}\see_k, \RR^{-1}\bb_{k + 1}, ..., \RR^{-1}\bb_n)$. Recall from Theorem \ref{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence} that because $\RR^{-1}$ is invertible it preserves linear independence. Thus, since $\see_1, ..., \see_k$ are linearly independent, $\aa_1 = \RR^{-1} \see_1, ..., \aa_k = \RR^{-1} \see_k$ are linearly independent.
    
%     [need to prove that $\see_1, ..., \see_k$ span $\im(\rref(\AA))$]
% \end{proof}


\begin{comment}
\begin{deriv}
    (Basis for the kernel of a matrix).

    The previous lemma immediately implies that $\ker(\AA) = \ker(\rref(\AA))$. Thus, if we can find a basis for $\ker(\rref(\AA))$, it is also a basis of $\ker(\AA)$.

    Let $(r_{ij}) = \rref(\AA)$. We have that $\xx = (x_1, ..., x_n) \in K^n$ is in $\ker(\rref(\AA))$ iff $\rref(\AA) \xx = \mathbf{0}$, that is, iff this system of equations holds:

    \begin{align*}
        \begin{cases}
            x_1 r_{11} + ... + x_n r_{1n} = 0 \\
            \vdots \\
            x_1 r_{m1} + ... + x_n r_{mn} = 0
        \end{cases}
        \text{ for all $x_1, ..., x_n \in K$}.
    \end{align*}

    Now, we consider two cases.
    
    (Case: $\rref(\AA) \neq \mathbf{0}$). In this case, $\rref(\AA)$ has at least one pivot variable. So, we can let ${x_{j_1}, ..., x_{j_p} \in \{x_1, ..., x_n\}}$ be the pivot variables of $\rref(\AA)$ and let $y_{k_1}, ..., y_{k_q} \in \{x_1, ..., x_n\}$ be the free variables of $\rref(\AA)$. Since $\rref(\AA)$ is in RREF, the $i$th row consists of a $1$ in the $j_i$ column, $0$'s in all other pivot columns, and $r_{j_i k_\alpha}$ in the free-variable-column of index $k_\alpha$. Thus, the system can be written as
    
    \begin{align*}
        \begin{cases}
            x_{j_1} + \sum_{\alpha = 1}^q r_{j_1 k_\alpha} y_{k_\alpha} = 0 \\
            \vdots \\
            x_{j_p} + \sum_{\alpha = 1}^q r_{j_p k_\alpha} y_{k_\alpha} = 0
        \end{cases}
        \text{ for all $x_{j_1}, ..., x_{j_p}, y_{k_1}, ..., y_{k_q} \in K$}.
    \end{align*}

    Solving for the pivot variables, and then appending $q$ equations relating the free variables to themselves, we obtain the system

    \begin{align*}
        \begin{cases}
            x_{j_1} = -\sum_{\alpha = 1}^q r_{j_1 k_\alpha} y_{k_\alpha} \\
            x_{j_2} = -\sum_{\alpha = 1}^q r_{j_2 k_\alpha} y_{k_\alpha} \\
            \vdots \\
            x_{j_p} = -\sum_{\alpha = 1}^q r_{j_p k_\alpha} y_{k_\alpha}  \\
            y_{k_1} = y_{k_1} \\
            y_{k_2} = y_{k_2} \\
            \vdots \\
            y_{k_q} = y_{k_q} \\
        \end{cases}
        \text{ for all $x_{j_1}, ..., x_{j_p}, y_{k_1}, ..., y_{k_q} \in K$}
    \end{align*}

    (Case: $\rref(\AA) = \mathbf{0}$). In this case, the system of equations corresponding to the linear-matrix equation ${\rref(\AA) \xx = \mathbf{0}}$ is the trivial system that only contains copies of the equation $(0 = 0)$. If we set $x_{j_1} = ... = x_{j_p} = y_{k_1}, ..., y_{k_q} := 0$, then the last system from the previous case is equal to this trivial system.

    \vspace{.25cm}

    We've seen that in either case, the system of equations corresponding to $\rref(\AA) \xx = \mathbf{0}$ is equivalent to the one shown at the end of the case $\rref(\AA) \neq \mathbf{0}$. That system of equations is equivalent to the vector equation

    \begin{align*}
        \begin{pmatrix}
            x_{j_1} \\
            x_{j_2} \\
            \vdots \\
            x_{j_p} \\
            y_{k_1} \\ 
            y_{k_2} \\
            \vdots \\
            y_{k_q}
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\sum_{\alpha = 1}^q r_{j_1 k_\alpha} y_{k_\alpha} \\
            -\sum_{\alpha = 1}^q r_{j_2 k_\alpha} y_{k_\alpha} \\
            \vdots \\
            -\sum_{\alpha = 1}^q r_{j_p k_\alpha} y_{k_\alpha} \\
            y_{k_1} \\
            y_{k_2} \\
            \vdots \\
            y_{k_q}
        \end{pmatrix}
        =
        y_{k_1}
        \begin{pmatrix}
            -r_{j_1 k_1} \\
            -r_{j_2 k_1} \\
            \vdots \\
            -r_{j_p k_1} \\
            1 \\
            0 \\
            \vdots \\
            0
        \end{pmatrix}
        +
        y_{k_2}
        \begin{pmatrix}
            -r_{j_1 k_2} \\
            -r_{j_2 k_2} \\
            \vdots \\
            -r_{j_p k_2} \\
            0 \\
            1 \\
            \vdots \\
            0
        \end{pmatrix}
        + ... +
        y_{k_q}
        \begin{pmatrix}
            -r_{j_1 k_q} \\
            -r_{j_2 k_q} \\
            \vdots \\
            -r_{j_p k_q} \\
            0 \\
            0 \\
            \vdots \\
            1
        \end{pmatrix}
        \text{ for all $x_{j_1}, ..., x_{j_p}, y_{k_1}, ..., y_{k_q} \in K$}.
    \end{align*}

    This vector equation is equivalent to the following set-membership statement:

    \begin{align*}
        \begin{pmatrix}
            x_{j_1} \\
            x_{j_2} \\
            \vdots \\
            x_{j_p} \\
            y_{k_1} \\ 
            y_{k_2} \\
            \vdots \\
            y_{k_q}
        \end{pmatrix}
         &\in
        \spann
        \left(\left\{
            \begin{pmatrix}
                -r_{j_1 k_1} \\
                -r_{j_2 k_1} \\
                \vdots \\
                -r_{j_p k_1} \\
                1 \\
                0 \\
                \vdots \\
                0
            \end{pmatrix}
            ,
            \begin{pmatrix}
                -r_{j_1 k_2} \\
                -r_{j_2 k_2} \\
                \vdots \\
                -r_{j_p k_2} \\
                0 \\
                1 \\
                \vdots \\
                0
            \end{pmatrix}
            , ...,
            \begin{pmatrix}
                -r_{j_1 k_q} \\
                -r_{j_2 k_q} \\
                \vdots \\
                -r_{j_p k_q} \\
                0 \\
                0 \\
                \vdots \\
                1
            \end{pmatrix}
        \right\}\right)
        \text{ for all $x_{j_1}, ..., x_{j_p}, y_{k_1}, ..., y_{k_q} \in K$}.
    \end{align*}

    Each vector in the set inside the above span is of the form $\begin{pmatrix} \vv_\alpha \\ \see_\alpha \end{pmatrix}$, where $\vv_\alpha \in K^p$, for each $\alpha$. Using this convenient notation, the entirety of what we've shown so far can now be stated as

    \begin{align*}
        \xx =
        \begin{pmatrix}
         x_1 \\
         \vdots \\
         x_n
        \end{pmatrix}
        \in \ker(\AA)
        \iff
        \begin{pmatrix}
            x_{j_1} \\
            x_{j_2} \\
            \vdots \\
            x_{j_p} \\
            y_{k_1} \\ 
            y_{k_2} \\
            \vdots \\
            y_{k_q}
        \end{pmatrix} \in \spann\left( \left\{\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}, ..., \begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}\right\} \right),
        \text{ where $\vv_1, ..., \vv_q \in K^p$}.
    \end{align*}
    
    Let $\ss:K^{m \times n} \rightarrow K^{m \times n}$ denote the linear function that swaps vector entries such that $\ss((x_{j_1}, ..., x_{j_p}, x_{k_1}, ..., x_{k_1}))^\top = (x_1, ..., x_n)^\top = \xx$. Applying $\ss$ to both sides of the right set membership statement to the right of the $\iff$, we have

    \begin{align*}
        \xx \in \ker(\AA)
        \iff
        \xx \in \spann\left( \left\{\ss\left(\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}\right), ..., \ss\left(\begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}\right) \right\} \right),
        \text{ where $\vv_1, ..., \vv_q \in K^p$}.
    \end{align*}

    and thus 

    \begin{align*}
        \ker(\AA) = \spann\left( \left\{\ss\left(\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}\right), ..., \ss\left(\begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}\right) \right\} \right),
        \text{ where $\vv_1, ..., \vv_q \in K^p$}.
    \end{align*}

    Now consider the set of vectors in the span. Since $\see_1, ..., \see_1$ are linearly independent, it follows that $\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}, ..., \begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}$ are linearly independent. Since $\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}, ..., \begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}$ are linearly independent, and as $\ss$ is an invertible linear function, then the above set of vectors is linearly independent, too (recall Theorem \ref{ch::lin_alg::thm::one_to_one_linear_fns_are_the_linear_fns_preserving_linear_independence}).
    
    Since the above set of vectors is a linearly independent spanning set for $\ker(\AA)$, it is a basis for $\ker(\AA)$.

    How did we obtain this basis? Well, since the vectors $\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}, ..., \begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}$ are obtained by using the [parametric form method] to compute an expression for $(x_{j_1}, ..., x_{j_p}, x_{k_1}, ..., x_{k_q})^\top$, and since $\ss$ swaps entries so that $\ss((x_{j_1}, ..., x_{j_p}, x_{k_1}, ..., x_{k_q})) = (x_1, ..., x_n)^\top = \xx$, then $\ss\left(\begin{pmatrix} \vv_1 \\ \see_1 \end{pmatrix}\right), ..., \ss\left(\begin{pmatrix} \vv_q \\ \see_q \end{pmatrix}\right)$ is the set of vectors in the set-argument of the span that is obtained by using [the parametric form method] to compute an expression for $(x_1, ..., x_n)^\top$.
    
    Looking back through the derivation, we see that the basis was obtained by applying [the parametric form method] to the system of equations corresponding to $(\rref(\AA) \xx = \mathbf{0} \text{ for all $\xx \in K^n$})$.

    Therefore,

    \begin{empheq}[box = \fbox]{align*}
        \text{The set of vectors obtained from applying [the parametric form method] to the equation} \\
        \text{($\rref(\AA) \xx = \mathbf{0}$  for all $\xx \in K^n$)} \\
        \text{ is a basis for $\ker(\AA)$}
    \end{empheq}
\end{deriv}
\end{comment}

\begin{theorem}
    ().
    
    row operations preserve the kernel

    column operations preserve the image
\end{theorem}

\begin{theorem}
    (Basis for the kernel of a matrix).
    
    Let $\AA$ be an $m \times n$ matrix with entries in a field $K$. Since row operations preserve the kernel, then $\ker(\rref(\AA)) = \ker(\AA)$.
    
    The following algorithm produces a basis for $\ker(\rref(\AA)) = \ker(\AA)$.

    \begin{enumerate}
        \item Since $\xx \in \ker(\rref(\AA))$ iff $\rref(\AA) \xx = \mathbf{0}$, consider the system of equations corresponding to the linear matrix equation $\rref(\AA) \xx = \mathbf{0}$. Let $x_{j_1}, ..., x_{j_p} \in \{x_1, ..., x_n\}$ be the pivot variables of $\rref(\AA)$ and let $x_{k_1}, ..., x_{k_q} \in \{x_1, ..., x_n\}$ be the free variables of $\rref(\AA)$. 
        \item Obtain an equivalent system of equations by solving each equation of the previous system for its pivot variable, so that the $i$th equation in the new system is $x_{j_i} = c_{i1} x_{k_1} + ... + c_{ip} x_{k_q}$ for some $c_{i1}, ..., c_{ip} \in K$.
        \item Obtain another equivalent system by appending the $q$ equations $x_{k_1} = x_{k_1}$, ..., $x_{k_q} = x_{k_q}$, which express the free variables in terms of themselves, to the previous system of equations.
        \item Consider the vector equation that corresponds to the previous system of linear equations. The left side of this vector equation is $(x_1, ..., x_n)^\top$. The right side has $i$th entry $c_{i_1} x_{k_1} + ... + c_{i_p} x_{k_q}$ when $i$ is the index of a pivot variable and $i$th entry $x_i$ when $i$ is the index of a free variable.
        \item Rewrite the previous vector equation so that it is of the form

        \begin{align*}
            \xx =
            \begin{pmatrix}
                x_1 \\ \vdots \\ x_n
            \end{pmatrix}
            = 
            x_{k_1} \vv_1 + … + x_{k_q} \vv_q,
        \end{align*}

        for some $\vv_1, ..., \vv_q \in K^n$. In this equation, $\xx$ is expressed as a weighted sum of vectors, where the weights are the free variables.

        \item Since all of the previous systems of equations and vector equations are equivalent, we clearly have ${\xx \in \ker(\rref(\AA))}$ iff $\xx \in \spann(\{\vv_1, ..., \vv_q\})$. Thus $\ker(\AA) = \ker(\rref(\AA)) = \spann(\{\vv_1, ..., \vv_q\})$. 
        In the below, we show that $\{\vv_1, ..., \vv_q\}$ is linearly independent and is thus a basis for $\ker(\rref(\AA)) = \ker(\AA)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    We prove that the set $\{\vv_1, ..., \vv_q\}$ returned by the algorithm is linearly independent.

    The form of the system of equations in (4) implies that in (5), the vector $\vv_j$ has an $i$th entry of $\delta_{ij}$ when $i$ is restricted to be an index of a free variable. It follows from this constraint that $\vv_1, ..., \vv_q$ are linearly independent. 
    
    To see this, consider the equation $c_1 \vv_1 + … + c_q \vv_q = 0$, where $c_1, ..., c_q \in K$. Since, when $i$ is the index of a free variable, the $i$th entry of $c_j \vv_j$ is $c_j \delta_{ij}$, then when $i$ is the index of a free variable, the $i$th entry of the sum is $\sum_{j = 1}^q c_j \delta_{ij} = c_i$. Thus the entries of the sum corresponding to free variables are $c_1, ..., c_q$. For the sum to be the zero vector, all entries of the sum must be zero, including all of $c_1, ..., c_q$. Thus $\vv_1, ..., \vv_q$ are linearly independent.
\end{proof}

\begin{example}
    (Basis for the kernel of a matrix).

    If $\AA \in \R^{3 \times 5}$ is such that
    
    \begin{align*}
        \rref(\AA) = 
        \begin{pmatrix}
            1 & 0 & 2 & 0 & 5 \\
            0 & 1 & -1 & 0 & -3 \\
            0 & 0 & 0 & 1 & 2
        \end{pmatrix},
    \end{align*}

     then $(x_1, x_2, x_3, x_4, x_5)^\top \in \ker(\AA)$ iff
     
     \begin{align*}
         \underbrace{
             \begin{pmatrix}
                1 & 0 & 2 & 0 & 5 \\
                0 & 1 & -1 & 0 & -3 \\
                0 & 0 & 0 & 1 & 2
            \end{pmatrix}
         }_{\AA}
         \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix}
         =
         \begin{pmatrix}
             0 \\ 0 \\ 0
         \end{pmatrix}.
     \end{align*}

     This linear matrix equation is equivalent to the following system of linear equations:

     \begin{align*}
         \begin{cases}
             x_1 + 2x_3 + 5x_5 = 0 \\
             x_2 - x_3 - 3x_5 = 0 \\ 
             x_4 + 2x_5 = 0 \\
         \end{cases}
         \text{ for all $x_1, x_2, x_3, x_4, x_5 \in \R$}.
     \end{align*}

     Solving for the pivot variables ($x_1, x_2, x_4$) in terms of the free variables ($x_3, x_5$), we obtain the equivalent system

     \begin{align*}
         \begin{cases}
             x_1 = -2x_3 - 5x_5 \\
             x_2 = x_3 + 3x_5 \\ 
             x_4 = -2x_5
        \end{cases}
        \text{ for all $x_1, x_2, x_3, x_4, x_5 \in \R$}.
     \end{align*}

     Appending equations relating the free variables to themselves, we obtain the system
     
     \begin{align*}
         \begin{cases}
             x_1 = -2x_3 - 5x_5 \\
             x_3 = x_3 \\
             x_2 = x_3 + 3x_5 \\ 
             x_4 = -2x_5 \\
             x_5 = x_5
        \end{cases}
        \text{ for all $x_1, x_2, x_3, x_4, x_5 \in \R$}.
     \end{align*}

     This system is equivalent to the vector equation

    \begin{align*}
        \begin{pmatrix}
            x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
        \end{pmatrix}
        =
        \begin{pmatrix}
            -2x_3 - 5x_5 \\
            x_3 + 3x_5 \\
            x_3 \\
            -2x_5 \\
            x_5
        \end{pmatrix}
        =
        x_3
        \begin{pmatrix}
            -2 \\ 1 \\ 1 \\ 0 \\ 0
        \end{pmatrix}
        + x_5
        \begin{pmatrix}
            -5 \\ 3 \\ 0 \\ -2 \\ 1
        \end{pmatrix} \text{ for all $x_1, x_2, x_3, x_4, x_5 \in \R$}.
    \end{align*}

    Thus,

    \begin{align*}
        \begin{pmatrix}
            x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
        \end{pmatrix}
        \in\spann\left(\left\{
        \begin{pmatrix}
            -2 \\ 1 \\ 1 \\ 0 \\ 0
        \end{pmatrix},
        \begin{pmatrix}
            -5 \\ 3 \\ 0 \\ -2 \\ 1
        \end{pmatrix}
        \right\}\right).
    \end{align*}

    The result of the previous theorem ensures us that

    \begin{align*}
        \left\{
        \begin{pmatrix}
            -2 \\ 1 \\ 1 \\ 0 \\ 0
        \end{pmatrix},
        \begin{pmatrix}
            -5 \\ 3 \\ 0 \\ -2 \\ 1
        \end{pmatrix}
        \right\}
    \end{align*}

    is a basis for $\ker(\AA)$. 
\end{example}

\begin{theorem}
    (Basis for the image of a matrix).

    Let $\AA = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}$ be a matrix, consider $\rref(\AA)$, and let $x_{j_1}, ..., x_{j_p}$ be the pivot variables of $\rref(\AA)$. The columns of $\AA$ corresponding to the pivot columns of $\rref(\AA)$ is a basis for $\im(\AA)$. More precisely, $\{\aa_{j_1}, ..., \aa_{j_p}\}$ is a basis for $\im(\AA)$.
\end{theorem}

\begin{proof}
    First, we show that the pivot columns of $\rref(\AA)$ are a basis for $\im(\AA)$. Secondly, we show that the corresponding columns of $\AA$ are a basis for $\im(\AA)$.

    \begin{enumerate}
        \item (Pivot columns of $\rref(\AA)$ are a basis for $\im(\AA)$). Let $\rr_j$ be the $j$th column of $\rref(\AA)$, and let $\rr_{j_1}, ..., \rr_{j_p} \in \{\rr_1, ..., \rr_n\}$ be the pivot columns of $\rref(\AA)$. Since $\rref(\AA)$ is in RREF, then $\rr_{j_\alpha}$ has in its $i$th row the entry $\delta_{ij_\alpha}$. Equivalently, $\rr_{j_\alpha} = \see_{j_\alpha}$. The pivot columns $\rr_{j_1} = \see_{j_1}, ..., \rr_{j_p} = \see_{j_p}$ are thus linearly independent. 

        We now show that the pivot columns span $\im(\AA)$. Since $\rref(\AA)$ is in RREF, then the columns of $\rref(\AA)$ containing free variables, $\rr_{k_1}, ..., \rr_{k_q} \in \{\rr_1, ..., \rr_n\}$, are linear combinations of the pivot columns of $\rref(\AA)$, so $R_\beta := \{\rr_{j_1}, ..., \rr_{j_p}, \rr_{k_1}, ..., \rr_{k_\beta}\}$ is linearly dependent for every $\beta \in [1, q]$. Since $\im(\AA) = \spann(\{\rr_{j_1}, ..., \rr_{j_p}, \rr_{k_1}, ..., \rr_{k_q}\}) = \spann(R_q)$, and since removing a vector from a linearly dependent set does not change the span of the set (recall Lemma \ref{ch::lin_alg::lemma::adding_removing_vectors_li_ld_sets}), then $R_\beta = R_{\beta + 1} - \{\rr_{k_{\beta + 1}}\}$ spans $\im(\AA)$ for all $\beta \in [0, q]$. In particular, $R_0 = \{\rr_{j_1}, ..., \rr_{j_p}\}$, which is the set of pivot columns, spans $\im(\AA)$. 
    
        Since $\rr_{j_1}, ..., \rr_{j_p}$ are linearly independent and span $\im(\AA)$, they are a basis of $\im(\AA)$.

        \item (Columns of $\AA$ corresponding to the pivot columns of $\rref(\AA)$ are a basis of $\im(\AA)$). Since row-reduction is an invertible linear operation, then there exists an invertible linear function $\ff$ such that $\ff(\AA) = \rref(\AA)$. We then have $\AA = \ff^{-1}(\rref(\AA))$, i.e., $\aa_i = \ff^{-1}(\rr_i)$, and, in particular, $\{\aa_{j_1}, ..., \aa_{j_p}\} = \{\ff^{-1}(\rr_{j_1}), ..., \ff^{-1}(\rr_{j_p})\}$. Since invertible linear functions send bases to bases (recall Theorem \ref{ch::lin_alg::thm::iso_bases_to_bases}), then $\{\ff^{-1}(\rr_{j_1}), ..., \ff^{-1}(\rr_{j_p})\} = \{\aa_{j_1}, ..., \aa_{j_p}\}$ is a basis of $\im(\AA)$.
    \end{enumerate}
\end{proof}

\begin{deriv}
    (Main dimension theorem with matrices).

    Let $\AA$ be a matrix with entries in a field $K$. The theorem about the basis for the kernel of a matrix showed that the number of vectors in a basis for $\ker(\AA)$ is the number of free variables of $\rref(\AA)$, and the theorem about the basis for the image of a matrix showed that the number of vectors in a basis for $\im(\AA)$ is the number of pivot variables of $\rref(\AA)$. Thus, $\dim(\ker(\AA))$ is the number of free variables of $\rref(\AA)$ and $\dim(\im(\AA))$ is the number of pivot variables of $\rref(\AA)$.
    
    Recall the main dimension theorem (Theorem \ref{ch::lin_alg::thm::main_dim}), which says that when $V$ is a finite-dimensional vector space we have $\dim(\im(\AA)) = \dim(V) - \dim(\ker(\AA))$. Applying this theorem, we see that if $\AA \in K^{m \times n}$, then, since the function $\vv \mapsto \AA \vv$ is a linear function $K^n \rightarrow K^m$, we have

    \begin{align*}
        (\text{number of pivot variables in $\rref(\AA)$}) = n - (\text{number of free variables in $\rref(\AA)$})
    \end{align*}.
\end{deriv}

\newpage

\subsection*{Outline}

\textbf{(T) Theory}
\begin{enumerate}
    \item defn. linear equation
    \item thm. $\{\text{solns}\} = \{ \vv + \vv_0 \mid \vv_0 \in \ker(\ff) \} \text{ for any $\vv \in \{\text{solns}\}$}$
    \item thm. number of solutions to a linear equation
\end{enumerate}

\textbf{(M) Motivating study of systems of linear equations}
\begin{enumerate}
    \item deriv. linear equations <-> linear matrix equations <-> systems of linear equations
    \item Remark. (Using intuition about planes with systems of linear equations). 
    
    Notice that a linear equation in $n$ variables $v_1, ..., v_n$ is of the form $\vv \cdot \nn = 0$ for some $\nn \in \R^n$ and $c \in K$. 

    \textbf{not all linear equations are of this form. some are of the form $\vv \cdot \nn = c$ for $c \neq 0$}
    
    In general, the \textit{orthogonal complement} to a vector subspace $W$ of vector space $V$ with inner product $\langle \cdot, \cdot \rangle$, is defined to be the vector subspace $W^\perp := \{ \langle \vv, \ww \rangle = 0 \mid \vv \in V \text{ and } \ww \in W \}$ of vectors in $V$ perpendicular to all vectors in $W$. An \textit{$(n - 1)$-dimensional plane in $\R^n$} is then defined to be the the orthogonal complement of $\spann(\nn)^\top = \{ \vv \cdot \ww = 0 \mid \vv \in \R^n \text{ and } \ww \in \spann(\nn)\} = \{ \vv \cdot \nn = 0 \mid \vv \in V\}$ for some $\nn \in \R^n$.
    
    Thus, a linear equation in $n$ variables is the equation of an $(n - 1)$-dimensional plane, and so a system of $m$ linear equations in $n$ variables can be interpreted to be an intersection of $m$ many $(n - 1)$-dimensional planes.

    Intuition about $2$-dimensional planes in $\R^3$ can therefore be used to gain intuition about the number of solutions to a system of linear equations.
\end{enumerate}

\textbf{(S) Solving linear equations with augmented matrices}
\begin{enumerate}
    \item Deriv. Augmented matrix corresponding to a system of linear equations
    \item Because adding equations, multiplying equations by a scalar, and swapping equations doesn't change a system of equations, performing the analogous row operations on the corresponding matrix doesn't change the system to which the augmented matrix correspond. The following algorithm takes advantage of this fact and uses it to put every matrix in a form from which it is easy to read the solution to the corresponding system of linear equations
    \item Definition. (Gaussian elimination algorithm).
    \item (Gaussian elimination produces matrices in RREF). After Gaussian elimination, an augmented matrix is in the following form:
     \begin{itemize}
        \item All zero rows (rows equal to $\mathbf{0}^\top$) are at the bottom of the matrix.
        \item Every nonzero row's leftmost nonzero entry is a $1$. Such $1$'s are called ``leading $1$'s'', or ``pivots''.
        \item Each column containing some row's leading $1$ has zeros in all its other entries.
    \end{itemize}
        
    This form is known as \textit{reduced row echelon form}, or \textit{RREF}.

    [Discuss how solutions to the corresponding system are easy to read off...]

    Variables whose corresponding columns have a pivot are called \textit{pivot variables}. Notice that, after considering the system of equations corresponding to an augmented matrix in RREF, we can express each pivot variable as the evaluation of a (linear) function on the non-pivot variables:

    \begin{align*}
        ...
    \end{align*}

    Thus, [we see the solution to the system is easy to read off.]
    
    Since the non-pivot variables are treated as independent variables, and since independent variables can be thought of as being allowed to freely vary, the non-pivot variables are called \textit{free variables}.
    \item Lemma about RREF.
    \begin{itemize}
        \item Every row has at most $1$ pivot and every column has at most $1$ pivot. (Proof. (Rows). A row is either zero or nonzero. If it is zero, it has no pivots; if it is nonzero, it has one pivot. (Columns). A column is either zero or nonzero. If it is zero, it has no pivots. If it is nonzero, it either contains some row's leading $1$ or doesn't. If it does, it contains one pivot. If it doesn't, it contains zero pivots.)
        \end{itemize}   
    \item reading solutions to a linear equation from a RREF
    \item RREF as linear function represented by product of elementary matrices
\end{enumerate}

\textbf{(F) Finding $\ker(\AA)$}
\begin{enumerate}    
    \item $\AA$ is invertible (and thus has a minimal kernel) iff $\rref(\AA) = \II$
    \begin{itemize}
        \item    
        ($\implies$). We prove the contrapositive, $\rref(\AA) \neq \II \implies \text{$\AA$ is not invertible}$. Suppose $\rref(\AA) \neq \II$, and consider the augmented matrix $(\rref(\AA) \mid \mathbf{0})$. Since $\rref(\AA) \neq \II$, then for some $i$ the set $\{j \mid \text{$j$ is a free column in the $i$th row of $\rref((\AA \mid \mathbf{0}))$}\}$ is nonempty. The $i$th row of $(\rref(\AA) \mid \mathbf{0})$ then corresponds to the equation $v_i + \sum_j c_j w_j = 0$. Since the $w_j$ are linearly independent [???], then $v_i = -\sum_j w_j \neq 0$. This means that the solution $\vv \in K^n$ to $\AA \vv = \mathbf{0}$ has a nonzero $i$th component, and is thus itself nonzero; $\vv \neq \mathbf{0}$. The kernel of $\AA$ is therefore nonminimal, so $\AA$ is not invertible. 

        ($\impliedby$). If $\rref(\AA) = \II$ then $\EE_k ... \EE_1 \AA = \II$ and thus $\AA^{-1} = \EE_1^{-1} ... \EE_k^{-1}$.
    \end{itemize}
    \item lemma. $\rref(\AA)$ has no pivots iff $\rref(\AA) = \mathbf{0}$.
    \begin{itemize}
        \item ($\implies$). It suffices to show that if a row has no pivots then it is $\mathbf{0}^\top$. Assume for contradiction that a row $\rr \in K^{1 \times n}$ has no pivots and is not equal to $\mathbf{0}^\top$. Then $\rr$ has some nonzero entries $r_1, ..., r_k$. Consider the leftmost such entry, $r_j$. This $r_i$ is the leading entry of $\rr$. Either $r_i = 1$ or $r_i \neq 1$. If $r_i = 1$, then $\rr$ has a pivot and thus $\rref(\AA)$ has a pivot; contradiction. If $r_i \neq 1$, then $\rr$ has a leading entry that is not equal to $1$, so $\rref(\AA)$ is not in RREF; contradiction. 
        ($\impliedby$). If $\rref(\AA) = \mathbf{0}$ then there are no $1$'s in $\rref(\AA)$. Thus there are no leading $1$'s, i.e., no pivots.
    \end{itemize}
    \item (lemma for basis for kernel). row swaps preserve kernel and column swaps preserve image
    \item basis for kernel
    \item basis for image
    \item $\dim(V)$ is number of columns, dimension of image is number of pivots, dimension of kernel is number of free variables, so $n$ is number of pivots plus number of free variables
\end{enumerate}

misc.
\begin{itemize}
    \item test for linear independence: $\vv_1, ..., \vv_k$ linearly independent iff the kernel of $\begin{pmatrix} \vv_1 & \hdots & \vv_k \end{pmatrix}$ is minimal. i.e. if its RREF is $\II$
    \item row operations don't change the kernel and column operations don't change the image
\end{itemize}

\newpage

\section{The determinant and orientation}

\subsection*{Permutations}

\begin{defn}
    (Permutation).
    
    A \textit{permutation (on $\{1, ..., n\}$)} is a bijection $\{1, ..., n\} \rightarrow \{1, ... n \}$. The set of permutations on $\{1, ..., n\}$ is denoted $S_n$.
\end{defn}

\begin{defn}
    (Cycle).

    A permutation in $S_n$ is a \textit{cycle} iff it is defined by

    \begin{align*}
        \begin{cases}
            i_j \mapsto i_{j + 1} & j \in \{1, ..., k - 1\} \\
            i_j \mapsto i_1 & j = k
        \end{cases}
    \end{align*}

    for some $i_1, ..., i_k \in \{1, ..., n\}$. If such $i_1, ..., i_k$ exist, then the cycle is represented as the following tuple without commas: $\begin{pmatrix} i_1 & \hdots & i_k \end{pmatrix}$.
\end{defn}

\begin{defn}
    (Transposition).

    A permutation in $S_n$ is a \textit{transposition} iff it swaps two elements and leaves all of the others fixed. Notice, every transposition can be written as the cycle $\transp{i}{j}$ for some $i, j \in \{1, ..., n\}$.
\end{defn}

Now, we prove an intuitive theorem.

\begin{theorem}
\label{ch::lin_alg::thm::permutations_decomposition_into_transpositions}
    (Every permutation is a composition of transpositions).

    Every permutation $\sigma \in S_n$ can be decomposed as $\sigma = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_n$ are transpositions. Note, the set of transpositions in the decomposition does not uniquely correspond to $\sigma$.
\end{theorem}

\begin{proof}  
    Notice that
    
    \begin{itemize}
        \item The function $\text{sort}_\sigma \in S_n$ that sorts $(\sigma(1), ..., \sigma(n))$ from least to greatest satisfies $\text{sort}_\sigma((\sigma(1), ..., \sigma(n))) = (1, ..., n)$.
        \item We also have $\sigma^{-1}((\sigma(1), ..., \sigma(n))) = (\sigma^{-1}(\sigma(1)), ..., \sigma^{-1}(\sigma(n)) = (1, ..., n)$.
    \end{itemize} 
    
    Thus, $\text{sort}_\sigma = \sigma^{-1}$. 
    
    Since there are many ways to describe $\text{sort}_\sigma$ as a composition of transpositions\footnote{See Definition \ref{ch::appendix::defn::bubble_sort} in the appendix for a description of one such description, the \textit{bubble sort algorithm}.}, we have $\text{sort}_\sigma = \sigma^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1 \circ ... \circ \sigma_k$ are transpositions. The inverse of a transposition is a transposition, so we have $\sigma = \sigma_1^{-1} \circ ... \circ \sigma_k^{-1}$, where $\sigma_1^{-1}, ..., \sigma_k^{-1}$ are transpositions. So, we have shown that $\sigma$ decomposes into a composition of transpositions. This decomposition is not unique, as it depends on which sorting algorithm is used to compute $\text{sort}_\sigma = \sigma^{-1}$.
    
    % (Case: $X \neq S_n$ for all $n$). Take any bijection $f:X \rightarrow \{1, ..., |X|\}$. Since $f \circ \sigma \circ f^{-1} \in S_{|X|}$, then applying the previous case implies $f \circ \sigma \circ f^{-1} = \sigma_k \circ ... \circ \sigma_1$, where $\sigma_1, ..., \sigma_k \in S_{|X|}$ are transpositions. Solving for $\sigma$, we have $\sigma = f^{-1} \circ (\sigma_k \circ ... \circ ... \sigma_1) \circ f = (f^{-1} \circ \sigma_k \circ f) \circ (f^{-1} \circ \sigma_{k - 1} \circ f) \circ ... \circ (f^{-1} \circ \sigma_1 \circ f)$. Each $f^{-1} \circ \sigma_i \circ f$ is a permutation on $X$, so we have shown that $\sigma$ is a composition of permutations on $X$, as desired.
\end{proof}

The following definition and theorem are the main reason for our excursion into permutations.

\begin{defn}
    (Parity of a permutation).

    A permutation in $S_n$ is said to have \textit{even parity}, or to be \textit{even}, iff it is equal to the composition of an even number of transpositions. A permutation is said to have \textit{odd parity}, or to be \textit{odd}, iff it is equal to the composition of an odd number of transpositions.
\end{defn}

\begin{theorem}
    (The parity of a permutation is unique).

    Every permutation in $S_n$ has a unique parity. That is, no permutation is both even and odd.
\end{theorem}

\begin{proof}
    Let $\sigma \in S_n$, and suppose $\sigma = \sigma_k \circ ... \circ \sigma_1$ and $\sigma = \pi_\ell \circ ... \circ \pi_1$, where $\sigma_1, ..., \sigma_k$ and $\pi_1, .., \pi_\ell$ are transpositions. We need to prove that $k$ and $\ell$ have the same parity.
    
    Suppose for contradiction that $k$ and $\ell$ have different parities. We have $\sigma_k \circ ... \circ \sigma_1 = \pi_k \circ ... \circ \pi_1$, and so $\text{id} = \pi_1^{-1} \circ ... \circ \pi_\ell^{-1} \circ \sigma_k \circ ... \circ \sigma_1 = \pi_1 \circ ... \circ \pi_\ell \circ \sigma_k \circ ... \circ \sigma_1$. Since $k$ and $\ell$ have different parities, then  $k + \ell$ is odd, and the identity permutation is a composition of an odd number of transpositions.
    
    Since the identity is a composition of an odd number of transpositions, it cannot be a composition of zero transpositions. It also cannot be a composition of one of one transposition, as no single transposition is the identity! Therefore, the identity is a composition of more than one transposition. Let $r$ be the smallest integer with $r > 1$ such that $\text{id} = \tau_r \circ ... \circ \tau_1$, where $\tau_1, ..., \tau_r$ are transpositions.
    
    Consider the following cases.

    (Case: $\tau_r = \tau_{r - 1}$). Then $\tau_r \circ \tau_{r - 1} = \text{id}$, and $\tau_r \circ ... \circ \tau_1 = \tau_{r - 2} \circ ... \circ \tau_1$ is a composition of $r - 2$ transpositions. We see $r$ is not minimal; this is a contradiction, so this case is impossible.

    (Case: $\tau_r \neq \tau_{r - 1}$). Suppose $\tau_r = \transp{i}{j}$.
    
    \fullindent
    {
        (Case: $\tau_r$ and $\tau_{r - 1}$ are disjoint). If $\tau_r$ and $\tau_{r - 1}$ are disjoint, then $\tau_r \circ \tau_{r - 1} = \tau_{r - 1} \circ \tau_r = \tau_{r - 1} \circ \transp{i}{j}$, where we note that $\tau_{r - 1}$ fixes $i$.
    
        (Case: $\tau_r$ and $\tau_{r - 1}$ share one element).
    
        \fullindent
        {
            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r}$ for some $r \neq j$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{i}{r} = \begin{pmatrix} i & r & j \end{pmatrix} = \begin{pmatrix} j & i & r \end{pmatrix} = \transp{j}{r} \circ \transp{i}{j}$. 
            }

            \fullindent
            {
                (Case: $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s}$ for some $s \neq i$). We have $\tau_r \circ \tau_{r - 1} = \transp{i}{j} \circ \transp{j}{s} = \begin{pmatrix} i & j & s \end{pmatrix} = \transp{s}{j} \circ \transp{i}{s}$.
            }
        }
    }

    \newcommand{\tttau}{\widetilde{\tau}}
            
    To summarize the above, in all of the possible cases, we have $\tau_r \circ \tau_{r - 1} = \tttau_r \circ \transp{i}{a_1}$ for some transposition $\tttau_r$ that fixes $i$ and some $a_1 \in \{1, ..., n\} - \{i\}$. Thus, $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ \transp{i}{a_1} \circ \tau_{r - 2} \circ ... \circ \tau_1$, where $\tttau_r$ fixes $i$.
    
    Repeating this process, we see there exist a transposition $\tttau_{r - 1}$ that fixes $i$ and $a_2 \in \{1, ..., n\} - \{i\}$ such that $\id = \tttau_r \circ \tttau_{r - 1} \circ \transp{i}{a_2} \circ ... \circ \tau_1$. It follows from induction that there exist transpositions $\tttau_2, ..., \tttau_r$ fixing $i$ and $a \in \{1, ..., n\} - \{i\}$ such that $\id = \tau_r \circ ... \circ \tau_1 = \tttau_r \circ ... \circ \tttau_2 \circ \transp{i}{a}$. Thus $\id(i) = a \neq i$. This is a contradiction, since the identity fixes every element of $\{1, ..., n\}$.
\end{proof}

\begin{defn}
    The \textit{sign} $\sgn(\sigma)$ of a permutation $\sigma \in S_n$ is defined to be
    
    \begin{align*}
        \sgn(\sigma) := 
        \begin{cases}
            1 & \text{$\sigma$ is even} \\
            -1 & \text{$\sigma$ is odd}
        \end{cases}.
    \end{align*}

    Notice, $\sgn$ is a well-defined function because the parity of a permutation is unique.

    It's useful to note that $\sgn(\sigma) = (-1)^k$, where $k$ is the number of transpositions in one of the decompositions for $\sigma$.
\end{defn}

\begin{theorem}
    (Sign of a composition of permutations).

    We have $\sgn(\pi \circ \sigma) = \sgn(\pi) \sgn(\sigma)$ for all permutations $\sigma, \pi \in S_n$.
\end{theorem}

\begin{proof}
    $\sigma$ is a composition of $k$ transpositions for some $k$, and $\pi$ is a composition of $\ell$ transpositions for some $\ell$. The theorem is equivalent to the statement ``the parity of $k + \ell$ is even iff $k$ and $\ell$ have the same parity and odd iff $k$ and $\ell$ have different parity''. This statement is an elementary fact about integers, so the theorem is true.
\end{proof}

\newpage

\subsection*{The determinant}

\begin{defn}
\label{ch::lin_alg::defn::determinant}
    (The determinant).
    
    Let $K$ be a field. We want to define a function $(K^n)^{\times n} \rightarrow K$ which, given $\vv_1, ..., \vv_n \in K^n$, returns the $n$-dimensional volume of the $n$-dimensional parallelapiped spanned by $\vv_1, ..., \vv_n$. We will denote this function by $\det:(K^n)^{\times n} \rightarrow K$. We require that $\det$ satisfy the following axioms:
    
    \begin{enumerate}
        \item $\det(\see_1, ..., \see_n) = 1$, since we want the unit $n$-cube to have an $n$-dimensional volume of $1$.
        \item
        \begin{enumerate}
            \item[2.1.]
            $\det(\vv_1, ..., \vv, ..., \vv_n) + \det(\vv_1, ..., \ww, ..., \vv_n) = \det(\vv_1, ..., \vv + \ww, ..., \vv_n)$ for all $\vv, \ww \in K^n$, since the volume of a parallelapiped that is the disjoint union of two smaller parallelapipeds should be the sum of the volumes of the smaller parallelapipeds.
            \item[2.2.] $\det(\vv_1, ..., c\vv, ..., \vv_n) = c\det(\vv_1, ..., \vv, ..., \vv_n)$ for all $c \in K$ and $\vv \in K^n$, since scaling one of the sides of a parallelapiped by $c \in K$ should increase that paralellapiped's volume by a factor of $c$. 
        \end{enumerate}
        \item $\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$, since when two sides of a parallelapiped coincide, its \textit{$n$-dimensional} volume should be zero.
    \end{enumerate}
\end{defn}

The second axiom of the determinant is equivalent to assumption that the determinant is linear in every argument. Quickly, we define vocabulary for such functions.

\begin{defn}
    (Multilinear function).

    If $V_1, ..., V_k, W$ are vector spaces, then a function $\ff:V_1 \times ... \times V_k \rightarrow W$ is said to be \textit{multilinear} iff $\vv_i \mapsto \ff(\vv_1, ..., \vv_i, ..., \vv_k)$ is a linear function for all $i \in \{1, ..., k\}$.
\end{defn}

The third axiom of the determinant can also be stated more succinctly after the investigation of the following derivation.

\begin{deriv}
    (Alternatingness of the determinant).

    Let $\vv, \ww \in K^n$. By the third axiom of the determinant, we have 

    \begin{align*}
        0 &= \det(\vv_1, ..., \vv + \ww, ..., \vv + \ww, ..., ..., \vv_n) \\
        &= \det(\vv_1, ..., \vv + \ww, ..., \vv, ..., \vv_n) \\
        &+ \det(\vv_1, ..., \vv + \ww, ..., \ww, ..., \vv_n) \\
        &= \det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) + \det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) \\
        &+ \det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n)+ \det(\vv_1, ..., \ww, ..., \ww, ..., \vv_n) \\
        &= \det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) + \det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n).
    \end{align*}
    
    Thus,

    \begin{align*}
        \det(\vv_1, ..., \vv, ..., \ww, ..., \vv_n) = -\det(\vv_1, ..., \ww, ..., \vv, ..., \vv_n) \text{ for all $\vv, \ww \in K^n$}.
    \end{align*}

    This condition is known as the \textit{alternatingness} property of the determinant. It is quick to show that the alternatingness property not only follows from the third axiom of the determinant, but is equivalent to it\footnote{Alternatingness only implies the third determinant axiom if $|K| \neq 2$, so that division by $2$ is defined.}. 

    Notice that the alternatingness property of the determinant implies that there exist $\vv_1, ..., \vv_n \in K^n$ with $\det(\vv_1, ..., \vv_n) < 0$. So, we see our intuitive assumptions about volume require that volume be \textit{signed}. In Theorem [...] we will see how signed volume corresponds to the concept of \textit{orientation}.
\end{deriv}

We quickly define alternating functions to be functions that have this alternatingness property.

\begin{defn}
    (Alternating function).

    If $V$ and $W$ are vector spaces, then a function $\ff:V^{\times k} \rightarrow W$ is \textit{alternating} iff \\ ${\ff(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_k) = -\ff(\vv_1, ..., \vv_j, ..., \vv_i, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$. Equivalently, $\ff:V^{\times k} \rightarrow W$ is alternating iff
    ${\ff(\vv_{\sigma(1)}, ..., \vv_{\sigma(k)}) = \sgn(\sigma) \ff(\vv_1, ..., \vv_k)}$ for all $\vv_1, ..., \vv_k \in V$ and $\sigma \in S_k$.
\end{defn}

Now, we have the promised succint characterization of the determinant.

\begin{theorem}
    (Succinct characterization of the determinant).

    The determinant is the unique\footnote{We have not yet explained why a function satisfying the determinant axioms is unique. We do so in Remark \ref{ch::lin_alg::rmk::det_existence_uniqueness}.} multilinear altermating function $K^n \rightarrow K$ sending $\sE \mapsto 1$.
\end{theorem}

We now investigate more properties of the determinant that follow from the axioms.

\begin{theorem}
\label{ch::lin_alg::thm::consequent_det_props}
    (Consequent properties of the determinant). 

    Let $V$ be an $n$-dimensional vector space over a field $K$.
    
    \begin{enumerate}
        \item $\det$ is invariant under linearly combining input vectors into a different input vector. That is, $\det(\vv_1, ..., \vv_i, ..., \vv_n) = \det(\vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^k d_j \vv_j, ..., \vv_n)$ for all $d_1, ..., d_k \in K$, $\vv_1, ..., \vv_n \in V$, and $i \in \{1, ..., n\}$.
        \item For all $\vv_1, ..., \vv_n \in V$, we have $\det(\vv_1, ..., \vv_n) = 0$ iff $\{\vv_1, ..., \vv_n\}$ is a linearly dependent set.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \mbox{} \\
    \begin{enumerate}
        \item 
        
        Using the axiom ``$\det(\vv_1, ..., \vv, ..., \vv, ..., \vv_n) = 0$ for all $\vv \in K^n$'' together with the multilinearity of the determinant, we have
        
        \begin{align*}
            \det(\vv_1, ..., \vv_i, ..., \vv_n)
            &= \det(\vv_1, ..., \vv_i, ..., \vv_j, ..., \vv_n) + \sum_{j = 1, j \neq i}^k d_j \det(\vv_1, ..., \vv_{i - 1}, \vv_j, \vv_{i + 1}, ..., \vv_j, ..., \vv_n) \\
            &= \det\Big( \vv_1, ..., \vv_i + \sum_{j = 1, j \neq i}^k d_j \vv_j, ..., \vv_n \Big).
        \end{align*}
        
        \item \indent ($\det(\vv_1, ..., \vv_n) = 0 \implies \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). If the input vectors are linearly dependent, we can use the invariance of $\det$ under linearly combining some columns into others (which we just proved) to produce an equal determinant in which two columns are the same. By the third axiom, this determinant is zero.
        \\ \indent ($\det(\vv_1, ..., \vv_n) = 0 \impliedby \{\vv_1, ..., \vv_n\}$ is a linearly dependent set). Suppose for contradiction that the determinant of a set of $n$ linearly independent vectors is zero. These $n$ linearly independent vectors form a basis for $K^n$, so we have shown that the determinant of a basis set is zero. But then, using multilinearity together with the invariance of $\det$ under linearly combining some vectors into a different vector, we can show that $\det(\vv_1, ..., \vv_n) = 0$ for \textit{all} $\vv_1, ..., \vv_n \in K^n$. This contradicts the first axiom that specifies $\det(\see_1, ..., \see_n) = 1$.
    \end{enumerate}
\end{proof}

Using the properties of the determinant we have amassed, we can derive a formula for the determinant.

\begin{defn}
    (Determinant of a square matrix). 
    
    We define the \textit{determinant of a square matrix} to be the result of applying $\det$ to the column vectors of that matrix.
\end{defn}

We now derive the \textit{permutation formula} for the determinant.

\begin{deriv}
\label{ch::lin_alg::deriv::permutation_formula_for_determinant}
    (Permutation formula for the determinant).
        
    Let $K$ be a field and consider a square matrix $(a_{ij}) = \begin{pmatrix} \aa_1 & \hdots & \aa_n \end{pmatrix}$ with entries in $K$. We have
    
    \begin{align*}
        \det((a_{ij})) = \det(\aa_1, ..., \aa_n) &= \det \Big(\sum_{i_1 = 1}^n a_{1 \spc i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a_{n \spc i_n} \see_{i_n} \Big) \\
        &= \sum_{i_1 = 1}^n \det \Big( a_{1 \spc i_1} \see_{i_1}, ..., \sum_{i_n = 1}^n a_{n \spc i_n} \see_{i_n} \Big) \\
        &\vdots \\
        &= \sum_{i_1 = 1}^n ... \sum_{i_n = 1}^n \det(a_{1 \spc i_1} \see_{i_1}, ..., a_{n \spc i_n} \see_{i_n}) \\
        &= \sum_{i_1, ..., i_n \in \{1, ..., n\}} \det(a_{1 \spc i_1} \see_{i_1}, ..., a_{n \spc i_n} \see_{i_n}) \\
        &= \sum_{\substack{i_1, ..., i_n \in \{1, ..., n\} \\ \text{where $i_1, ..., i_n$ are distinct}}} \det(a_{1 \spc i_1} \see_{i_1}, ..., a_{n \spc i_n} \see_{i_n}) \\
        &= \sum_{\sigma \in S_n}
        \det(a_{1 \spc \sigma(1)} \see_{\sigma(1)}, ..., a_{n \spc \sigma(n)} \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n}
        a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \det(\see_{\sigma(1)}, ..., \see_{\sigma(n)}) \\
        &= \sum_{\sigma \in S_n} a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \text{sgn}(\sigma)
        \det(\see_1, ..., \see_n) \\
        &= \sum_{\sigma \in S_n} a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \text{sgn}(\sigma)
    \end{align*}
    
    Therefore, we have
    
    \begin{align*}
        \boxed
        {
            \det((a_{ij})) = \sum_{\sigma \in S_n} a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \sgn(\sigma)
        }
    \end{align*}
    
    The most conceptually difficult step in the derivation is the one that transitions from the sum over $i_1, ..., i_n \in \{1, ..., n\}$ to the sum over $i_1, ..., i_n \in \{1, ..., n\}$, where $i_1, ..., i_n$ are distinct from each other. This step is valid because any sum term for which some $i_k$ and $i_j$ are equal is the determinant of a matrix whose columns are linearly dependent; i.e., is zero.
\end{deriv}

\begin{remark}
\label{ch::lin_alg::rmk::det_existence_uniqueness}
    (Existence and uniqueness of the determinant).

    The previous derivation shows that the function $\det$ specified in Definition \ref{ch::lin_alg::defn::determinant} exists; since the right side of the formula is a well-defined function, the determinant is the unique function satisfying the axioms.
\end{remark}

\begin{deriv}
\label{ch::lin_alg::thm::det_transpose_invariant}    
    (Determinant of a matrix is transpose-invariant).
    
    Let $\AA$ be a square matrix with entries in $K$. From the derivation of the permutation formula for the determinant, we have
    
    \begin{align*}
        \det(\AA) = 
        \sum_{\sigma \in S_n}
        \det(a_{1 \spc \sigma(1)} \see_{\sigma(1)}, ..., a_{n \spc \sigma(n)} \see_{\sigma(n)}).
    \end{align*}

    In general, if we have a diagonal matrix $\DD$ so that $\DD^\sigma = \begin{pmatrix} \dd_{\sigma(1)} & \hdots & \dd_{\sigma(n)} \end{pmatrix}$ is a permutation of $\DD$, then, unshuffling the columns of $\DD^\sigma$, transposing the result, and then reshuffling the result of that gives us back $\DD^\sigma$: $\DD^\sigma = ((\DD^{\sigma^{-1}})^\top)^\sigma$.

    We apply this fact to every matrix in the above sum:

    \begin{align*}
        \sum_{\sigma \in S_n}
        \det(a_{1 \spc \sigma(1)} \see_{\sigma(1)}, ..., a_{n \spc \sigma(n)} \see_{\sigma(n)})
        &=
        \sum_{\sigma \in S_n}
        \det\Big( \Big(\Big[\Big(a_{1 \spc \sigma(1)} \see_{\sigma(1)}, ..., a_{n \spc \sigma(n)} \see_{\sigma(n)}\Big)^{\sigma^{-1}}\Big]^\top\Big)^\sigma \Big) \\
        &= \sum_{\sigma \in S_n} \det\Big(\Big(\Big[ a_{11} \see_1, ..., a_{nn} \see_n \Big]^\top \Big)^\sigma\Big) \\
        &= \sum_{\sigma \in S_n} \det\Big(\Big( b_{11} \see_1, ..., b_{nn} \see_n\Big)^\sigma\Big), \text{ where $(b_{ij}) := (a_{ij})^\top$} \\
        &= \sum_{\sigma \in S_n}
        \det(b_{1 \spc \sigma(1)} \see_{\sigma(1)}, ..., b_{n \spc \sigma(n)} \see_{\sigma(n)}) \\
        &= \det((b_{ij})) = \det((a_{ij})^\top) = \det(\AA^\top).
    \end{align*}
    
    Therefore
    
    \begin{align*}
        \det(\AA) = \det(\AA^\top).
    \end{align*}
\end{deriv}

\begin{theorem}
    (Laplace expansion for the determinant).
    
    Consider an $n \times n$ matrix $\AA = (a_{ij})$, and let $\AA_{ij}$ denote the so-called \textit{$ij$ minor matrix} obtained by erasing the $i$th row and $j$th column of $\AA$. We have
    
    \begin{align*}
        \det(\AA) = \sum_{i = 1}^{n} a_{ij} \det(\AA_{ij}) \text{ for all $i \in \{1, ..., n\}$} \\
        \det(\AA) = \sum_{j = 1}^{n} a_{ij} \det(\AA_{ij}) \text{ for all $j \in \{1, ..., n\}$}
    \end{align*}
    
    The first equation is called the \textit{Laplace expansion for the determinant along the $i$th row}, and the second equation is called the \textit{Laplace expansion for the determinant along the $j$th column}. Note that each equation implies the other because $\det(\AA) = \det(\AA^\top)$.
\end{theorem}
    
\begin{proof}
    We prove the second equation of the theorem.
    
    Consider all terms in the permutation formula's sum for $\det(\AA)$ that have the factor $a_{ij}$. Let $\BB$ denote the shuffled diagonal matrix that corresponds to one of these terms. We can view $\det(\BB)$ as $\det(\BB) = \pm a_{ij} \det(\BB_{ij})$, where $\BB_{ij}$ is the determinant of the matrix obtained by removing the $i$th column and $j$th row from $\BB$. The $\pm$ sign is a result of the fact that the matrices $\BB$ and $\BB_{ij}$ may have different inversion counts.

    The main effort of this proof is to determine the $\pm$ sign and specify how the inversion counts of $\BB$ and $\BB_{ij}$ differ.
    
    As a first step, note that the difference in the inversion count between $\BB$ and $\BB_{ij}$ is the number of inversions that involve $a_{ij}$. Thus, our problem reduces to determining an expression for the number of inversions that involve $a_{ij}$. So, divide the matrix $\BB$ into quadrants that are centered on $a_{ij}$. Let $k_1, k_2, k_3, k_4$ be the number of inversions in the upper left, upper right, lower left, and bottom right corners of $\AA$, respectively. The number of inversions involving $a_{ij}$ is $k_2 + k_3$. Since we know $k_1 + k_2 + 1 = i$ and $k_1 + k_3 + 1 = j$, we have $k_2 + k_3 = i + j - 2 - 2k_1 = i + j - 2(k_1 + 1)$. (We also know $k_1 + k_2 + k_3 + k_4 = n$, but this is not that helpful). Thus, if $\sigma$ is the permutation corresponding to $\BB$ and $\pi$ is the permutation corresponding to $\BB_{ij}$, then $\sgn(\sigma) = \sgn(\pi)(-1)^{i + j - 2(k_1 + 1)} = \sgn(\pi)(-1)^{i + j}$. Thus $\sgn(\sigma) = (-1)^{i + j}\sgn(\pi) \iff \sgn(\pi) = (-1)^{i + j}\sgn(\sigma)$. 
    
    So,
    
    \begin{align*}
        a_{ij} \det(\BB_{ij}) &= a_{ij} \sum_{\pi \in S_n} a_{\pi(1) \spc 1} ... \cancel{a_{\pi(i) \spc i}} ..., a_{\pi(n) \spc n} \sgn(\pi) \\
        &= a_{ij} \sum_{\sigma \in S_n} a_{\pi(1) \spc 1} ... \cancel{a_{\pi(i) \spc i}} ..., a_{\pi(n) \spc n} (-1)^{i + j} \sgn(\sigma) \\
        &= (-1)^{i + j} a_{ij} \det(\BB).
    \end{align*}
    
    Thus $a_{ij} \det(\BB_{ij}) = (-1)^{i + j} a_{ij} \det(\BB) \iff \det(\BB) = (-1)^{i + j} a_{ij} \det(\BB_{ij})$. Now sum all of the $\BB$'s (the diagonal shuffled matrices) to get $\det(\AA) = \sum_{j = 1}^{n} a_{ij} \det(\AA_{ij})$.
\end{proof}
    
%\begin{theorem}
%    (Determinant of an upper triangular matrix).
%\end{theorem}

%\begin{theorem}
%    (Adjoint and Cramer's rule).
%\end{theorem}

\begin{remark}
    We have not yet shown that the determinant of a linear function $V \rightarrow W$ is well-defined; we have not shown that it doesn't depend on the bases chosen for $V$ and $W$ . We will see that this is the case in Theorem \ref{ch::lin_alg::rmk::det_well_defined}.
\end{remark}

\begin{lemma}
\label{ch::lin_alg::lemma:det_as_scale_factor}
    (Determinant as scale factor).

    Let $V$ and $W$ be $n$-dimensional vector spaces and let $h$ be a multilinear alternating function $V^{\times n} \rightarrow K$. If $\ff:V \rightarrow W$ is a linear function, then $h(\ff(\vv_1), ..., \ff(\vv_k)) = \det(\ff) h(\vv_1, ..., \vv_k)$ for all $\vv_1, ..., \vv_k \in V$.
\end{lemma}

\begin{proof}
     Because $h$ is multilinear and alternating, $h(\ff(\ee_1), ..., \ff(\ee_n))$ is analogous to the determinant $\det(\ff(\ee_1), ..., \ff(\ee_n))$. So, set $(a_{ij}) = [\ff(E)]_F$, consider the expression $h(\ff(\ee_1), ..., \ff(\ee_n))$, and use essentially the same argument as was made to derive the permutation formula of the determinant. We obtain
    
    \begin{align*}
        h(\ff(\ee_1), ...,
        \ff(\ee_n)) 
        &= \sum_{\sigma \in S_n} a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \sgn(\sigma) h(\ee_1, ..., \ee_n)
        = \Big( \sum_{\sigma \in S_n} a_{1 \spc \sigma(1)} ... a_{n \spc \sigma(n)} \sgn(\sigma) \Big) h(\ee_1, ..., \ee_n) \\
        &= \det([\ff(E)]_F) h(\ee_1, ..., \ee_n)
        = \det(\ff) h(\ee_1, ..., \ee_n).
    \end{align*}
    
   So, we have the statement on the basis $E$
   
   \begin{align*}
        h(\ff(\ee_1), ..., 
        \ff(\ee_k)) = \det(\ff) h(\ee_1, ..., \ee_k).
   \end{align*}
   
   Since $h$ is multilinear, we can extend this fact to apply to any list of vectors in $V$. This gives the lemma.
\end{proof}

\begin{theorem}
    (Product rule for determinants). 
    
    Let $V, W$ and $Z$ be finite-dimensional vector spaces of the same dimension, and consider linear functions $\ff:V \rightarrow W$ and $\gg:W \rightarrow Z$. Then $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$. Thus, if $\AA$ is an $m \times n$ matrix and $\BB$ is an $n \times p$ matrix, then $\det(\BB \AA) = \det(\BB) \det(\AA)$.
\end{theorem}

\begin{proof}
   Set $n := \dim(V) = \dim(W) = \dim(Z)$, and let $h$ be a multilinear alternating function $V^{\times n} \rightarrow K$. By the lemma, $\det(\gg \circ \ff)$ satisfies
   
   \begin{align*}
       h((\gg \circ \ff)(\vv_1), ..., (\gg \circ \ff)(\vv_n)) = \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) \text{ for all $\vv_1, ..., \vv_n \in V$}.
   \end{align*}
   
   Notice that the left side is
   
   \begin{align*}
       h\Big(\gg(\ff(\vv_1)), ..., \gg(\ff(\vv_n))\Big) = \det(\gg) h(\ff(\vv_1), ..., \ff(\vv_n)) =
       \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   Thus
   
   \begin{align*}
       \det(\gg \circ \ff) h(\vv_1, ..., \vv_n) = \det(\gg) \det(\ff) h(\vv_1, ..., \vv_n).
   \end{align*}
   
   This is a statement on the tuple $(\vv_1, ..., \vv_n) \in V^{\times n}$. Extending this statement to a statement on any tuple $\TT \in V^{\times n}$, we have $\det(\gg \circ \ff) h(\TT) = \det(\gg) \det(\ff) h(\TT)$. Thus ${\det(\gg \circ \ff) - \det(\gg) \det(\ff)) h(\TT) = \mathbf{0}}$ for all $\TT \in V^{\times n}$. Since we can choose $h$ such that $h(\TT) \neq 0$ for some $\TT \in V^{\times n}$, this forces $\det(\gg \circ \ff) - \det(\gg) \det(\ff) = 0$, giving us $\det(\gg \circ \ff) = \det(\gg) \det(\ff)$, as desired.
\end{proof}

\begin{theorem}
    (Determinant of an inverse function).
    
    Let $V$ and $W$ be finite-dimensional vector spaces of the same dimension, and consider a linear function $\ff:V \rightarrow W$. Then $\det(\ff^{-1}) = 1/\det(\ff)$.
\end{theorem}

\begin{proof}
   We have $\det(\ff \circ \ff^{-1}) = \det(\II) = 1$, and $\det(\ff \circ \ff^{-1}) = \det(\ff) \det(\ff^{-1})$ by the previous theorem, so $\det(\ff) \det(\ff^{-1}) = 1$.
\end{proof}

Now, we can show that the determinant of a linear function $\ff$ does not depend on the bases $E$ and $F$ in the definition $\det(\ff) := \det([\ff(E)]_F)$. 

\begin{theorem}
\label{ch::lin_alg::rmk::det_well_defined}
    The determinant of every linear function is well-defined.
\end{theorem}

\begin{proof}
    We need to show that if $V$ and $W$ are vector spaces of the same dimension with bases $E, G$ and $F, H$, then $\det([\ff(E)]_F) = \det([\ff(G)]_H)$.

    Recall from Theorem \ref{ch::lin_alg::deriv::matrix_relative_to_bases} that $[\ff(E)]_F = \ff_{E,F}(\sE)$, where $\ff_{E,F} = [\cdot]_F \circ \ff \circ [\cdot]_E^{-1}$. Solving for $\ff$, we have $\ff = [\cdot]_F^{-1} \circ \ff_{E,F} \circ [\cdot]_E$, and thus $\ff_{G,H} = [\cdot]_H \circ \ff \circ [\cdot]_G^{-1} = [\cdot]_H \circ [\cdot]_F^{-1} \circ \ff_{E,F} \circ [\cdot]_E \circ [\cdot]_G^{-1}$. Applying both sides of this equation to $\sE$, we obtain $[\ff(G)]_H = ([\cdot]_H \circ [\cdot]_F^{-1})(\sE) \spc [\ff(E)]_F \spc ([\cdot]_E \circ [\cdot]_G^{-1})(\sE)$.

    Now, consider the following cases.

    \fullindent
    {
        (Special case: $V = W$, $E = F$, $G = H$). In this case, the previous equation can be written as $[\ff(G)]_G = [\cdot]_G(\sE) [\cdot]_E^{-1}(\sE) \spc [\ff(E)]_E \spc [\cdot]_E(\sE) [\cdot]_G^{-1}(\sE)$. Taking the determinant of both sides\footnote{This is valid because, even though we don't yet know the determinant of a linear function is well-defined, we know the determinant of a matrix is well-defined.} and using the determinant product rule produces the factors $\det([\cdot]_G(\sE))$, $\det([\cdot]_G(\sE))^{-1} = 1/(\det([\cdot]_G(\sE))$ and $\det([\cdot]_E(\sE))$, $\det([\cdot]_E(\sE))^{-1} = 1/(\det([\cdot]_E(\sE))$, which cancel. This yields $[\ff(G)]_G = [\ff(E)]_E$, as desired. 
    }

    \fullindent
    {
        (General case). From above, we have the equation $[\ff(G)]_H = ([\cdot]_H \circ [\cdot]_F^{-1})(\sE) \spc [\ff(E)]_F \spc ([\cdot]_E \circ [\cdot]_G^{-1})(\sE)$. 

        Notice that we have $[\cdot]_H \circ [\cdot]_F^{-1} = [\cdot]_H \circ \II_W \circ [\cdot]_F^{-1} = (\II_W)_{F,H}$ and $[\cdot]_E \circ [\cdot]_G^{-1} = [\cdot]_E \circ \II_V \circ [\cdot]_G^{-1} = (\II_V)_{E,G}$, where $\II_V:V \rightarrow V$ and $\II_W:W \rightarrow W$ are the identities on $V$ and $W$. So $[\ff(G)]_H = (\II_W)_{F,H}(\sE) \spc [\ff(E)]_F \spc (\II_V)_{E,G}(\sE) = [\II_W(F)]_H [\ff(E)]_F [\II_V(E)]_G$.

        From the special case, we know that $\det([\II_W(F)]_H) = \det([\II_W(\widetilde{F})]_{\widetilde{H}})$ for any choice of $\widetilde{F}, \widetilde{H}$. We have $[\II_W(F)]_F = \II$, so using $\widetilde{F} = F$ and $\widetilde{H} = F$ shows that $\det([\II_W(F)]_H) = 1$. Similarly, $\det([\II_V(E)]_G) = 1$. Thus, we have $[\ff(G)]_H = [\ff(E)]_F$, as desired.
    }
\end{proof}

\newpage

\subsection*{Orientation of finite-dimensional vector spaces}
\label{ch::lin_alg::section::orientation}
%\begin{itemize}
%    \item \url{https://arxiv.org/pdf/1103.5263.pdf}
%    \item \url{https://en.wikipedia.org/wiki/Cartan\%E2\%80\%93Dieudonn\%C3\%A9_theorem}
%    \item \url{https://en.wikipedia.org/wiki/Rotor_(mathematics)#:~:text=A\%20rotor\%20is\%20an\%20object,the\%20Cartan\%E2\%80\%93Dieudonn\%C3\%A9\%20theorem).}
%    \item \url{https://en.wikipedia.org/wiki/Geometric_algebra#Rotating_systems}
%    \item \url{https://www.euclideanspace.com/maths/algebra/clifford/d4/transforms/index.htm}
%\end{itemize}

\textit{Orientation} is the mathematical formalization of the notions of ``clockwise'' and ``counterclockwise''; it is the notion which distinguishes different ``rotational configurations'' from each other.

Formally, an \textit{orientation} for an $n$-dimensional inner product space\footnote{We need to use inner product spaces so that we can speak of orthonormal ordered bases.} will be an orthonormal ordered basis for that space, subject to an equivalence relation, so that orientations are considered equivalent iff one of them can be ($n$-dimensionally) rotated into the other. We will say that equivalent orthonormal ordered bases are ``rotationally equivalent''.

In the intuitive case of $n = 2$, we will see that the rotational equivalence notion leads to the key \textit{antisymmetry} property of orthonormal ordered bases. We will also see that- even in $n$-dimensions- an arbitrary orthonormal ordered basis is rotationally equivalent to to one of two orientations. When an arbitrary orthonormal ordered basis is rotationally equivalent to the one, it is said to be ``positively oriented''; when it is rotationally equivalent to the other, it is said to be ``negatively oriented''.

After the above is established, the remainder of this section is devoted to computing the orientation of arbitrary orthonormal ordered bases relative to a chosen orientation. Relying on antisymmetry alone, we are able to compute orientations of orthonormal ordered bases that are permutations of the chosen orientation. To compute the orientation of other orthonormal bases, we have to engage a bit with the notion of rotation in $n$-dimensions, since this is, after all, the fundamental concept behind rotational equivalence. The idea is that the orientation of an arbitrary orthonormal ordered basis is equal to the orientation of a ``close-by'' permuted ordered orthonormal basis.

The culminating fact of this section is that the determinant ``tracks'' orientation: the determinant of an ordered orthonormal basis for $K^n$ gives the orientation of that basis.

\subsection*{First notions of orientation}

\subsubsection*{First notions of orientation in two dimensions}

\begin{defn}
    (Ordered basis).
    
    An \textit{ordered basis} for a vector space is a simply a tuple containing some ordering of the basis vectors for that space.
    
    For example, if $V$ is a 2-dimensional vector space and has basis $E = \{\ee_1, \ee_2\}$, then $(\ee_1, \ee_2)$ and $(\ee_2, \ee_1)$ are both ordered bases of $V$. We have $(\ee_1, \ee_2) \neq (\ee_2, \ee_1)$.
\end{defn}

\begin{defn}
\label{ch::lin_alg::defn::2-rotation}
    ($2$-dimensional rotation).
    
    Let $V$ be a $2$-dimensional inner product space, and let $\hU$ be an orthonormal ordered basis for $V$.
    A \textit{$2$-dimensional rotation on $V$} is a linear function $\RR_\theta:V \rightarrow V$ whose matrix relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix},
        \text{ where $\theta \in [0, 2\pi)$}.
    \end{align*}
\end{defn}

\begin{defn}
    (Equivalence under rotation for $2$-dimensional inner product spaces).

    We define orthonormal ordered bases $\hU = (\huu_1, \huu_2)$ and $\hW = (\hww_1, \hww_2)$ of a 2-dimensional inner product space\footnote{Note, this is the first time we have required that the vector space under consideration be an inner product space. We need this constraint so that we can speak of orthonormal bases.} to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).

    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

We now see how rotational equivalence leads to the key property- antisymmetry- of orthonormal ordered bases.

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_2_dimensions}

    (Antisymmetry of orthonormal ordered bases for a $2$-dimensional inner product space).
    
    Let $\hU = (\huu_1, \huu_2)$ be an orthonormal ordered basis of a $2$-dimensional inner product space. When $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2} \}$, the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is
    
    \begin{align*}
        \pm
        \begin{pmatrix}
            0 & -1 \\
            1 & 0
        \end{pmatrix}.
    \end{align*}
    
    Computing $\RR_\theta(\hU) = \RR_\theta((\huu_1, \huu_2)) = (\RR_\theta(\huu_1), \RR_\theta(\huu_2))$ for $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$, we see that the following ordered bases are rotationally equivalent:
    
    \begin{align*}
        (\huu_2, \huu_1 ) &\sim (-\huu_1, \huu_2) \sim (\huu_1, - \huu_2) \\
        (\huu_1, \huu_2) &\sim (-\huu_2, \huu_1) \sim (\huu_2, -\huu_1)
    \end{align*}
\end{theorem}

\subsubsection*{First notions of orientation in $n$ dimensions}

To study orientation in $n$ dimensions, we generalize the key notion of rotational equivalence by ``extending'' $2$-dimensional rotations so that they can be applied to elements of $n$-dimensional inner product spaces. 

\begin{defn}
    (Extension of a function).

    Let $X$ and $Y$ be sets, let $X_1 \subset X$ be subset of $X$, and consider a function $f:X_1 \rightarrow Y$. The \textit{extension of $f$ to $X$} is the function $f_{\text{ext}}$ defined by

    \begin{align*}
        f_{\text{ext}}(x) =
        \begin{cases}
            f(x) & x \in X_1 \\
            x & x \notin X_1 
        \end{cases}.
    \end{align*}
    
    We will speak frequently of extensions of $2$-dimensional rotations. In fact, we will engage in a bit of abuse of notation: when $V$ is an inner product space, we will say ``the $2$-dimensional rotation $\RR_\theta$ on $V$'' to mean ``the extension of the $2$-dimensional rotation $\RR_\theta$ to $V$'', and use $\RR_\theta$ to denote $(\RR_\theta)_{\text{ext}}$.
    
    It's helpful to spell out a further detail. If $\RR_\theta$ is a $2$-dimensional rotation on a finite-dimensional inner product space and $\hU$ is an orthonormal basis of the space, then the matrix of $\RR_\theta$ relative to $\hU$ and $\hU$ is    
    
    \begin{align*}
        [\RR_\theta(\hU)]_{\hU} =
        \kbordermatrix
        {
             & & & \text{$i$th column} &  & \text{$j$th column}   \\
             & 1 & \hdots & \cos(\theta) & 0 & -\sin(\theta) & 0 \\
             & 0 & & 0 & \vdots & 0 & \vdots \\
             & 0 & & \vdots & 1 & \vdots & \vdots \\
             & \vdots & & 0 & \vdots & 0 & \vdots \\
             & 0 & \hdots & \sin(\theta)  & 0 & \cos(\theta) & 1
        }.
    \end{align*}
    
    (The columns other than the $i$th and $j$th columns are the columns of the $n \times n$ identity matrix).
\end{defn}

Now that we are equipped with $2$-rotations that can be applied on finite-dimensional inner product spaces, we can generalize our definition of rotational equivalence to $n$ dimensions.

\begin{defn}
    (Equivalence under rotation).
    
    We define orthonormal ordered bases $\hU$ and $\hW$ of a finite-dimensional inner product space to be \textit{equivalent under rotation}, and write $\hU \sim \hW$, iff there exists a $2$-dimensional rotation $\RR_\theta$ for which $\hW = \RR_\theta(\hU)$. (Recall Definition \ref{ch::lin_alg::defn::linear_fn_acts_on_vectors} for the meaning of $\RR_\theta(\hU)$).
    
    Note, this notion of rotational equivalence $\sim$ is an equivalence relation.
\end{defn}

\begin{defn}
    (Permutation acting on a tuple). 
    
    Let $X = (x_1, ..., x_n)$ be an $n$-tuple. Given a permutation $\sigma \in S_n$, we define $X^\sigma := (x_{\sigma(1)}, ..., x_{\sigma(n)})$. For example, if $E = (\ee_1, ..., \ee_n)$ is a basis of a finite-dimensional vector space, then $E^\sigma = (\ee_{\sigma(1)}, ..., \ee_{\sigma(n)})$.
\end{defn}

\begin{theorem}
    \label{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}

    (Permutations preserve rotational equivalence).
    
    If $\hU$ and $\hW$ are orthonormal ordered bases of an $n$-dimensional inner product space, then ${\hU \sim \hW \iff \hU^\sigma \sim \hW^\sigma}$ for all permutations $\sigma \in S_n$.
\end{theorem}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$ and $\hW = (\hww_1, ..., \hww_n)$. We have that $\hU \sim \hW$ iff there exists $\theta \in [0, 2\pi)$ such that $\hW = \RR_\theta(\hU)$, which is true iff $\hww_i = \RR_\theta(\huu_i)$ for all $i$. This is true iff for all $\sigma \in S_n$ we have $\hww_{\sigma(i)} = \RR_\theta(\huu_{\sigma(i)})$, which is equivalent to $\hU^\sigma \sim \hW^\sigma$.
\end{proof}

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_general}

    (Antisymmetry of orthonormal ordered bases for an $n$-dimensional inner product space).

    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. Similarly to what was done in Theorem \ref{ch::lin_alg::thm::antisymmetry_ordered_bases_2_dimensions}, we use $\theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$ in the matrix relative to $\hU$ and $\hU$ of a $2$-dimensional rotation to obtain the formal characterization of the antisymmetry of orthonormal ordered bases.
    
    For any orthonormal ordered basis $\hU = (\huu_1, ..., \huu_n)$ of an $n$-dimensional inner product space, we have
    
    \begin{align*}
        (\huu_1, ..., \huu_i, ..., \huu_j, ..., \huu_n)
        \sim
        (\huu_1, ..., -\huu_j, ..., \huu_i, ..., \huu_n)
        \sim
        (\huu_1, ..., \huu_j, ..., -\huu_i, ..., \huu_n) \text{ for all $i, j \in \{1, ..., n\}$}.
    \end{align*}
\end{theorem}

This fundamental result leads to the following [...].

\begin{theorem}
    \label{ch::lin_alg::thm::antisymmetry_ordered_bases_signs}

    (Orthonormal ordered bases of signed vectors).
    
    Let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis of an $n$-dimensional inner product space. If $s_1, ..., s_n \in \{-1, 1\}$, then

    \begin{align*}
        (s_1 \huu_1, ..., s_n \huu_n) \sim
        \begin{cases}
            (\huu_1, ..., \huu_n), & \text{the number of $s_i$ equal to $-1$ is $-1$ is even} \\
            (\huu_1, ..., \huu_{j - 1}, -\huu_j, \huu_{j + 1}, ..., \huu_n) \text{ for any $j \in \{1, ..., n\}$}, & \text{the number of $s_i$ equal to $-1$ is $-1$ is odd} 
        \end{cases}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\
    
    (Case: the number of $s_i$ equal to $-1$ is even). Since the number of $s_i$ equal to $-1$ is even, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. If we swap the vectors in each pair twice, then, as illustrated by the example $(-\huu_i, -\huu_j) \sim (\huu_j, -\huu_i) \sim (\huu_i, \huu_j)$, the coefficients on these vectors in the rotationally equivalent orthonormal ordered basis will be $1$, with the order of the vectors being the same as in the original pair. Thus $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_n)$.

    (Case: the number of $s_i$ equal to $-1$ is odd). Since the number of $s_i$ equal to $-1$ is odd, we can pair every $s_i \huu_i$ with $s_i = -1$ to some $s_j \huu_j$ with $s_j = -1$, so that all but one $s_i \huu_i$ with $s_i = -1$ appear as a member of only one pair. Let $s_k \huu_k$ be the vector with $s_k = -1$ that is not a member of a pair. Applying the same reasoning as in the previous case, we see that $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ... \huu_n)$. Making use of the example $(-\huu_k, \huu_\ell) \sim (\huu_\ell, \huu_k) \sim (\huu_k, -\huu_\ell)$, we see that this orthonormal ordered basis is rotationally equivalent to $(\huu_1, \huu_2, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Thus, when the number of $s_i$ equal to $-1$ is odd, the orthonormal ordered basis $(s_1 \huu_1, ..., s_n \huu_n)$ is rotationally equivalent to $(\huu_1, ..., \huu_{\ell - 1}, -\huu_\ell, \huu_{\ell + 1}, ..., \huu_n)$ for any $\ell \in \{1, ..., n\}$. Replace $\ell$ with $j$ to obtain the result.
\end{proof}

\begin{lemma}
    Let $\hU$ be an orthonormal ordered basis of an $n$-dimensional inner product space. We have $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, ..., \huu_n)$. The sign of a permutation is $1$ iff the permutation consists of an even number of swaps. So, an equivalent statement to the theorem is: for all $n$, if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We prove this statement by induction on $n$.
    
    (Base case). Consider $\hU^\sigma$, where $\sigma = \tau \circ \pi$, and where $\pi, \tau$ are transpositions. 
    
    The permutation $\pi$ swaps some $i, j$. Using the example $(\huu_j, \huu_i) \sim (-\huu_i, \huu_j)$, we see \\ $\hU^\pi \sim (\huu_1, ..., \huu_{i - 1}, -\huu_i, \huu_{i + 1}, ..., \huu_n)$. Since this orthonormal ordered basis has only one basis vector with a coefficient of $-1$, it follows from the previous theorem that it is rotationally equivalent to every other such orthonormal ordered basis; in particular, it is rotationally equivalent to $\hW :=  (-\huu_1, ..., \huu_n)$.

    So far, we've shown $\hU^\pi \sim \hW$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}), we may apply the permutation $\tau$ to each side of the rotational equivalence $\hU^\pi \sim \hW$ to obtain $(\hU^\pi)^\tau = \hU^{\tau \circ \pi}  = \hU^\sigma \sim \hW^\tau$.

    We now compute the right side of this rotational equivalence, $\hW^\tau$.

    \fullindent
    {
        (Case: $\tau$ swaps $1$ with some $\ell > 1$). By studying the example $(\huu_\ell, -\huu_1) \sim (\huu_1, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (\huu_1, ..., \huu_n) = \hU$.
    }
    \fullindent
    {
        (Case: $\tau$ swaps $k \neq 1$ with some $\ell > k$). By studying the example $(\huu_\ell, \huu_k) \sim (-\huu_k, \huu_\ell)$, we see $\hW^\tau = (-\huu_1, ..., \huu_n)^\tau \sim (-\huu_1, ..., \huu_{k - 1}, -\huu_k, \huu_{k + 1}, ..., \huu_n)$. In this last orthonormal ordered basis, the number of basis vectors with a coefficient of $-1$ is even. It follows from the previous theorem that this last orthonormal ordered basis is rotationally equivalent to $\hU$.
    }

    We've shown that $\hW^\tau = \hU$ for all $\tau \in S_n$. Therefore, we have $\hU^\sigma \sim \hW^\tau \sim \hU$, as desired.
    
    (Inductive case). Assume as the inductive hypothesis if $\sigma$ consists of $2n$ transpositions, then $\hU^\sigma \sim \hU$. We need to prove that if $\sigma$ consists of $2(n + 1)$ transpositions, then $\hU^\sigma \sim \hU$.

    So, assume $\sigma$ consists of $2(n + 1) = 2n + 2$ transpositions. Then $\sigma = \tau \circ \pi$, where $\pi$ consists of $2n$ transpositions and $\tau$ consists of $2$ transpositions. By the inductive hypothesis, $\hU^\pi \sim \hU$. Since permutations preserve rotational equivalence (see Theorem \ref{ch::lin_alg::thm::permutations_preserve_rotational_equivalence}), then $(\hU^\pi)^\tau = \hU^{\tau \circ \pi} = \hU^\sigma \sim \hU^\tau$. Since $\tau$ consists of $2$ transpositions, then from the base case it follows that $\hU^\tau \sim \hU$. Thus $\hU^\sigma \sim \hU^\tau \sim \hU$, as desired.
\end{proof}

\begin{theorem}
    \begin{align*}
        \hU^\sigma \sim \hU^\pi \iff \sgn(\sigma) = \sgn(\pi) \text{ for all $\sigma, \pi \in S_n$}.
    \end{align*}
\end{theorem}

\begin{proof}
    \mbox{} \\ \indent
    ($\impliedby$).
    
    The reverse implication is true when the following two statements are:
    
    \begin{enumerate}
        \item If $\sgn(\pi) = 1$, then $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
        \item If $\sgn(\pi) = -1$, then $\sgn(\sigma) = -1 \implies \hU^\sigma \sim \hU^\pi$ for all $\sigma \in S_n$.
    \end{enumerate}

    We restate the previous lemma for convenience:

    \begin{itemize}
        \item $\sgn(\sigma) = 1 \implies \hU^\sigma \sim \hU$ for all $\sigma \in S_n$.
    \end{itemize}

    Now we prove (1) and (2).

    \begin{enumerate}
        \item If $\sgn(\sigma) = \sgn(\pi) = 1$, then $\hU^\sigma \sim \hU \sim \hU^\pi$ by the above bullet.
        \item Let $\pi$ be any odd permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi$, i.e. $\hU^{\sigma \circ \pi} \sim \hU^\pi$. Thus $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} = \hU^\pi$. In general, every odd permutation is of the form $\sigma \circ \pi$, where $\sigma$ is some even permutation. Thus, as $\sigma$ varies over the even permutations, the permutation $\tau := \sigma \circ \pi$ varies over the odd permutations. This makes the statement $\sgn(\sigma) = 1 \implies \hU^{\sigma \circ \pi} \sim \hU^\pi$ equivalent to the statement $\sgn(\tau) = -1 \implies \hU^\tau \sim \hU^\pi$. Since the former statement is true, so is the later, as desired.
    \end{enumerate}

   ($\implies$).

    The forward implication is true when the following two statements are:

    \begin{enumerate}
        \item[3.] If $\sgn(\pi) = 1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.
        \item[4.] If $\sgn(\pi) = -1$, then $\hU^\sigma \sim \hU^\pi \implies \sgn(\sigma) = -1$ for all $\sigma \in S_n$.
    \end{enumerate}

    Before we can prove (3) and (4), we need to prove the following bullet point:

    \begin{itemize}
        \item $\hU \sim \hU^\sigma \implies \sgn(\sigma) = 1$ for all $\sigma \in S_n$.

        We show the contrapositive, $\sgn(\sigma) = -1 \implies \hU \nsim \hU^\sigma$ for all $\sigma \in S_n$. Because of (2), it suffices to show that $\hU \nsim \hU^\sigma$ for \textit{any} $\sigma$ with $\sgn(\sigma) = -1$.

        Since $\sgn(\transp{1}{2}) = -1$, we will show that $\hU \nsim \hU^{(1 \spc 2)}$.
            
        Suppose for contradiction that $\hU = (\huu_1, \huu_2, \huu_3 ..., \huu_n) \sim \hU^{(1 \spc 2)} = (\huu_2, \huu_1, \huu_3 ..., \huu_n)$. Then there exists $\theta \in [0, 2\pi)$ such that ${\RR_\theta(\hU) = (\RR_\theta(\huu_1), \RR_\theta(\huu_2), \RR_\theta(\huu_3), ..., \RR_\theta(\huu_n)) = (\huu_2, \huu_1, \huu_3, ..., \huu_n) = \hU^{(1 \spc 2)}}$. From the first two entries of these tuples, we have $[\RR_\theta(\huu_1)]_{\hU} = [\huu_2]_{\hU} = (0, 1)^\top$ and $[\RR_\theta(\huu_2)]_{\hU} = [\huu_1]_{\hU} = (1, 0)^\top$.
            
        Since $[\RR_\theta(\huu_1)]_{\hU} = (\cos(\theta), \sin(\theta))^\top$ and ${[\RR_\theta(\huu_2)]_{\hU} = (-\sin(\theta), \cos(\theta))^\top}$, then this is equivalent to $((\cos(\theta), \sin(\theta))^\top = (0, 1)^\top \text{ and } (-\sin(\theta), \cos(\theta))^\top = (1, 0)^\top)$, which further implies $\sin(\theta) = 1$ and $\sin(\theta) = -1$. Since $\sin$ is a well-defined function, this is a contradiction.
    \end{itemize}

    Now we prove (3) and (4).
    
    \begin{enumerate}
        \item[3.] Let $\pi$ be any even permutation. If $\sigma$ is an even permutation, then applying the above bullet yields $(\hU^\pi)^\sigma \sim \hU^\pi \implies \sgn(\sigma) = 1$, i.e., $\hU^{\sigma \circ \pi} \sim \hU^\pi \implies \sgn(\sigma) = 1$. If we allow $\pi$ to vary over the even permutations, then since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the even permutations, and we obtain the statement $\hU^\tau \sim \hU^\pi \implies \sgn(\tau) = 1$ for all $\tau \in S_n$, as desired.
        \item[4.] The proof is analogous to that of (3). Begin the proof with ``Let $\pi$ be any odd permutation'', and use the fact that since $\sigma$ varies over the even permutations, $\tau := \sigma \circ \pi$ varies over the odd permutations.
    \end{enumerate}
\end{proof}

\begin{defn}
    (Orientation of permuted ordered bases).

    \textbf{LOOK OVER THIS. MAYBE EDIT IN LIGHT OF NEW PREVIOUS THEOREM}
    
    This means that there are only two equivalence classes\footnote{I find this relatively surprising. My intuition is that there would be something like $2^n$ or $n!$ equivalence classes of ``equivalence under rotation'' in $n$ dimensions, but nope! There are $2$ equivalence classes of ``equivalence under rotation'' for every $n$.} of ``equivalence under rotation''.
    
    We can now begin to set up the notion of orientation. An \textit{orientation for the $n$-dimensional inner product space $V$} is a choice of an orthonormal ordered basis $\hU$ for $V$. When $V$ is given the orientation $\hU$, then the \textit{orientation of a permutation of $\hU$ (relative to $\hU$)} is said to be \textit{positive} iff that permutation of $\hU$ is rotationally equivalent to $\hU$, and is said to be \textit{negative} otherwise. Per the previous paragraph, every permutation of $\hU$ is either positively oriented or negatively oriented relative to $\hU$.
\end{defn}

\begin{remark}
\label{ch::lin_alg::rmk::formalization_ccw_cw}
    (The formalization of ``counterclockwise'' and ``clockwise'').
    
    At the beginning of this section, we said that orientation would formalize the notions of ``clockwise'' and ``counterclockwise''. This formalization has been achieved by the previous definition.
    
    A \textit{counterclockwise rotational configuration} is another name for the orientation given to $\R^3$ by the standard basis, \textit{when we use the normal} human \textit{convention} of drawing the ordered basis $\sE = (\see_1, \see_2, \see_3)$ such that $\sE$ can be rotated so that $\see_1$ points out of the page, $\see_2$ points to the right, and $\see_3$ points upwards on the page. In this visual convention, the direction of each basis vector corresponds to its position in $\sE$. Counterclockwise rotational configurations are also called ``right handed coordinate systems''.
    
    A \textit{clockwise rotational configuration} then corresponds to the ordered bases which are not rotationally equivalent to $\sE$. One such ordered basis, $(-\see_1, \see_2, \see_3)$, can be depicted using the visual convention just established by drawing $\see_1$ as pointing into the page (i.e. $-\see_1$ points out of the page), $\see_2$ as pointing to the right, and $\see_3$ as pointing upwards on the page. Clockwise rotational configurations are also called ``left handed coordinate systems''.
    
    We could have easily picked a different visual convention (i.e. a different permutation of in/out, left/right, up/down) to represent the ordering of the basis that is considered to orient the space.
\end{remark}

At this point, we need some definitions and facts about $n$-rotations before we complete our development of orientation.

\subsection*{Orientation in $n$ dimensions}

\begin{defn}
    ($n$-dimensional rotations by Euler angles).

    Let $V$ be an $n$-dimensional inner product space and let $\hU = (\huu_1, ..., \huu_n)$ be an orthonormal ordered basis for $V$. An \textit{($n$-dimensional) Euler rotation} is a function $\RR_{\theta_1, ..., \theta_k}:V \rightarrow V$ of the form ${\RR_{\theta_1, ..., \theta_k} = \RR_{k, \theta_k} \circ ... \circ \RR_{1, \theta_1}}$, where, for each $i$, $\RR_{i, \theta_i}$ is the $2$-dimensional rotation by $\theta_i \in [0, 2\pi)$ on the subspace $\spann(\hU - \{\huu_i\})$. The angles $\theta_1, ..., \theta_k$ are called \textit{Euler angles}.
\end{defn}

\begin{theorem}
    All Euler rotations are orthogonal linear functions.
\end{theorem}

\begin{proof}
    This follows because $2$-dimensional rotations are orthogonal linear functions, and since a composition of orthogonal linear functions is another orthogonal linear function.
\end{proof}

% \begin{theorem}
%     Outline of proof that determinant tracks orientation:

%     \begin{itemize}
%         \item arbitrary ordered basis of $V$ -> orthonormal ordered basis of $V$ via Gram-Schmidt
%         \item orthonormal ordered basis of $V$ -> permutation of $\hU$ via Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
%         \item Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}: any ordered basis is rotationally close to a permuted orthonormal basis
%     \end{itemize} 

%     ``To do so, we first choose $\alpha, \beta, \gamma$ such that $\RR(\hww_1) = \huu_1$.''    
% \end{theorem}

\begin{comment}
\begin{lemma}
    (Standard matrix of Euler rotation).

    Let $V$ be a $3$-dimensional inner product space and let $\hU$ be an orthonormal ordered basis for $V$. Consider a rotation $\RR_{\alpha, \beta, \gamma}$ by the Euler angles $\alpha, \beta, \gamma \in [0, 2\pi)$. The matrix of $\RR_{\alpha, \beta, \gamma}$ relative to $\hU$ and $\hU$ is the product of the standard matrices of $\RR_\alpha$, $\RR_\beta$, and $\RR_\gamma$, and is equal to

    \begin{align*}
        &\begin{pmatrix}
            \cos(\gamma) & -\sin(\gamma) & 0 \\ \sin(\gamma) & \cos(\gamma) & 0 \\
            0 & 0 & 1
        \end{pmatrix} 
        \begin{pmatrix}
            \cos(\beta) & 0 & \sin(\beta) \\
            0 & 1 & 0 \\
            -\sin(\beta) & 0 & \cos(\beta)
            \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & \cos(\alpha) & -\sin(\alpha) \\
            0 & \sin(\alpha) & \cos(\alpha)
        \end{pmatrix} \\
        = 
        &\begin{pmatrix}
            \cos(\gamma)\cos(\beta) & \cos(\gamma)\sin(\beta)\sin(\alpha)-\sin(\gamma)\cos(\alpha) & \cos(\gamma)\sin(\beta)\cos(\alpha)+\sin(\gamma)\sin(\alpha) \\
            \sin(\gamma)\cos(\beta) & \sin(\gamma)\sin(\beta)\sin(\alpha)+\cos(\gamma)\cos(\alpha) & \sin(\gamma)\sin(\beta)\cos(\alpha)-\cos(\gamma)\sin(\alpha) \\
            -\sin(\beta) & \cos(\beta)\sin(\alpha) & \cos(\beta)\cos(\alpha)
        \end{pmatrix}.
    \end{align*}
\end{lemma}

\begin{proof}
    Left as an exercise :)
\end{proof}
\end{comment}

\begin{lemma}
    (In three dimensions, there is an Euler rotation taking any nonzero vector to any other vector).

    Let $V$ be a $3$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    Let $\hU = (\huu_1, \huu_2, \huu_3)$ be an orthornormal ordered basis of $V$. If we can find angles $\alpha_1, \beta_1$ that rotate $\vv$ to align with $\huu_3$,

    \begin{align*}
        (\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3,
    \end{align*}
    
    and angles $\alpha_2, \beta_2$ that rotate $\huu_3$ to align with $\ww$, 

    \begin{align*}
        (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww,
    \end{align*}

    then the Euler rotation $\RR := \RR_{2, \beta_2} \circ \RR_{1, \alpha_2} \circ \RR_{2, \beta_1} \circ \RR_{1, \alpha_1}$ satisfies $\RR(\vv) = \ww$, and the lemma is proven.

    \vspace{.5cm}

    First, we find $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    Letting $(v_1, v_2, v_3)^\top := [\vv]_{\hU}$, we have
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_1) & -\sin(\alpha_1) \\
                0 & \sin(\alpha_1) & \cos(\alpha_1)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\ v_2 \\ v_3
            \end{pmatrix}
        }_{[\vv]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \\
            v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) \\
            v_2 \sin(\alpha_1) + v_3 \cos(\alpha_1)
        \end{pmatrix}.
    \end{align*}

    $\alpha_1$ is such that the second component of the above is zero,

    \begin{align*}
        v_2 \cos(\alpha_1) - v_3 \sin(\alpha_1) = 0.
    \end{align*}
    
    One value of $\alpha_1$ that solves this equation is $\alpha_1 = \arctan(v_2/v_3)$. With this choice of $\alpha_1$, we have \\ ${\cos(\alpha_1) = v_3/\sqrt{v_2^2 + v_3^2}}$ and ${\sin(\alpha_1) = v_2/\sqrt{v_2^2 + v_3^2}}$. Thus the above is
    
    \begin{align*}
        [\RR_{1, \alpha_1}(\vv)]_{\hU} =
        \begin{pmatrix}
            v_1 \\
            (v_2 v_3)/\sqrt{v_2^2 + v_3^2} - (v_3 v_2)/\sqrt{v_2^2 + v_3^2} \\
            (v_2^2 + v_3^2)/\sqrt{v_2^2 + v_3^2}
        \end{pmatrix}
        =
        \begin{pmatrix}
            v_1 \\
            0 \\
            \sqrt{v_2^2 + v_3^2}
        \end{pmatrix}.
    \end{align*}

    Now we have

    \begin{align*}
        [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} &=
        [\RR_{2, \beta_1}(\RR_{1, \alpha_1}(\vv))]_{\hU} = [\RR_{2, \beta_1}(\hU)]_{\hU} [\RR_{1, \alpha_1}(\vv)]_{\hU} \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_1) & 0 & -\sin(\beta_1) \\
                0 & 1 & 0 \\
                \sin(\beta_1) & 0 & \cos(\beta_1)
            \end{pmatrix}
        }_{[\RR_{2, \beta_1}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                v_1 \\
                0 \\
                \sqrt{v_2^2 + v_3^2}
            \end{pmatrix}
        }_{[\RR_{1, \alpha_1}(\vv)]_{\hU}}
        =
        \begin{pmatrix}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) \\
            0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1)
        \end{pmatrix}.
    \end{align*}

    $\beta_1$ is such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3 \iff [(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv)]_{\hU} = ||\vv||\see_3$. So $\beta_1$ satisfies the following system of equations:

    \begin{align*}
        \begin{cases}
            v_1 \cos(\beta_1) - \sqrt{v_2^2 + v_3^2} \sin(\beta_1) = 0 \\
            0 = 0 \\
            v_1 \sin(\beta_1) + \sqrt{v_2^2 + v_3^2} \cos(\beta_1) = ||\vv||
        \end{cases}.
    \end{align*}

    One value of $\beta_1$ that solves the first equation is $\beta_1 = \arctan(v_1/ \sqrt{v_2^2 + v_3^2})$. 
    
    Since $\cos(\beta_1) = \sqrt{v_2^2 + v_3^2}/||\vv||$ and $\sin(\beta_1) = v_1/||\vv||$, the third equation is also satisfied:

    \begin{align*}
        \frac{v_1^2}{||\vv||} + \frac{\sqrt{v_2^2 + v_3^2}^2}{||\vv||} = \frac{v_1^2 + v_2^2 + v_3^2}{||\vv||} = \frac{||\vv||^2}{||\vv||} = ||\vv||.
    \end{align*}

    Thus we have found $\alpha_1$ and $\beta_1$ such that $(\RR_{2, \beta_1} \circ \RR_{1, \alpha_1})(\vv) = ||\vv||\huu_3$.

    \vspace{.5cm}

    Now, we find $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
    
    We have

    \begin{align*}
        [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU} &=
        [\RR_{2, \beta_2}(\hU)]_{\hU} [\RR_{1, \alpha_2}(\hU)]_{\hU} \huu_3 \\
        &=
        \underbrace
        {
            \begin{pmatrix}
                \cos(\beta_2) & 0 & -\sin(\beta_2) \\
                0 & 1 & 0 \\
                \sin(\beta_2) & 0 & \cos(\beta_2)
            \end{pmatrix}
        }_{[\RR_{2, \beta_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & \cos(\alpha_2) & -\sin(\alpha_2) \\
                0 & \sin(\alpha_2) & \cos(\alpha_2)
            \end{pmatrix}
        }_{[\RR_{1, \alpha_2}(\hU)]_{\hU}}
        \underbrace
        {
            \begin{pmatrix}
                0 \\ 0 \\ 1
            \end{pmatrix}
        }_{\huu_3} \\
        &=
        \begin{pmatrix}
            \cos(\beta_2) & 0 & -\sin(\beta_2) \\
            0 & 1 & 0 \\
            \sin(\beta_2) & 0 & \cos(\beta_2)
        \end{pmatrix}
        \begin{pmatrix}
            0 \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2)
        \end{pmatrix}
        =
        \begin{pmatrix}
            -\cos(\alpha_2) \sin(\beta_2) \\
            -\sin(\alpha_2) \\
            \cos(\alpha_2) \cos(\beta_2)
        \end{pmatrix}.
    \end{align*}

    \newcommand{\hw}{\hat{w}}

    $\alpha_2$ and $\beta_2$ are such that $\hww = (\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) \iff [\hww]_{\hU} = [(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3)]_{\hU}$. Letting $\hw_i$ denote the $i$th component of $[\hww]_{\hU}$, we see $\alpha_2$ and $\beta_2$ must satisfy the following system of equations:

    \begin{align*}
        \begin{cases}
            \hw_1 = -\cos(\alpha_2) \sin(\beta_2) \\
            \hw_2 = -\sin(\alpha_2) \\
            \hw_3 = \cos(\alpha_2) \cos(\beta_2)
        \end{cases}.
    \end{align*}

    One value of $\alpha_2$ that solves the second equation is $\alpha_2 = -\arcsin(\hw_2)$. Dividing the first equation by the third, we have $\hw_1/\hw_3 = -\tan(\beta_2)$ and thus one value of $\beta_2$ that solves the first equation is $\beta_2 = \arctan(-\hw_1/\hw_3) = -\arctan(\hw_1/\hw_3)$.

    Thus we have found $\alpha_2$ and $\beta_2$ such that $(\RR_{2, \beta_2} \circ \RR_{1, \alpha_2})(\huu_3) = \hww$.
\end{proof}

The previous lemma can easily be generalized so that it holds for all dimensions greater than or equal to $2$.

\begin{lemma}
    (There is an Euler rotation taking any nonzero vector to any other vector).
    
    Let $V$ be an $n$-dimensional inner product space. If $\vv \neq \mathbf{0}$, then for all $\ww \in V$ with $||\ww|| = ||\vv||$ there exists an Euler rotation $\RR$ such that $\RR(\vv) = \ww$.
\end{lemma}

\begin{proof}
    \mbox{} \\

    (Case: $n = 2$). The rotation $\RR_{\theta(\vv, \ww)}$ by $\theta(\vv, \ww)$ sends $\vv$ to $\ww$.
    
    (Case: $n = 3$). See the previous lemma.
    
    (Case: $n > 3$).
    
    \indent (Case: $n$ is even). [TO-DO]
    
    \indent (Case: $n$ is odd). [TO-DO]
\end{proof}

\begin{theorem}
\label{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis}
     (Any ordered basis is ``rotationally close'' to a permuted orthonormal basis).
     
     Let $V$ be an finite-dimensional inner product space. If $\hW = (\hww_1, ..., \hww_k)$ and $\hU = (\huu_1, ..., \huu_k)$ are orthonormal ordered bases of subspaces of $V$, then there is an Euler rotation taking $\hW$ to an ordered basis that is rotationally equivalent to some permutation $\hU^\sigma$ of $\hU$. 
\end{theorem}

\begin{proof}
    By the previous lemma, we know there is an Euler rotation $\RR$ that satisfies $\RR(\hww_1) = \huu_1$. 
    
    We first prove that for all $i$ we have $\RR(\hww_i) = s_i \huu_i$, where $s_i \in \{-1, 1\}$. And we show this by using induction to show that for all $i$, $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    (Base case). We have $R(\hww_1) = \huu_1 = s_1 u_1$, where $s_1 = 1$.

    (Inductive case). Assume that $\text{when $j \leq i$, we have $\RR(\hww_j) = s_j \huu_j$, where $s_j \in \{-1, 1\}$}$.

    Since $\RR$ is an orthogonal linear function, it preserves orthonormality (\textbf{see Theorem [...]}), and so $(\RR(\hww_1), ..., \RR(\hww_k))$ is orthonormal. Thus, for each $j$, $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \cancel{\RR(\hww_j)}, ..., \RR(\hww_k)$. In particular, when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $\RR(\hww_1), ..., \RR(\hww_i)$. By the inductive hypothesis, $\RR(\hww_1) = s_1 \huu_1, ..., \RR(\hww_i) = s_i \huu_i$, where $s_1, ..., s_i \in \{-1, 1\}$. So when $j \geq i + 1$, each $\RR(\hww_j)$ is perpendicular to all of $s_1 \huu_1, ..., s_i \huu_i$; it follows that each $\RR(\hww_j)$ with $j \geq i + 1$ is perpendicular to all of $\huu_1, ..., \huu_i$. This means that each $\RR(\hww_j)$ with $j \geq i + 1$ is in the orthogonal complement of $\spann((\huu_1, …, \huu_i))$.
    
    This orthogonal complement is equal to $\spann((\huu_{i + 1}, …, \huu_k)$. Therefore $\RR(\hww_{i + 1}), ..., \RR(\hww_k) \in \spann(\huu_{i + 1}, ..., \huu_k)$. Since $\RR$ preserves orthonormality (\textbf{again, see Theorem [...]}), then $\RR(\hww_{i + 1}), ..., \RR(\hww_k)$ are also orthonormal.
    
    Since $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1}, ..., \huu_k)$, then $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1} + c_{i + 2} \huu_{i + 2} + ... + c_k \huu_k$ for some scalars $c_{i + 1}, ..., c_k$. We claim that we must have $c_{i + 2} = ... = c_k = 0$; assume for contradiction that $c_{\ell_1}, ..., c_{\ell_p}$ are nonzero scalars with $\ell_1, ..., \ell_p > i + 1$. A quick inner product computation shows that $\RR(\hww_{i + 1})$ is not perpendicular to any of $\huu_{\ell_1}, ..., \huu_{\ell_p}$. Since $\RR(\hww_{i + 2}), ..., \RR(\hww_k)$ must all be perpendicular to $\RR(\hww_{i + 1})$, it follows that $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \notin \spann((\huu_{\ell_1}, ..., \huu_{\ell_p}))$. Thus $\RR(\hww_{i + 2}), ..., \RR(\hww_k) \in \spann((\huu_{i + 2}, ..., \huu_k) - (\huu_{\ell_1}, ..., \huu_{\ell_p}))$, which is a subspace of dimension $|[i + 2, k] \cap \Z| - p$. On the other hand, since $(\RR(\hww_{i + 2}, ..., \RR(\hww_k))$ is an orthonormal basis, then $(\RR(\hww_{i + 2}), ..., \RR(\hww_k))$ span a $|[i + 2, k]|$-dimensional space. This is a contradiction; vectors cannot be in a space that has a lesser dimension than their span. Therefore, $c_{i + 2} = ... = c_k = 0$, and $\RR(\hww_{i + 1}) = c_{i + 1} \huu_{i + 1}$, i.e., $\RR(\hww_{i + 1}) \in \spann(\huu_{i + 1})$. Since $\RR(\hww_{i + 1})$ is a unit vector, then $||\RR(\hww_{i + 1})|| = ||s_{i + 1} \huu_{i + 1}|| = |s_{i + 1}| = 1$. Thus $s_{i + 1} \in \{-1, 1\}$, and $\RR(\hww_{i + 1}) = s_{i + 1} \huu_{i + 1}$, as desired.

    \vspace{.25cm}

    Since $\RR(\hww_i) = s_i \huu_i$ for all $i$, where $s_i \in \{-1, 1\}$, then $\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k)$, where $s_1, ..., s_k \in \{-1, 1\}$. 

    If the number of $s_i$ equal to $-1$ is even, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = 1$, and if the number of $s_i$ equal to $-1$ is odd, then ${(s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for all $\sigma$ with $\sgn(\sigma) = -1$. (Recall Derivation     \ref{ch::lin_alg::thm::antisymmetry_ordered_bases_signs}). We see that in either case, ${\RR(\hW) = (s_1 \huu_1, ..., s_k \huu_k) \sim \hU^\sigma}$ for some $\sigma \in S_k$. This proves the theorem.
\end{proof}

\subsubsection*{Completing the definition of orientation for finite-dimensional inner product spaces}

\begin{defn}
    (Orientation of arbitrary ordered bases).
    
    Let $V$ be an $n$-dimensional inner product space, and fix an orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ for $V$. We know how to ``orient'' ordered bases for $V$ that happen to be permutations of $\hU$. Now, we generalize the notion of orientation so that it applies to any orthonormal ordered basis of $V$.
    
    We define the \textit{orientation of an orthonormal ordered basis $E$ of $V$} that is not a permutation of $\hU$ to be the orientation of the unique permuted orthonormal basis $\hU^\sigma$, $\sigma \in S^n$, of $\hU$ for which there exists an $n$-rotation taking $E$ to $\hU^\sigma$.
    
    Then, we define the \textit{orientation of an arbitrary orthonormal ordered basis $E$ of $V$} to be the orientation of the unique orthonormal basis $\hU_E$ obtained from performing the Gram-Schmidt process on $E$ (see Theorem \ref{ch::bilinear_forms_metric_tensors::theorem::Gram-Schmidt}).
\end{defn}

\begin{theorem}
\label{ch::lin_alg::thm::det_tracks_orientation}
    (The determinant tracks orientation). 
    
    Let $V$ be an $n$-dimensional inner product space with an orientation given by an orthonormal ordered basis $\hU$. Let $E = (\ee_1, ..., \ee_n)$ be any ordered basis (not necessarily orthonormal) of $V$. We have $\det([\EE]_{\hU}) > 0$ iff $E$ is positively oriented relative to $\hU$, and $\det([\EE]_{\hU}) < 0$ iff $E$ is negatively oriented relative to $\hU$.
\end{theorem}

\begin{proof}
   This proof has two overarching steps. First, we pass the definition of orientation for arbitrary ordered bases of $V$ to the definition of orthonormal ordered bases of $V$ by obtaining an orthonormal ordered basis $\hU_E$ from $E$. Then we pass the definition of orientation for orthonormal ordered bases of $V$ that are not permutations of $\hU$ to the definition of orientation for orthonormal ordered bases of $V$ that are permutations of $\hU$.
   
   To begin the first step, consider $\det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$, and perform Gram-Schmidt on $([\ee_1]_{\hU}, ..., [\ee_n]_{\hU})$. In the $i$th step of Gram-Schmidt, a linear combination of the vectors $[\ee_1]_{\hU}, ..., \cancel{[\ee_i]_{\hU}}, ..., [\ee_n]_{\hU}$ is added to $[\ee_i]_{\hU}$. Recall from Theorem \ref{ch::lin_alg::thm::consequent_det_props} that the determinant is invariant under linearly combining input vectors into a different input vector. Therefore, performing Gram-Schmidt does not change the determinant. That is, if $\hU_E = (\hww_1, ..., \hww_n)$ is the orthonormal basis obtained by performing Gram-Schmidt on $E$, then 
   
   \begin{align*}
       \det([\EE]_{\hU}) = \det([\ee_1]_{\hU}, ..., [\ee_n]_{\hU}) = \det([\hww_1]_{\hU}, ..., [\hww_n]_{\hU})
       =
       \det([\hU_E]_{\hU}).
   \end{align*}
   
   In performing this first step of the proof, the determinant has stayed the same as we've passed from $E$ to $\hU_E$. We now show that the determinant continues to stay the same as we pass from  $\hU_E$ to some permutation $\hU^\sigma$ of $\hU$.
   
   Theorem \ref{ch::lin_alg::thm::n_rot_acts_on_orthonormal_basis} says that there is a $n$-rotation $\RR$ taking $\hU_E$ to $\hU^\sigma$, for some $\sigma \in S_n$, and Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1} guarantees that $\det(\RR) = 1$. Thus, since $\hU^\sigma = \RR(\hU_E)$, we have
   
   \begin{align*}
        \det([\hU_E]_{\hU}) 
        = \det([\RR(\hU_E)]_{\hU}) \det([\hU_E]_{\hU})
        = \det([(\RR \circ \II)(\hU_E)]_{\hU})
        = \det([\RR(\hU_E)]_{\hU})
        = \det([\hU^\sigma]_{\hU}).
   \end{align*}
   
   To conclude the proof, we will show that $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$; once we have shown this, we are done, since $\sgn(\sigma) \det([\hU]_{\hU}) = \sgn(\sigma) \det(\II) = \sgn(\sigma)$. Since any permutation is a composition of transpositions, then $\hU^\sigma$ can be obtained from $\hU$ by repeatedly swapping vectors in $\hU$. Whenever vectors are swapped in the determinant, the sign of the determinant is multiplied by $-1$. This accounts for the $\sgn(\sigma)$ factor in the equation $\det([\hU^\sigma]_{\hU}) = \sgn(\sigma) \det([\hU]_{\hU})$.
\end{proof}

\subsection*{Orientation of finite-dimensional vector spaces}

\label{ch::lin_alg::orientation_finite_dim_vector_space}

The fact that the determinant tracks orientation is the main result of our discussion of orientation. Because determinants do not rely on the existence of an inner product, the determinant can be used to generalize the notion of orientation to any finite-dimensional vector space.

\begin{defn}
\label{ch::lin_alg::defn::orientation_finite_dim_vector_space}
    (Orientation of a finite-dimensional vector space).
    
    Let $V$ be a finite-dimensional vector space (not necessarily an inner product space). An \textit{orientation on $V$} is a choice of ordered basis $E$ for $V$. (Notice here that $E$ is not necessarily orthonormal, because $V$ might not have an inner product!). If we have given $V$ the orientation $E$, then we say that an ordered basis $F$ of $V$ is \textit{positively oriented (relative to $E$)} iff $\det([\FF]_E) > 0$, and that $F$ is \textit{negatively oriented (relative to $E$)} iff $\det([\FF]_E) < 0$.
    
    A finite-dimensional vector space that has an orientation is called an \textit{oriented (finite-dimensional) vector space}.
\end{defn}
 
\begin{remark}
    (Antisymmetry of orthonormal ordered bases).
    
    Notice that we still have the previous antisymmetry of orthonormal ordered bases due to the antisymmetry of the determinant.
\end{remark}

As a last sidenote, the following theorem gives some justification as to why our definition of $n$-rotation was a good definition. (Strictly speaking, though, the justification is somewhat circular, since we used the notion of $n$-rotations to explain how the determinant- which is involved in the justification- tracks orientation).

\begin{theorem}
    If $V$ is an $n$-dimensional inner product space, then 
    
    \begin{align*}
        \{\text{$n$-rotations on $V$}\} = \{\text{orthogonal linear functions $V \rightarrow V$ with determinant 1}\}
    \end{align*}
\end{theorem}

\begin{proof}
    \scriptsize Proof idea is from jagr2808. 
    \fontsize{10pt}{12pt}\selectfont \\
    \indent ($\subseteq$). This is just Theorem \ref{ch::lin_alg::thm::n_dim_rot_det_1}.
    
    \indent ($\supseteq$). If $\ff:V \rightarrow V$ is an orthogonal linear function, then $\ff$ sends an arbitrary orthonormal basis $\hU = (\huu_1, ..., \huu_n)$ of $V$ to another orthonormal basis $\ff(\hU) = (\ff(\huu_1), ..., \ff(\huu_n))$.

    Let $\RR_1$ be the $2$-dimensional rotation sending $\ff(\huu_1)$ to $\huu_1$. Then $\RR_2 := \RR_1 \circ \ff:V \rightarrow V$ fixes $\huu_1$, so we can think of it as a function from $\spann(\huu_2, ..., \huu_n)$ to $\spann(\huu_2, ..., \huu_n)$. 
    
    We now use the above idea finitely many times. Define $\RR_{i + 1}$ to be the $2$-dimensional rotation sending ${(\RR_i \circ \RR_{i - 1})(\huu_i)}$ to $\huu_i$. We know that such a $2$-dimensional rotation always exists because $\det(\ff) = 1$ implies that $(\ff(\huu_1), ..., \ff(\huu_n))$ has the same orientation as $(\huu_1, ..., \huu_n)$. (Recall, having the same orientation involves ``rotational equivalence'' via $n$-rotations, which are compositions of $2$-dimensional rotations). By induction, $\RR_n \circ \ff$ is a composition of $2$-dimensional rotations.
    
    Thus, since $\ff = \RR_n^{-1} \circ (\RR_n \circ \ff)$, we see $\ff$ is a composition of the $2$-dimensional rotations $\RR_n^{-1}$ and $\RR_n^{-1} \circ \ff$.
    
    %If T is an orthogonal endomorphism then T maps the standard basis e_1, ..., e_n to an orthonormal basis u_1, ..., u_n.

    %Let R be the 2-rotation sending u_1 to e_1. Then RT is an endomorphism in SO(n) that fixes e_1, so we can think of it as an endomorphism in SO(n-1) on the orthogonal complement. By induction RT is the composition of 2-rotations. And T = R-1RT, so T is as well.
\end{proof}

\section{Eigenvectors and eigenvalues}

[This section not required for rest of book]

\begin{defn}
    (Eigenvectors and eigenvalues of a linear function).
    
    Let $V$ and $W$ be vector spaces over a field $K$, and let $\ff:V \rightarrow W$ be a linear function. A vector $\vv \in V$ is said to be an \textit{eigenvector (of $\ff$)} iff there is a scalar $\lambda \in K$ such that $\ff(\vv) = \lambda \vv$. Iff $\vv$ is indeed an eigenvector of $\ff$, then the $\lambda \in K$ for which $\ff(\vv) = \lambda \vv$ is said to be the \textit{eigenvalue corresponding to $\vv$}.
    
    In other words, the eigenvectors of a linear function are the vectors that get sent to scalar multiples of themselves by the function, and the eigenvalues corresponding to those eigenvectors are the scalars involved in said scalar multiples.
\end{defn}

\begin{remark}
    (``Characteristic vectors'').
    
    The word ``eigen'' within ``eigenvector'' is German. Back when eigenvectors were first defined, English-speaking mathematicians called them ``characteristic vectors''. This terminology is not used today, but is helpful to keep it in mind, as we will see that, in some cases, a linear function is completely specified if its ``characteristic vectors'' and corresponding ``characteristic values'' are known.
\end{remark}

\begin{theorem}
    Let $V$ be a vector space. A linear function $\ff:V \rightarrow V$ is invertible iff $\mathbf{0}$ is the only eigenvector of $\ff$ whose eigenvalue is $0$.
\end{theorem}

\begin{proof}
   Consider the contrapositive. Some nonzero eigenvector $\vv$ has $0$ as its eigenvalue iff $\ff$ has a nonminimal kernel, which is equivalent to $\ff$ not being invertible.
\end{proof}

\begin{remark}
    (Eigenvectors, eigenvalues, and zero).
    
    According to the above definition, $\mathbf{0}$ is an eigenvector of every linear function, with its corresponding eigenvalue being $0$. 
    
    Some authors explicitly disallow $\mathbf{0}$ from being an eigenvector of any linear function in their definition of ``eigenvector'' so that the condition ``$\mathbf{0}$ is the only eigenvector of $\ff$ whose eigenvalue is $0$'' is equivalent to the condition ``$0$ is not an eigenvalue of $\ff$''. The later is easier to say than the former. This convention is a bit too contrived, though, so we will not use it.
\end{remark}

\begin{theorem}
    (Intuition for an invertibility condition).
    
    [intuition on what a determinant is]
    
    $\ff$ is not invertible iff for any basis $E$ of $V$, the set $\ff(E)$ is linearly dependent

    [$n$-dimensional volume spanned by a linearly dependent set is $0$]
\end{theorem}

\begin{deriv}
    (Eigenvectors).

     Let $V$ and $W$ be vector spaces over a field $K$ and let $\ff:V \rightarrow W$ be a linear function. A vector $\vv \in V$ is an eigenvector of $\ff$ with eigenvalue $\lambda$ iff

    \begin{align*}
        &\quad \quad \quad \ff(\vv) = \lambda \vv \\
        &\iff \ff(\vv) - \lambda \vv = \mathbf{0} \\
        &\iff \ff(\vv) - \lambda \II(\vv) = \mathbf{0} \\
        &\iff (\ff - \lambda \II)(\vv) = \mathbf{0} \\
        &\iff \vv \in \ker(\ff - \lambda \II).
    \end{align*}

    Thus,

    \begin{align*}
        \boxed
        {
            \text{$\vv$ is an eigenvector of $\ff$ with eigenvalue $\lambda$ iff $\vv \in \ker(\ff - \lambda \II)$}
        }
    \end{align*}
\end{deriv}

\begin{deriv}
    (Eigenvalues).

    Let $V$ and $W$ be vector spaces over a field $K$ and let $\ff:V \rightarrow W$ be a linear function.
    
    A scalar $\lambda \in K$ is an eigenvalue of $\ff$ if and only if $\ker(\ff - \lambda \II)$ is nonempty, which is the case if and only if $\ff - \lambda \II$ is not invertible. This is equivalent to the condition $\det(\ff - \lambda \II) = 0$. Thus,

    \begin{align*}
        \boxed
        {
            \text{$\lambda$ is an eigenvalue of $\ff$ iff $\det(\ff - \lambda \II) = 0$}
        }
    \end{align*}
\end{deriv}
